{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac67ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: scipy in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (1.7.2)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.20.0-cp310-cp310-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (2.4.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow[and-cuda])\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow[and-cuda])\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (0.7.0)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow[and-cuda])\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow[and-cuda])\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow[and-cuda])\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (26.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (6.33.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow[and-cuda])\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (2.1.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow[and-cuda])\n",
      "  Using cached grpcio-1.76.0-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow[and-cuda])\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow[and-cuda])\n",
      "  Downloading keras-3.12.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow[and-cuda])\n",
      "  Downloading h5py-3.15.1-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow[and-cuda])\n",
      "  Downloading ml_dtypes-0.5.4-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting nvidia-cublas-cu12<13.0,>=12.5.3.2 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu12-12.9.1.4-py3-none-win_amd64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12<13.0,>=12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.9.79-py3-none-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12<13.0,>=12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-win_amd64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12<13.0,>=12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.9.86-py3-none-win_amd64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12<13.0,>=12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.9.79-py3-none-win_amd64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12<10.0,>=9.3.0.75 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cudnn_cu12-9.19.0.56-py3-none-win_amd64.whl.metadata (1.9 kB)\n",
      "Collecting nvidia-cufft-cu12<12.0,>=11.2.3.61 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cufft_cu12-11.4.1.4-py3-none-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-curand-cu12<11.0,>=10.3.6.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_curand_cu12-10.3.10.19-py3-none-win_amd64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12<12.0,>=11.6.3.83 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusolver_cu12-11.7.5.82-py3-none-win_amd64.whl.metadata (1.9 kB)\n",
      "Collecting nvidia-cusparse-cu12<13.0,>=12.5.1.3 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusparse_cu12-12.5.10.65-py3-none-win_amd64.whl.metadata (1.8 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.19.1-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow[and-cuda])\n",
      "  Downloading protobuf-5.29.6-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow[and-cuda])\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow[and-cuda])\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting nvidia-cublas-cu12==12.5.3.2 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu12-12.5.3.2-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.5.82-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.5.82-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.5.82-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.5.82-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.3.0.75 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cudnn_cu12-9.3.0.75-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.3.61 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cufft_cu12-11.2.3.61-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.6.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_curand_cu12-10.3.6.82-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.3.83 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusolver_cu12-11.6.3.83-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.1.3 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusparse_cu12-12.5.1.3-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "  Downloading tensorflow-2.18.1-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_intel-2.18.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.17.1-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.17.1 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_intel-2.17.1-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting nvidia-cublas-cu12==12.3.4.1 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.3.101 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12==12.3.107 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.3.107 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.3.101 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.7.29 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.12.1 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.4.107 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.5.4.101 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.2.0.103 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Using cached tensorflow-2.17.0-cp310-cp310-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow[and-cuda])\n",
      "  Using cached tensorflow_intel-2.17.0-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.16.2-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.16.2 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_intel-2.16.2-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "INFO: pip is still looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Using cached tensorflow-2.16.1-cp310-cp310-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow[and-cuda])\n",
      "  Using cached tensorflow_intel-2.16.1-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.15.1-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting tensorflow-intel==2.15.1 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_intel-2.15.1-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting nvidia-cublas-cu12==12.2.5.6 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu12-12.2.5.6-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.2.142 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.2.142-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12==12.2.140 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.2.140-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.2.140 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.2.140 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.2.140-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.4.25 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cudnn_cu12-8.9.4.25-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.8.103 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cufft_cu12-11.0.8.103-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.3.141 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_curand_cu12-10.3.3.141-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.5.2.141 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusolver_cu12-11.5.2.141-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.2.141 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusparse_cu12-12.1.2.141-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-intel==2.15.0 (from tensorflow[and-cuda])\n",
      "  Using cached tensorflow_intel-2.15.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.14.1-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.14.1 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_intel-2.14.1-cp310-cp310-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting nvidia-cublas-cu11==11.11.3.6 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.8.87-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu11-11.8.89-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.14.0-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.14.0 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_intel-2.14.0-cp310-cp310-win_amd64.whl.metadata (4.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.13.1-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.1 (from tensorflow[and-cuda])\n",
      "  Downloading tensorflow_intel-2.13.1-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp310-cp310-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading grpcio-1.75.1-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "  Downloading grpcio-1.75.0-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "  Using cached grpcio-1.74.0-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached google_auth-2.48.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading markdown-3.10.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting wheel>=0.26 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Downloading wheel-0.46.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting cryptography>=38.0.3 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached cryptography-46.0.4-cp38-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2026.1.4)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached pyasn1-0.6.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=38.0.3->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached cffi-2.0.0-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "INFO: pip is looking at multiple versions of cryptography to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cryptography>=38.0.3 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached cryptography-46.0.3-cp38-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "  Downloading cryptography-46.0.2-cp38-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "  Downloading cryptography-46.0.1-cp38-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "  Downloading cryptography-46.0.0-cp38-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "  Using cached cryptography-45.0.7-cp37-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda])\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.0.3)\n",
      "Downloading tensorflow-2.13.1-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Downloading tensorflow_intel-2.13.1-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "   ---------------------------------------- 0.0/276.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/276.5 MB 2.4 MB/s eta 0:01:55\n",
      "   ---------------------------------------- 1.0/276.5 MB 2.4 MB/s eta 0:01:55\n",
      "   ---------------------------------------- 1.3/276.5 MB 2.1 MB/s eta 0:02:12\n",
      "   ---------------------------------------- 1.8/276.5 MB 2.2 MB/s eta 0:02:03\n",
      "   ---------------------------------------- 2.1/276.5 MB 2.0 MB/s eta 0:02:18\n",
      "   ---------------------------------------- 2.4/276.5 MB 1.8 MB/s eta 0:02:32\n",
      "   ---------------------------------------- 2.6/276.5 MB 1.8 MB/s eta 0:02:37\n",
      "   ---------------------------------------- 2.9/276.5 MB 1.7 MB/s eta 0:02:39\n",
      "   ---------------------------------------- 2.9/276.5 MB 1.7 MB/s eta 0:02:39\n",
      "   ---------------------------------------- 2.9/276.5 MB 1.7 MB/s eta 0:02:39\n",
      "   ---------------------------------------- 3.1/276.5 MB 1.3 MB/s eta 0:03:25\n",
      "   ---------------------------------------- 3.1/276.5 MB 1.3 MB/s eta 0:03:25\n",
      "   ---------------------------------------- 3.1/276.5 MB 1.3 MB/s eta 0:03:25\n",
      "   ---------------------------------------- 3.1/276.5 MB 1.3 MB/s eta 0:03:25\n",
      "   ---------------------------------------- 3.4/276.5 MB 1.0 MB/s eta 0:04:21\n",
      "   ---------------------------------------- 3.4/276.5 MB 1.0 MB/s eta 0:04:21\n",
      "   ---------------------------------------- 3.4/276.5 MB 1.0 MB/s eta 0:04:21\n",
      "   ---------------------------------------- 3.4/276.5 MB 1.0 MB/s eta 0:04:21\n",
      "    --------------------------------------- 3.7/276.5 MB 879.5 kB/s eta 0:05:11\n",
      "    --------------------------------------- 3.7/276.5 MB 879.5 kB/s eta 0:05:11\n",
      "    --------------------------------------- 3.7/276.5 MB 879.5 kB/s eta 0:05:11\n",
      "    --------------------------------------- 3.7/276.5 MB 879.5 kB/s eta 0:05:11\n",
      "    --------------------------------------- 3.7/276.5 MB 879.5 kB/s eta 0:05:11\n",
      "    --------------------------------------- 3.9/276.5 MB 783.0 kB/s eta 0:05:49\n",
      "    --------------------------------------- 3.9/276.5 MB 783.0 kB/s eta 0:05:49\n",
      "    --------------------------------------- 3.9/276.5 MB 783.0 kB/s eta 0:05:49\n",
      "    --------------------------------------- 3.9/276.5 MB 783.0 kB/s eta 0:05:49\n",
      "    --------------------------------------- 4.2/276.5 MB 713.0 kB/s eta 0:06:22\n",
      "    --------------------------------------- 4.2/276.5 MB 713.0 kB/s eta 0:06:22\n",
      "    --------------------------------------- 4.5/276.5 MB 695.5 kB/s eta 0:06:32\n",
      "    --------------------------------------- 4.5/276.5 MB 695.5 kB/s eta 0:06:32\n",
      "    --------------------------------------- 4.5/276.5 MB 695.5 kB/s eta 0:06:32\n",
      "    --------------------------------------- 4.5/276.5 MB 695.5 kB/s eta 0:06:32\n",
      "    --------------------------------------- 4.5/276.5 MB 695.5 kB/s eta 0:06:32\n",
      "    --------------------------------------- 4.5/276.5 MB 695.5 kB/s eta 0:06:32\n",
      "    --------------------------------------- 4.7/276.5 MB 610.7 kB/s eta 0:07:26\n",
      "    --------------------------------------- 4.7/276.5 MB 610.7 kB/s eta 0:07:26\n",
      "    --------------------------------------- 4.7/276.5 MB 610.7 kB/s eta 0:07:26\n",
      "    --------------------------------------- 5.0/276.5 MB 607.7 kB/s eta 0:07:27\n",
      "    --------------------------------------- 5.2/276.5 MB 615.4 kB/s eta 0:07:21\n",
      "    --------------------------------------- 5.5/276.5 MB 630.8 kB/s eta 0:07:10\n",
      "    --------------------------------------- 5.8/276.5 MB 646.5 kB/s eta 0:06:59\n",
      "    --------------------------------------- 6.0/276.5 MB 666.3 kB/s eta 0:06:46\n",
      "    --------------------------------------- 6.3/276.5 MB 683.0 kB/s eta 0:06:36\n",
      "    --------------------------------------- 6.8/276.5 MB 712.1 kB/s eta 0:06:19\n",
      "   - -------------------------------------- 7.1/276.5 MB 729.4 kB/s eta 0:06:10\n",
      "   - -------------------------------------- 7.3/276.5 MB 745.0 kB/s eta 0:06:02\n",
      "   - -------------------------------------- 7.9/276.5 MB 779.7 kB/s eta 0:05:45\n",
      "   - -------------------------------------- 8.4/276.5 MB 811.4 kB/s eta 0:05:31\n",
      "   - -------------------------------------- 8.7/276.5 MB 828.5 kB/s eta 0:05:24\n",
      "   - -------------------------------------- 9.2/276.5 MB 852.7 kB/s eta 0:05:14\n",
      "   - -------------------------------------- 9.4/276.5 MB 863.5 kB/s eta 0:05:10\n",
      "   - -------------------------------------- 9.7/276.5 MB 877.9 kB/s eta 0:05:04\n",
      "   - ------------------------------------- 10.2/276.5 MB 905.6 kB/s eta 0:04:55\n",
      "   - ------------------------------------- 10.7/276.5 MB 928.2 kB/s eta 0:04:47\n",
      "   - ------------------------------------- 11.0/276.5 MB 944.9 kB/s eta 0:04:42\n",
      "   - ------------------------------------- 11.3/276.5 MB 953.5 kB/s eta 0:04:39\n",
      "   - ------------------------------------- 11.5/276.5 MB 960.6 kB/s eta 0:04:36\n",
      "   - ------------------------------------- 11.8/276.5 MB 961.2 kB/s eta 0:04:36\n",
      "   - ------------------------------------- 12.3/276.5 MB 983.2 kB/s eta 0:04:29\n",
      "   - -------------------------------------- 12.8/276.5 MB 1.0 MB/s eta 0:04:22\n",
      "   - -------------------------------------- 13.1/276.5 MB 1.0 MB/s eta 0:04:19\n",
      "   - -------------------------------------- 13.6/276.5 MB 1.0 MB/s eta 0:04:14\n",
      "   -- ------------------------------------- 13.9/276.5 MB 1.0 MB/s eta 0:04:13\n",
      "   -- ------------------------------------- 14.2/276.5 MB 1.0 MB/s eta 0:04:12\n",
      "   -- ------------------------------------- 14.4/276.5 MB 1.0 MB/s eta 0:04:11\n",
      "   -- ------------------------------------- 14.7/276.5 MB 1.1 MB/s eta 0:04:09\n",
      "   -- ------------------------------------- 14.9/276.5 MB 1.1 MB/s eta 0:04:07\n",
      "   -- ------------------------------------- 15.5/276.5 MB 1.1 MB/s eta 0:04:03\n",
      "   -- ------------------------------------- 15.7/276.5 MB 1.1 MB/s eta 0:04:01\n",
      "   -- ------------------------------------- 16.3/276.5 MB 1.1 MB/s eta 0:03:57\n",
      "   -- ------------------------------------- 16.8/276.5 MB 1.1 MB/s eta 0:03:52\n",
      "   -- ------------------------------------- 17.0/276.5 MB 1.1 MB/s eta 0:03:51\n",
      "   -- ------------------------------------- 17.6/276.5 MB 1.1 MB/s eta 0:03:47\n",
      "   -- ------------------------------------- 17.8/276.5 MB 1.1 MB/s eta 0:03:46\n",
      "   -- ------------------------------------- 18.4/276.5 MB 1.2 MB/s eta 0:03:43\n",
      "   -- ------------------------------------- 18.6/276.5 MB 1.2 MB/s eta 0:03:42\n",
      "   -- ------------------------------------- 18.9/276.5 MB 1.2 MB/s eta 0:03:42\n",
      "   -- ------------------------------------- 19.4/276.5 MB 1.2 MB/s eta 0:03:38\n",
      "   -- ------------------------------------- 19.9/276.5 MB 1.2 MB/s eta 0:03:35\n",
      "   -- ------------------------------------- 20.2/276.5 MB 1.2 MB/s eta 0:03:34\n",
      "   -- ------------------------------------- 20.7/276.5 MB 1.2 MB/s eta 0:03:32\n",
      "   --- ------------------------------------ 21.0/276.5 MB 1.2 MB/s eta 0:03:31\n",
      "   --- ------------------------------------ 21.5/276.5 MB 1.2 MB/s eta 0:03:28\n",
      "   --- ------------------------------------ 21.8/276.5 MB 1.2 MB/s eta 0:03:27\n",
      "   --- ------------------------------------ 22.3/276.5 MB 1.2 MB/s eta 0:03:25\n",
      "   --- ------------------------------------ 22.5/276.5 MB 1.2 MB/s eta 0:03:24\n",
      "   --- ------------------------------------ 22.8/276.5 MB 1.2 MB/s eta 0:03:24\n",
      "   --- ------------------------------------ 23.1/276.5 MB 1.3 MB/s eta 0:03:22\n",
      "   --- ------------------------------------ 23.6/276.5 MB 1.3 MB/s eta 0:03:21\n",
      "   --- ------------------------------------ 24.1/276.5 MB 1.3 MB/s eta 0:03:19\n",
      "   --- ------------------------------------ 24.4/276.5 MB 1.3 MB/s eta 0:03:18\n",
      "   --- ------------------------------------ 24.9/276.5 MB 1.3 MB/s eta 0:03:16\n",
      "   --- ------------------------------------ 25.2/276.5 MB 1.3 MB/s eta 0:03:15\n",
      "   --- ------------------------------------ 25.4/276.5 MB 1.3 MB/s eta 0:03:14\n",
      "   --- ------------------------------------ 26.0/276.5 MB 1.3 MB/s eta 0:03:13\n",
      "   --- ------------------------------------ 26.2/276.5 MB 1.3 MB/s eta 0:03:13\n",
      "   --- ------------------------------------ 26.7/276.5 MB 1.3 MB/s eta 0:03:11\n",
      "   --- ------------------------------------ 27.3/276.5 MB 1.3 MB/s eta 0:03:09\n",
      "   --- ------------------------------------ 27.5/276.5 MB 1.3 MB/s eta 0:03:08\n",
      "   ---- ----------------------------------- 27.8/276.5 MB 1.3 MB/s eta 0:03:07\n",
      "   ---- ----------------------------------- 28.0/276.5 MB 1.3 MB/s eta 0:03:07\n",
      "   ---- ----------------------------------- 28.3/276.5 MB 1.3 MB/s eta 0:03:07\n",
      "   ---- ----------------------------------- 28.6/276.5 MB 1.3 MB/s eta 0:03:07\n",
      "   ---- ----------------------------------- 29.1/276.5 MB 1.3 MB/s eta 0:03:06\n",
      "   ---- ----------------------------------- 29.4/276.5 MB 1.3 MB/s eta 0:03:06\n",
      "   ---- ----------------------------------- 29.9/276.5 MB 1.3 MB/s eta 0:03:04\n",
      "   ---- ----------------------------------- 30.1/276.5 MB 1.3 MB/s eta 0:03:03\n",
      "   ---- ----------------------------------- 30.7/276.5 MB 1.4 MB/s eta 0:03:02\n",
      "   ---- ----------------------------------- 30.9/276.5 MB 1.4 MB/s eta 0:03:01\n",
      "   ---- ----------------------------------- 31.5/276.5 MB 1.4 MB/s eta 0:03:00\n",
      "   ---- ----------------------------------- 31.7/276.5 MB 1.4 MB/s eta 0:02:59\n",
      "   ---- ----------------------------------- 32.2/276.5 MB 1.4 MB/s eta 0:02:59\n",
      "   ---- ----------------------------------- 32.2/276.5 MB 1.4 MB/s eta 0:02:59\n",
      "   ---- ----------------------------------- 32.5/276.5 MB 1.4 MB/s eta 0:02:59\n",
      "   ---- ----------------------------------- 32.8/276.5 MB 1.4 MB/s eta 0:02:59\n",
      "   ---- ----------------------------------- 33.0/276.5 MB 1.4 MB/s eta 0:02:59\n",
      "   ---- ----------------------------------- 33.3/276.5 MB 1.4 MB/s eta 0:02:59\n",
      "   ---- ----------------------------------- 33.6/276.5 MB 1.4 MB/s eta 0:02:59\n",
      "   ---- ----------------------------------- 34.1/276.5 MB 1.4 MB/s eta 0:02:58\n",
      "   ---- ----------------------------------- 34.1/276.5 MB 1.4 MB/s eta 0:02:58\n",
      "   ----- ---------------------------------- 34.6/276.5 MB 1.4 MB/s eta 0:02:58\n",
      "   ----- ---------------------------------- 34.9/276.5 MB 1.4 MB/s eta 0:02:57\n",
      "   ----- ---------------------------------- 35.4/276.5 MB 1.4 MB/s eta 0:02:56\n",
      "   ----- ---------------------------------- 35.9/276.5 MB 1.4 MB/s eta 0:02:54\n",
      "   ----- ---------------------------------- 36.2/276.5 MB 1.4 MB/s eta 0:02:54\n",
      "   ----- ---------------------------------- 36.4/276.5 MB 1.4 MB/s eta 0:02:54\n",
      "   ----- ---------------------------------- 36.7/276.5 MB 1.4 MB/s eta 0:02:53\n",
      "   ----- ---------------------------------- 37.2/276.5 MB 1.4 MB/s eta 0:02:53\n",
      "   ----- ---------------------------------- 37.5/276.5 MB 1.4 MB/s eta 0:02:52\n",
      "   ----- ---------------------------------- 37.7/276.5 MB 1.4 MB/s eta 0:02:51\n",
      "   ----- ---------------------------------- 38.3/276.5 MB 1.4 MB/s eta 0:02:51\n",
      "   ----- ---------------------------------- 38.5/276.5 MB 1.4 MB/s eta 0:02:50\n",
      "   ----- ---------------------------------- 39.1/276.5 MB 1.4 MB/s eta 0:02:49\n",
      "   ----- ---------------------------------- 39.6/276.5 MB 1.4 MB/s eta 0:02:48\n",
      "   ----- ---------------------------------- 39.8/276.5 MB 1.4 MB/s eta 0:02:48\n",
      "   ----- ---------------------------------- 40.1/276.5 MB 1.4 MB/s eta 0:02:48\n",
      "   ----- ---------------------------------- 40.6/276.5 MB 1.4 MB/s eta 0:02:47\n",
      "   ----- ---------------------------------- 40.6/276.5 MB 1.4 MB/s eta 0:02:47\n",
      "   ----- ---------------------------------- 40.9/276.5 MB 1.4 MB/s eta 0:02:47\n",
      "   ----- ---------------------------------- 41.4/276.5 MB 1.4 MB/s eta 0:02:46\n",
      "   ------ --------------------------------- 41.9/276.5 MB 1.4 MB/s eta 0:02:45\n",
      "   ------ --------------------------------- 42.2/276.5 MB 1.4 MB/s eta 0:02:45\n",
      "   ------ --------------------------------- 42.7/276.5 MB 1.4 MB/s eta 0:02:44\n",
      "   ------ --------------------------------- 43.0/276.5 MB 1.4 MB/s eta 0:02:44\n",
      "   ------ --------------------------------- 43.3/276.5 MB 1.4 MB/s eta 0:02:44\n",
      "   ------ --------------------------------- 43.5/276.5 MB 1.4 MB/s eta 0:02:45\n",
      "   ------ --------------------------------- 43.8/276.5 MB 1.4 MB/s eta 0:02:45\n",
      "   ------ --------------------------------- 44.0/276.5 MB 1.4 MB/s eta 0:02:45\n",
      "   ------ --------------------------------- 44.6/276.5 MB 1.4 MB/s eta 0:02:44\n",
      "   ------ --------------------------------- 44.8/276.5 MB 1.4 MB/s eta 0:02:44\n",
      "   ------ --------------------------------- 45.1/276.5 MB 1.4 MB/s eta 0:02:45\n",
      "   ------ --------------------------------- 45.1/276.5 MB 1.4 MB/s eta 0:02:45\n",
      "   ------ --------------------------------- 45.4/276.5 MB 1.4 MB/s eta 0:02:42\n",
      "   ------ --------------------------------- 45.9/276.5 MB 1.4 MB/s eta 0:02:42\n",
      "   ------ --------------------------------- 46.1/276.5 MB 1.5 MB/s eta 0:02:38\n",
      "   ------ --------------------------------- 46.7/276.5 MB 1.5 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 46.9/276.5 MB 1.5 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 47.2/276.5 MB 1.5 MB/s eta 0:02:37\n",
      "   ------ --------------------------------- 47.4/276.5 MB 1.5 MB/s eta 0:02:36\n",
      "   ------ --------------------------------- 47.7/276.5 MB 1.5 MB/s eta 0:02:33\n",
      "   ------ --------------------------------- 48.0/276.5 MB 1.5 MB/s eta 0:02:33\n",
      "   ------ --------------------------------- 48.2/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 48.8/276.5 MB 1.5 MB/s eta 0:02:29\n",
      "   ------- -------------------------------- 49.0/276.5 MB 1.5 MB/s eta 0:02:29\n",
      "   ------- -------------------------------- 49.3/276.5 MB 1.5 MB/s eta 0:02:28\n",
      "   ------- -------------------------------- 49.8/276.5 MB 1.5 MB/s eta 0:02:28\n",
      "   ------- -------------------------------- 50.1/276.5 MB 1.5 MB/s eta 0:02:28\n",
      "   ------- -------------------------------- 50.3/276.5 MB 1.6 MB/s eta 0:02:25\n",
      "   ------- -------------------------------- 50.6/276.5 MB 1.6 MB/s eta 0:02:24\n",
      "   ------- -------------------------------- 50.9/276.5 MB 1.6 MB/s eta 0:02:24\n",
      "   ------- -------------------------------- 50.9/276.5 MB 1.6 MB/s eta 0:02:24\n",
      "   ------- -------------------------------- 51.1/276.5 MB 1.6 MB/s eta 0:02:23\n",
      "   ------- -------------------------------- 51.4/276.5 MB 1.6 MB/s eta 0:02:24\n",
      "   ------- -------------------------------- 51.4/276.5 MB 1.6 MB/s eta 0:02:24\n",
      "   ------- -------------------------------- 51.4/276.5 MB 1.6 MB/s eta 0:02:24\n",
      "   ------- -------------------------------- 51.4/276.5 MB 1.6 MB/s eta 0:02:24\n",
      "   ------- -------------------------------- 51.4/276.5 MB 1.6 MB/s eta 0:02:24\n",
      "   ------- -------------------------------- 51.6/276.5 MB 1.6 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 51.6/276.5 MB 1.6 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 51.6/276.5 MB 1.6 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 51.6/276.5 MB 1.6 MB/s eta 0:02:22\n",
      "   ------- -------------------------------- 51.9/276.5 MB 1.6 MB/s eta 0:02:23\n",
      "   ------- -------------------------------- 51.9/276.5 MB 1.6 MB/s eta 0:02:23\n",
      "   ------- -------------------------------- 51.9/276.5 MB 1.6 MB/s eta 0:02:23\n",
      "   ------- -------------------------------- 51.9/276.5 MB 1.6 MB/s eta 0:02:23\n",
      "   ------- -------------------------------- 51.9/276.5 MB 1.6 MB/s eta 0:02:23\n",
      "   ------- -------------------------------- 52.2/276.5 MB 1.5 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 52.2/276.5 MB 1.5 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 52.2/276.5 MB 1.5 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 52.2/276.5 MB 1.5 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 52.2/276.5 MB 1.5 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 52.4/276.5 MB 1.5 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 52.7/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 53.0/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 53.2/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 53.5/276.5 MB 1.5 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 53.7/276.5 MB 1.5 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 54.3/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 54.5/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 54.8/276.5 MB 1.5 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 55.1/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   -------- ------------------------------- 55.6/276.5 MB 1.5 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 55.8/276.5 MB 1.5 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 56.1/276.5 MB 1.5 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 56.6/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   -------- ------------------------------- 56.9/276.5 MB 1.5 MB/s eta 0:02:32\n",
      "   -------- ------------------------------- 57.4/276.5 MB 1.5 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 57.4/276.5 MB 1.5 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 57.9/276.5 MB 1.5 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 58.2/276.5 MB 1.5 MB/s eta 0:02:30\n",
      "   -------- ------------------------------- 58.5/276.5 MB 1.5 MB/s eta 0:02:30\n",
      "   -------- ------------------------------- 58.7/276.5 MB 1.5 MB/s eta 0:02:30\n",
      "   -------- ------------------------------- 59.0/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 59.2/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 59.8/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 60.0/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 60.6/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   -------- ------------------------------- 60.6/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   -------- ------------------------------- 60.8/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   -------- ------------------------------- 61.1/276.5 MB 1.4 MB/s eta 0:02:32\n",
      "   -------- ------------------------------- 61.3/276.5 MB 1.4 MB/s eta 0:02:32\n",
      "   -------- ------------------------------- 61.6/276.5 MB 1.4 MB/s eta 0:02:32\n",
      "   -------- ------------------------------- 61.9/276.5 MB 1.4 MB/s eta 0:02:33\n",
      "   --------- ------------------------------ 62.4/276.5 MB 1.4 MB/s eta 0:02:32\n",
      "   --------- ------------------------------ 62.7/276.5 MB 1.4 MB/s eta 0:02:32\n",
      "   --------- ------------------------------ 63.2/276.5 MB 1.4 MB/s eta 0:02:32\n",
      "   --------- ------------------------------ 63.4/276.5 MB 1.4 MB/s eta 0:02:32\n",
      "   --------- ------------------------------ 63.7/276.5 MB 1.4 MB/s eta 0:02:32\n",
      "   --------- ------------------------------ 64.2/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 64.5/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 65.0/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 65.3/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 65.5/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   --------- ------------------------------ 66.1/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   --------- ------------------------------ 66.3/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   --------- ------------------------------ 66.6/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 66.8/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 67.1/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 67.4/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 67.6/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 67.9/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 68.4/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   --------- ------------------------------ 68.7/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   ---------- ----------------------------- 69.2/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   ---------- ----------------------------- 69.5/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   ---------- ----------------------------- 69.7/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   ---------- ----------------------------- 70.3/276.5 MB 1.4 MB/s eta 0:02:29\n",
      "   ---------- ----------------------------- 70.5/276.5 MB 1.4 MB/s eta 0:02:28\n",
      "   ---------- ----------------------------- 70.8/276.5 MB 1.4 MB/s eta 0:02:28\n",
      "   ---------- ----------------------------- 71.0/276.5 MB 1.4 MB/s eta 0:02:29\n",
      "   ---------- ----------------------------- 71.3/276.5 MB 1.4 MB/s eta 0:02:29\n",
      "   ---------- ----------------------------- 71.6/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   ---------- ----------------------------- 71.8/276.5 MB 1.4 MB/s eta 0:02:31\n",
      "   ---------- ----------------------------- 72.1/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   ---------- ----------------------------- 72.6/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   ---------- ----------------------------- 72.9/276.5 MB 1.4 MB/s eta 0:02:30\n",
      "   ---------- ----------------------------- 73.4/276.5 MB 1.4 MB/s eta 0:02:28\n",
      "   ---------- ----------------------------- 73.4/276.5 MB 1.4 MB/s eta 0:02:28\n",
      "   ---------- ----------------------------- 73.7/276.5 MB 1.4 MB/s eta 0:02:28\n",
      "   ---------- ----------------------------- 73.9/276.5 MB 1.4 MB/s eta 0:02:28\n",
      "   ---------- ----------------------------- 74.2/276.5 MB 1.4 MB/s eta 0:02:28\n",
      "   ---------- ----------------------------- 74.7/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ---------- ----------------------------- 75.0/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ---------- ----------------------------- 75.2/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ---------- ----------------------------- 75.5/276.5 MB 1.4 MB/s eta 0:02:26\n",
      "   ---------- ----------------------------- 76.0/276.5 MB 1.4 MB/s eta 0:02:26\n",
      "   ----------- ---------------------------- 76.3/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 76.5/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 76.8/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 77.1/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 77.3/276.5 MB 1.4 MB/s eta 0:02:26\n",
      "   ----------- ---------------------------- 77.6/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 77.9/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 78.1/276.5 MB 1.3 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 78.6/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 78.9/276.5 MB 1.4 MB/s eta 0:02:26\n",
      "   ----------- ---------------------------- 79.2/276.5 MB 1.4 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 79.7/276.5 MB 1.3 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 80.0/276.5 MB 1.3 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 80.2/276.5 MB 1.3 MB/s eta 0:02:26\n",
      "   ----------- ---------------------------- 80.5/276.5 MB 1.3 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 80.7/276.5 MB 1.3 MB/s eta 0:02:27\n",
      "   ----------- ---------------------------- 81.0/276.5 MB 1.3 MB/s eta 0:02:26\n",
      "   ----------- ---------------------------- 81.5/276.5 MB 1.3 MB/s eta 0:02:25\n",
      "   ----------- ---------------------------- 81.8/276.5 MB 1.3 MB/s eta 0:02:26\n",
      "   ----------- ---------------------------- 82.3/276.5 MB 1.3 MB/s eta 0:02:26\n",
      "   ----------- ---------------------------- 82.6/276.5 MB 1.3 MB/s eta 0:02:26\n",
      "   ----------- ---------------------------- 82.8/276.5 MB 1.3 MB/s eta 0:02:26\n",
      "   ------------ --------------------------- 83.1/276.5 MB 1.3 MB/s eta 0:02:25\n",
      "   ------------ --------------------------- 83.4/276.5 MB 1.3 MB/s eta 0:02:25\n",
      "   ------------ --------------------------- 83.9/276.5 MB 1.3 MB/s eta 0:02:24\n",
      "   ------------ --------------------------- 84.1/276.5 MB 1.3 MB/s eta 0:02:24\n",
      "   ------------ --------------------------- 84.7/276.5 MB 1.3 MB/s eta 0:02:23\n",
      "   ------------ --------------------------- 84.9/276.5 MB 1.3 MB/s eta 0:02:23\n",
      "   ------------ --------------------------- 85.5/276.5 MB 1.4 MB/s eta 0:02:22\n",
      "   ------------ --------------------------- 85.7/276.5 MB 1.4 MB/s eta 0:02:21\n",
      "   ------------ --------------------------- 86.0/276.5 MB 1.4 MB/s eta 0:02:21\n",
      "   ------------ --------------------------- 86.2/276.5 MB 1.4 MB/s eta 0:02:21\n",
      "   ------------ --------------------------- 86.8/276.5 MB 1.4 MB/s eta 0:02:20\n",
      "   ------------ --------------------------- 87.0/276.5 MB 1.4 MB/s eta 0:02:20\n",
      "   ------------ --------------------------- 87.3/276.5 MB 1.4 MB/s eta 0:02:19\n",
      "   ------------ --------------------------- 87.8/276.5 MB 1.4 MB/s eta 0:02:19\n",
      "   ------------ --------------------------- 88.1/276.5 MB 1.4 MB/s eta 0:02:19\n",
      "   ------------ --------------------------- 88.3/276.5 MB 1.4 MB/s eta 0:02:19\n",
      "   ------------ --------------------------- 88.9/276.5 MB 1.4 MB/s eta 0:02:18\n",
      "   ------------ --------------------------- 89.1/276.5 MB 1.4 MB/s eta 0:02:18\n",
      "   ------------ --------------------------- 89.4/276.5 MB 1.4 MB/s eta 0:02:17\n",
      "   ------------ --------------------------- 89.7/276.5 MB 1.4 MB/s eta 0:02:17\n",
      "   ------------- -------------------------- 89.9/276.5 MB 1.4 MB/s eta 0:02:17\n",
      "   ------------- -------------------------- 90.4/276.5 MB 1.4 MB/s eta 0:02:17\n",
      "   ------------- -------------------------- 91.0/276.5 MB 1.4 MB/s eta 0:02:16\n",
      "   ------------- -------------------------- 91.2/276.5 MB 1.4 MB/s eta 0:02:16\n",
      "   ------------- -------------------------- 91.5/276.5 MB 1.4 MB/s eta 0:02:15\n",
      "   ------------- -------------------------- 91.8/276.5 MB 1.4 MB/s eta 0:02:15\n",
      "   ------------- -------------------------- 92.0/276.5 MB 1.4 MB/s eta 0:02:15\n",
      "   ------------- -------------------------- 92.3/276.5 MB 1.4 MB/s eta 0:02:14\n",
      "   ------------- -------------------------- 92.5/276.5 MB 1.4 MB/s eta 0:02:14\n",
      "   ------------- -------------------------- 92.8/276.5 MB 1.4 MB/s eta 0:02:13\n",
      "   ------------- -------------------------- 93.3/276.5 MB 1.4 MB/s eta 0:02:09\n",
      "   ------------- -------------------------- 93.8/276.5 MB 1.4 MB/s eta 0:02:08\n",
      "   ------------- -------------------------- 94.1/276.5 MB 1.4 MB/s eta 0:02:08\n",
      "   ------------- -------------------------- 94.4/276.5 MB 1.4 MB/s eta 0:02:08\n",
      "   ------------- -------------------------- 94.9/276.5 MB 1.5 MB/s eta 0:02:04\n",
      "   ------------- -------------------------- 95.2/276.5 MB 1.5 MB/s eta 0:02:04\n",
      "   ------------- -------------------------- 95.2/276.5 MB 1.5 MB/s eta 0:02:04\n",
      "   ------------- -------------------------- 95.7/276.5 MB 1.5 MB/s eta 0:02:04\n",
      "   ------------- -------------------------- 95.9/276.5 MB 1.5 MB/s eta 0:02:01\n",
      "   ------------- -------------------------- 96.2/276.5 MB 1.5 MB/s eta 0:02:01\n",
      "   ------------- -------------------------- 96.5/276.5 MB 1.5 MB/s eta 0:02:00\n",
      "   -------------- ------------------------- 97.0/276.5 MB 1.5 MB/s eta 0:02:00\n",
      "   -------------- ------------------------- 97.3/276.5 MB 1.5 MB/s eta 0:02:00\n",
      "   -------------- ------------------------- 97.8/276.5 MB 1.6 MB/s eta 0:01:55\n",
      "   -------------- ------------------------- 98.0/276.5 MB 1.6 MB/s eta 0:01:55\n",
      "   -------------- ------------------------- 98.3/276.5 MB 1.6 MB/s eta 0:01:55\n",
      "   -------------- ------------------------- 98.6/276.5 MB 1.6 MB/s eta 0:01:55\n",
      "   -------------- ------------------------- 98.8/276.5 MB 1.6 MB/s eta 0:01:55\n",
      "   -------------- ------------------------- 99.1/276.5 MB 1.6 MB/s eta 0:01:54\n",
      "   -------------- ------------------------- 99.1/276.5 MB 1.6 MB/s eta 0:01:54\n",
      "   -------------- ------------------------- 99.1/276.5 MB 1.6 MB/s eta 0:01:54\n",
      "   -------------- ------------------------- 99.1/276.5 MB 1.6 MB/s eta 0:01:54\n",
      "   -------------- ------------------------- 99.1/276.5 MB 1.6 MB/s eta 0:01:54\n",
      "   -------------- ------------------------- 99.4/276.5 MB 1.5 MB/s eta 0:01:57\n",
      "   -------------- ------------------------- 99.4/276.5 MB 1.5 MB/s eta 0:01:57\n",
      "   -------------- ------------------------- 99.4/276.5 MB 1.5 MB/s eta 0:01:57\n",
      "   -------------- ------------------------- 99.4/276.5 MB 1.5 MB/s eta 0:01:57\n",
      "   -------------- ------------------------- 99.6/276.5 MB 1.5 MB/s eta 0:02:00\n",
      "   -------------- ------------------------- 99.6/276.5 MB 1.5 MB/s eta 0:02:00\n",
      "   -------------- ------------------------- 99.6/276.5 MB 1.5 MB/s eta 0:02:00\n",
      "   -------------- ------------------------- 99.6/276.5 MB 1.5 MB/s eta 0:02:00\n",
      "   -------------- ------------------------- 99.9/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 99.9/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 99.9/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 99.9/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 99.9/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 100.1/276.5 MB 1.4 MB/s eta 0:02:07\n",
      "   -------------- ------------------------- 100.4/276.5 MB 1.4 MB/s eta 0:02:07\n",
      "   -------------- ------------------------- 100.7/276.5 MB 1.4 MB/s eta 0:02:07\n",
      "   -------------- ------------------------- 100.9/276.5 MB 1.4 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 101.4/276.5 MB 1.4 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 101.7/276.5 MB 1.4 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 102.2/276.5 MB 1.4 MB/s eta 0:02:06\n",
      "   -------------- ------------------------- 102.5/276.5 MB 1.4 MB/s eta 0:02:04\n",
      "   -------------- ------------------------- 102.8/276.5 MB 1.4 MB/s eta 0:02:04\n",
      "   -------------- ------------------------- 103.3/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   -------------- ------------------------- 103.3/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   --------------- ------------------------ 103.8/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   --------------- ------------------------ 104.3/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 104.6/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 104.9/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 105.1/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 105.4/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 105.6/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 105.9/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 106.4/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 106.7/276.5 MB 1.4 MB/s eta 0:02:01\n",
      "   --------------- ------------------------ 107.0/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 107.0/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 107.2/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 107.5/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   --------------- ------------------------ 107.7/276.5 MB 1.4 MB/s eta 0:02:03\n",
      "   --------------- ------------------------ 108.3/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   --------------- ------------------------ 108.5/276.5 MB 1.4 MB/s eta 0:02:01\n",
      "   --------------- ------------------------ 109.1/276.5 MB 1.4 MB/s eta 0:02:00\n",
      "   --------------- ------------------------ 109.3/276.5 MB 1.4 MB/s eta 0:02:00\n",
      "   --------------- ------------------------ 109.6/276.5 MB 1.4 MB/s eta 0:02:00\n",
      "   --------------- ------------------------ 110.1/276.5 MB 1.4 MB/s eta 0:01:59\n",
      "   --------------- ------------------------ 110.1/276.5 MB 1.4 MB/s eta 0:01:59\n",
      "   --------------- ------------------------ 110.4/276.5 MB 1.4 MB/s eta 0:02:00\n",
      "   --------------- ------------------------ 110.4/276.5 MB 1.4 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 110.6/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   ---------------- ----------------------- 110.9/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   ---------------- ----------------------- 111.1/276.5 MB 1.4 MB/s eta 0:02:02\n",
      "   ---------------- ----------------------- 111.7/276.5 MB 1.4 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 112.2/276.5 MB 1.4 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 112.2/276.5 MB 1.4 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 112.7/276.5 MB 1.4 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 113.0/276.5 MB 1.4 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 113.2/276.5 MB 1.4 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 113.2/276.5 MB 1.4 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 113.2/276.5 MB 1.4 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 113.5/276.5 MB 1.4 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 113.8/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 114.0/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 114.3/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 114.6/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 114.8/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 115.1/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 115.3/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 115.6/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 115.9/276.5 MB 1.3 MB/s eta 0:02:01\n",
      "   ---------------- ----------------------- 116.4/276.5 MB 1.3 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 116.7/276.5 MB 1.3 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 116.9/276.5 MB 1.3 MB/s eta 0:02:00\n",
      "   ---------------- ----------------------- 117.2/276.5 MB 1.3 MB/s eta 0:01:59\n",
      "   ---------------- ----------------------- 117.4/276.5 MB 1.3 MB/s eta 0:01:59\n",
      "   ----------------- ---------------------- 117.7/276.5 MB 1.3 MB/s eta 0:01:59\n",
      "   ----------------- ---------------------- 118.0/276.5 MB 1.3 MB/s eta 0:01:59\n",
      "   ----------------- ---------------------- 118.2/276.5 MB 1.3 MB/s eta 0:01:58\n",
      "   ----------------- ---------------------- 118.8/276.5 MB 1.3 MB/s eta 0:01:58\n",
      "   ----------------- ---------------------- 119.0/276.5 MB 1.3 MB/s eta 0:01:57\n",
      "   ----------------- ---------------------- 119.3/276.5 MB 1.3 MB/s eta 0:01:58\n",
      "   ----------------- ---------------------- 119.5/276.5 MB 1.3 MB/s eta 0:01:58\n",
      "   ----------------- ---------------------- 119.8/276.5 MB 1.3 MB/s eta 0:01:58\n",
      "   ----------------- ---------------------- 120.1/276.5 MB 1.3 MB/s eta 0:01:58\n",
      "   ----------------- ---------------------- 120.3/276.5 MB 1.3 MB/s eta 0:01:58\n",
      "   ----------------- ---------------------- 120.8/276.5 MB 1.3 MB/s eta 0:01:57\n",
      "   ----------------- ---------------------- 121.1/276.5 MB 1.3 MB/s eta 0:01:56\n",
      "   ----------------- ---------------------- 121.4/276.5 MB 1.3 MB/s eta 0:01:56\n",
      "   ----------------- ---------------------- 121.9/276.5 MB 1.3 MB/s eta 0:01:56\n",
      "   ----------------- ---------------------- 122.2/276.5 MB 1.3 MB/s eta 0:01:56\n",
      "   ----------------- ---------------------- 122.4/276.5 MB 1.3 MB/s eta 0:01:56\n",
      "   ----------------- ---------------------- 122.4/276.5 MB 1.3 MB/s eta 0:01:56\n",
      "   ----------------- ---------------------- 122.7/276.5 MB 1.3 MB/s eta 0:01:57\n",
      "   ----------------- ---------------------- 122.7/276.5 MB 1.3 MB/s eta 0:01:57\n",
      "   ----------------- ---------------------- 122.7/276.5 MB 1.3 MB/s eta 0:01:57\n",
      "   ----------------- ---------------------- 122.7/276.5 MB 1.3 MB/s eta 0:01:57\n",
      "   ----------------- ---------------------- 122.7/276.5 MB 1.3 MB/s eta 0:01:57\n",
      "   ----------------- ---------------------- 122.7/276.5 MB 1.3 MB/s eta 0:01:57\n",
      "   ----------------- ---------------------- 122.9/276.5 MB 1.3 MB/s eta 0:02:02\n",
      "   ----------------- ---------------------- 122.9/276.5 MB 1.3 MB/s eta 0:02:02\n",
      "   ----------------- ---------------------- 122.9/276.5 MB 1.3 MB/s eta 0:02:02\n",
      "   ----------------- ---------------------- 123.2/276.5 MB 1.2 MB/s eta 0:02:04\n",
      "   ----------------- ---------------------- 123.2/276.5 MB 1.2 MB/s eta 0:02:04\n",
      "   ----------------- ---------------------- 123.2/276.5 MB 1.2 MB/s eta 0:02:04\n",
      "   ----------------- ---------------------- 123.2/276.5 MB 1.2 MB/s eta 0:02:04\n",
      "   ----------------- ---------------------- 123.2/276.5 MB 1.2 MB/s eta 0:02:04\n",
      "   ----------------- ---------------------- 123.2/276.5 MB 1.2 MB/s eta 0:02:04\n",
      "   ----------------- ---------------------- 123.5/276.5 MB 1.2 MB/s eta 0:02:10\n",
      "   ----------------- ---------------------- 123.5/276.5 MB 1.2 MB/s eta 0:02:10\n",
      "   ----------------- ---------------------- 123.5/276.5 MB 1.2 MB/s eta 0:02:10\n",
      "   ----------------- ---------------------- 123.5/276.5 MB 1.2 MB/s eta 0:02:10\n",
      "   ----------------- ---------------------- 123.7/276.5 MB 1.1 MB/s eta 0:02:15\n",
      "   ----------------- ---------------------- 123.7/276.5 MB 1.1 MB/s eta 0:02:15\n",
      "   ----------------- ---------------------- 123.7/276.5 MB 1.1 MB/s eta 0:02:15\n",
      "   ----------------- ---------------------- 123.7/276.5 MB 1.1 MB/s eta 0:02:15\n",
      "   ----------------- ---------------------- 124.0/276.5 MB 1.1 MB/s eta 0:02:19\n",
      "   ----------------- ---------------------- 124.0/276.5 MB 1.1 MB/s eta 0:02:19\n",
      "   ----------------- ---------------------- 124.0/276.5 MB 1.1 MB/s eta 0:02:19\n",
      "   ----------------- ---------------------- 124.0/276.5 MB 1.1 MB/s eta 0:02:19\n",
      "   ----------------- ---------------------- 124.3/276.5 MB 1.1 MB/s eta 0:02:23\n",
      "   ----------------- ---------------------- 124.3/276.5 MB 1.1 MB/s eta 0:02:23\n",
      "   ----------------- ---------------------- 124.3/276.5 MB 1.1 MB/s eta 0:02:23\n",
      "   ----------------- ---------------------- 124.3/276.5 MB 1.1 MB/s eta 0:02:23\n",
      "   ----------------- ---------------------- 124.3/276.5 MB 1.1 MB/s eta 0:02:23\n",
      "   ------------------ --------------------- 124.5/276.5 MB 1.0 MB/s eta 0:02:29\n",
      "   ------------------ --------------------- 124.5/276.5 MB 1.0 MB/s eta 0:02:29\n",
      "   ------------------ --------------------- 124.5/276.5 MB 1.0 MB/s eta 0:02:29\n",
      "   ----------------- -------------------- 124.8/276.5 MB 993.6 kB/s eta 0:02:33\n",
      "   ----------------- -------------------- 124.8/276.5 MB 993.6 kB/s eta 0:02:33\n",
      "   ----------------- -------------------- 124.8/276.5 MB 993.6 kB/s eta 0:02:33\n",
      "   ----------------- -------------------- 124.8/276.5 MB 993.6 kB/s eta 0:02:33\n",
      "   ----------------- -------------------- 125.0/276.5 MB 974.5 kB/s eta 0:02:36\n",
      "   ----------------- -------------------- 125.0/276.5 MB 974.5 kB/s eta 0:02:36\n",
      "   ----------------- -------------------- 125.0/276.5 MB 974.5 kB/s eta 0:02:36\n",
      "   ----------------- -------------------- 125.3/276.5 MB 945.7 kB/s eta 0:02:40\n",
      "   ----------------- -------------------- 125.3/276.5 MB 945.7 kB/s eta 0:02:40\n",
      "   ----------------- -------------------- 125.6/276.5 MB 926.2 kB/s eta 0:02:43\n",
      "   ----------------- -------------------- 125.8/276.5 MB 919.9 kB/s eta 0:02:44\n",
      "   ----------------- -------------------- 126.1/276.5 MB 918.9 kB/s eta 0:02:44\n",
      "   ----------------- -------------------- 126.4/276.5 MB 920.9 kB/s eta 0:02:44\n",
      "   ----------------- -------------------- 126.6/276.5 MB 919.4 kB/s eta 0:02:44\n",
      "   ----------------- -------------------- 126.9/276.5 MB 947.1 kB/s eta 0:02:39\n",
      "   ----------------- -------------------- 127.4/276.5 MB 956.9 kB/s eta 0:02:36\n",
      "   ----------------- -------------------- 127.7/276.5 MB 960.2 kB/s eta 0:02:36\n",
      "   ----------------- -------------------- 127.9/276.5 MB 960.5 kB/s eta 0:02:35\n",
      "   ----------------- -------------------- 128.2/276.5 MB 961.7 kB/s eta 0:02:35\n",
      "   ----------------- -------------------- 128.5/276.5 MB 985.8 kB/s eta 0:02:31\n",
      "   ----------------- -------------------- 128.7/276.5 MB 989.0 kB/s eta 0:02:30\n",
      "   ----------------- -------------------- 128.7/276.5 MB 989.0 kB/s eta 0:02:30\n",
      "   ----------------- -------------------- 129.0/276.5 MB 982.3 kB/s eta 0:02:31\n",
      "   ----------------- -------------------- 129.0/276.5 MB 982.3 kB/s eta 0:02:31\n",
      "   ------------------ --------------------- 129.5/276.5 MB 1.0 MB/s eta 0:02:26\n",
      "   ------------------ --------------------- 129.8/276.5 MB 1.0 MB/s eta 0:02:25\n",
      "   ------------------ --------------------- 130.3/276.5 MB 1.0 MB/s eta 0:02:24\n",
      "   ------------------ --------------------- 130.5/276.5 MB 1.0 MB/s eta 0:02:23\n",
      "   ------------------ --------------------- 130.8/276.5 MB 1.0 MB/s eta 0:02:19\n",
      "   ------------------ --------------------- 131.1/276.5 MB 1.1 MB/s eta 0:02:19\n",
      "   ------------------ --------------------- 131.3/276.5 MB 1.1 MB/s eta 0:02:18\n",
      "   ------------------- -------------------- 131.6/276.5 MB 1.1 MB/s eta 0:02:18\n",
      "   ------------------- -------------------- 132.1/276.5 MB 1.1 MB/s eta 0:02:16\n",
      "   ------------------- -------------------- 132.6/276.5 MB 1.1 MB/s eta 0:02:15\n",
      "   ------------------- -------------------- 132.6/276.5 MB 1.1 MB/s eta 0:02:15\n",
      "   ------------------- -------------------- 132.6/276.5 MB 1.1 MB/s eta 0:02:15\n",
      "   ------------------- -------------------- 132.6/276.5 MB 1.1 MB/s eta 0:02:15\n",
      "   ------------------- -------------------- 132.9/276.5 MB 1.0 MB/s eta 0:02:18\n",
      "   ------------------- -------------------- 132.9/276.5 MB 1.0 MB/s eta 0:02:18\n",
      "   ------------------- -------------------- 132.9/276.5 MB 1.0 MB/s eta 0:02:18\n",
      "   ------------------- -------------------- 132.9/276.5 MB 1.0 MB/s eta 0:02:18\n",
      "   ------------------- -------------------- 132.9/276.5 MB 1.0 MB/s eta 0:02:18\n",
      "   ------------------- -------------------- 132.9/276.5 MB 1.0 MB/s eta 0:02:18\n",
      "   ------------------ ------------------- 133.2/276.5 MB 978.7 kB/s eta 0:02:27\n",
      "   ------------------ ------------------- 133.2/276.5 MB 978.7 kB/s eta 0:02:27\n",
      "   ------------------ ------------------- 133.2/276.5 MB 978.7 kB/s eta 0:02:27\n",
      "   ------------------ ------------------- 133.2/276.5 MB 978.7 kB/s eta 0:02:27\n",
      "   ------------------ ------------------- 133.2/276.5 MB 978.7 kB/s eta 0:02:27\n",
      "   ------------------ ------------------- 133.2/276.5 MB 978.7 kB/s eta 0:02:27\n",
      "   ------------------ ------------------- 133.4/276.5 MB 920.4 kB/s eta 0:02:36\n",
      "   ------------------ ------------------- 133.4/276.5 MB 920.4 kB/s eta 0:02:36\n",
      "   ------------------ ------------------- 133.4/276.5 MB 920.4 kB/s eta 0:02:36\n",
      "   ------------------ ------------------- 133.4/276.5 MB 920.4 kB/s eta 0:02:36\n",
      "   ------------------ ------------------- 133.7/276.5 MB 889.0 kB/s eta 0:02:41\n",
      "   ------------------ ------------------- 134.0/276.5 MB 891.8 kB/s eta 0:02:40\n",
      "   ------------------ ------------------- 134.5/276.5 MB 900.5 kB/s eta 0:02:38\n",
      "   ------------------ ------------------- 134.7/276.5 MB 902.9 kB/s eta 0:02:38\n",
      "   ------------------ ------------------- 135.3/276.5 MB 901.9 kB/s eta 0:02:37\n",
      "   ------------------ ------------------- 135.5/276.5 MB 902.8 kB/s eta 0:02:37\n",
      "   ------------------ ------------------- 135.8/276.5 MB 901.9 kB/s eta 0:02:37\n",
      "   ------------------ ------------------- 136.1/276.5 MB 900.5 kB/s eta 0:02:36\n",
      "   ------------------ ------------------- 136.6/276.5 MB 901.4 kB/s eta 0:02:36\n",
      "   ------------------ ------------------- 136.8/276.5 MB 895.0 kB/s eta 0:02:37\n",
      "   ------------------ ------------------- 137.1/276.5 MB 900.0 kB/s eta 0:02:35\n",
      "   ------------------ ------------------- 137.4/276.5 MB 901.9 kB/s eta 0:02:35\n",
      "   ------------------ ------------------- 137.9/276.5 MB 915.9 kB/s eta 0:02:32\n",
      "   ------------------ ------------------- 138.1/276.5 MB 918.0 kB/s eta 0:02:31\n",
      "   ------------------- ------------------ 138.4/276.5 MB 920.4 kB/s eta 0:02:31\n",
      "   ------------------- ------------------ 138.9/276.5 MB 926.7 kB/s eta 0:02:29\n",
      "   ------------------- ------------------ 139.2/276.5 MB 920.9 kB/s eta 0:02:30\n",
      "   ------------------- ------------------ 139.5/276.5 MB 919.4 kB/s eta 0:02:30\n",
      "   ------------------- ------------------ 139.7/276.5 MB 914.0 kB/s eta 0:02:30\n",
      "   ------------------- ------------------ 140.0/276.5 MB 918.5 kB/s eta 0:02:29\n",
      "   ------------------- ------------------ 140.2/276.5 MB 913.5 kB/s eta 0:02:30\n",
      "   ------------------- ------------------ 140.8/276.5 MB 920.4 kB/s eta 0:02:28\n",
      "   ------------------- ------------------ 141.3/276.5 MB 936.5 kB/s eta 0:02:25\n",
      "   ------------------- ------------------ 141.6/276.5 MB 940.8 kB/s eta 0:02:24\n",
      "   ------------------- ------------------ 141.8/276.5 MB 944.3 kB/s eta 0:02:23\n",
      "   ------------------- ------------------ 142.1/276.5 MB 944.7 kB/s eta 0:02:23\n",
      "   ------------------- ------------------ 142.1/276.5 MB 944.7 kB/s eta 0:02:23\n",
      "   ------------------- ------------------ 142.6/276.5 MB 945.2 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 142.9/276.5 MB 946.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 143.4/276.5 MB 954.0 kB/s eta 0:02:20\n",
      "   ------------------- ------------------ 143.7/276.5 MB 957.0 kB/s eta 0:02:19\n",
      "   ------------------- ------------------ 143.9/276.5 MB 957.9 kB/s eta 0:02:19\n",
      "   ------------------- ------------------ 144.2/276.5 MB 958.9 kB/s eta 0:02:19\n",
      "   ------------------- ------------------ 144.7/276.5 MB 963.7 kB/s eta 0:02:17\n",
      "   ------------------- ------------------ 145.0/276.5 MB 962.7 kB/s eta 0:02:17\n",
      "   ------------------- ------------------ 145.0/276.5 MB 962.7 kB/s eta 0:02:17\n",
      "   ------------------- ------------------ 145.0/276.5 MB 962.7 kB/s eta 0:02:17\n",
      "   ------------------- ------------------ 145.0/276.5 MB 962.7 kB/s eta 0:02:17\n",
      "   ------------------- ------------------ 145.2/276.5 MB 928.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 145.2/276.5 MB 928.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 145.2/276.5 MB 928.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 145.2/276.5 MB 928.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 145.2/276.5 MB 928.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 145.2/276.5 MB 928.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 145.2/276.5 MB 928.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 145.2/276.5 MB 928.7 kB/s eta 0:02:22\n",
      "   ------------------- ------------------ 145.5/276.5 MB 861.2 kB/s eta 0:02:33\n",
      "   ------------------- ------------------ 145.5/276.5 MB 861.2 kB/s eta 0:02:33\n",
      "   -------------------- ----------------- 145.8/276.5 MB 851.6 kB/s eta 0:02:34\n",
      "   -------------------- ----------------- 146.0/276.5 MB 849.8 kB/s eta 0:02:34\n",
      "   -------------------- ----------------- 146.3/276.5 MB 841.5 kB/s eta 0:02:35\n",
      "   -------------------- ----------------- 146.5/276.5 MB 840.2 kB/s eta 0:02:35\n",
      "   -------------------- ----------------- 146.8/276.5 MB 841.0 kB/s eta 0:02:35\n",
      "   -------------------- ----------------- 147.3/276.5 MB 840.2 kB/s eta 0:02:34\n",
      "   -------------------- ----------------- 147.6/276.5 MB 841.0 kB/s eta 0:02:34\n",
      "   -------------------- ----------------- 148.1/276.5 MB 857.0 kB/s eta 0:02:30\n",
      "   -------------------- ----------------- 148.4/276.5 MB 861.7 kB/s eta 0:02:29\n",
      "   -------------------- ----------------- 148.6/276.5 MB 889.7 kB/s eta 0:02:24\n",
      "   -------------------- ----------------- 148.9/276.5 MB 892.0 kB/s eta 0:02:24\n",
      "   -------------------- ----------------- 149.2/276.5 MB 896.2 kB/s eta 0:02:23\n",
      "   -------------------- ----------------- 149.4/276.5 MB 898.5 kB/s eta 0:02:22\n",
      "   -------------------- ----------------- 149.4/276.5 MB 898.5 kB/s eta 0:02:22\n",
      "   -------------------- ----------------- 149.4/276.5 MB 898.5 kB/s eta 0:02:22\n",
      "   -------------------- ----------------- 149.4/276.5 MB 898.5 kB/s eta 0:02:22\n",
      "   -------------------- ----------------- 149.4/276.5 MB 898.5 kB/s eta 0:02:22\n",
      "   -------------------- ----------------- 149.7/276.5 MB 910.8 kB/s eta 0:02:20\n",
      "   -------------------- ----------------- 149.7/276.5 MB 910.8 kB/s eta 0:02:20\n",
      "   -------------------- ----------------- 149.7/276.5 MB 910.8 kB/s eta 0:02:20\n",
      "   -------------------- ----------------- 149.7/276.5 MB 910.8 kB/s eta 0:02:20\n",
      "   -------------------- ----------------- 149.7/276.5 MB 910.8 kB/s eta 0:02:20\n",
      "   -------------------- ----------------- 149.7/276.5 MB 910.8 kB/s eta 0:02:20\n",
      "   -------------------- ----------------- 149.7/276.5 MB 910.8 kB/s eta 0:02:20\n",
      "   -------------------- ----------------- 149.9/276.5 MB 896.2 kB/s eta 0:02:22\n",
      "   -------------------- ----------------- 149.9/276.5 MB 896.2 kB/s eta 0:02:22\n",
      "   -------------------- ----------------- 150.2/276.5 MB 896.1 kB/s eta 0:02:21\n",
      "   -------------------- ----------------- 150.7/276.5 MB 904.2 kB/s eta 0:02:20\n",
      "   -------------------- ----------------- 151.3/276.5 MB 929.6 kB/s eta 0:02:15\n",
      "   -------------------- ----------------- 151.8/276.5 MB 941.4 kB/s eta 0:02:13\n",
      "   -------------------- ----------------- 152.0/276.5 MB 945.3 kB/s eta 0:02:12\n",
      "   -------------------- ----------------- 152.6/276.5 MB 954.5 kB/s eta 0:02:10\n",
      "   --------------------- ---------------- 152.8/276.5 MB 975.8 kB/s eta 0:02:07\n",
      "   --------------------- ---------------- 153.1/276.5 MB 979.5 kB/s eta 0:02:07\n",
      "   --------------------- ---------------- 153.4/276.5 MB 983.7 kB/s eta 0:02:06\n",
      "   --------------------- ---------------- 153.9/276.5 MB 988.9 kB/s eta 0:02:05\n",
      "   ---------------------- ----------------- 154.1/276.5 MB 1.0 MB/s eta 0:02:01\n",
      "   ---------------------- ----------------- 154.7/276.5 MB 1.0 MB/s eta 0:01:59\n",
      "   ---------------------- ----------------- 154.9/276.5 MB 1.0 MB/s eta 0:01:59\n",
      "   ---------------------- ----------------- 155.5/276.5 MB 1.0 MB/s eta 0:01:57\n",
      "   ---------------------- ----------------- 155.7/276.5 MB 1.0 MB/s eta 0:01:56\n",
      "   ---------------------- ----------------- 156.2/276.5 MB 1.1 MB/s eta 0:01:53\n",
      "   ---------------------- ----------------- 156.5/276.5 MB 1.1 MB/s eta 0:01:53\n",
      "   ---------------------- ----------------- 156.8/276.5 MB 1.1 MB/s eta 0:01:52\n",
      "   ---------------------- ----------------- 157.0/276.5 MB 1.1 MB/s eta 0:01:50\n",
      "   ---------------------- ----------------- 157.5/276.5 MB 1.1 MB/s eta 0:01:49\n",
      "   ---------------------- ----------------- 157.5/276.5 MB 1.1 MB/s eta 0:01:49\n",
      "   ---------------------- ----------------- 158.1/276.5 MB 1.1 MB/s eta 0:01:47\n",
      "   ---------------------- ----------------- 158.3/276.5 MB 1.1 MB/s eta 0:01:46\n",
      "   ---------------------- ----------------- 158.9/276.5 MB 1.1 MB/s eta 0:01:45\n",
      "   ----------------------- ---------------- 159.4/276.5 MB 1.1 MB/s eta 0:01:43\n",
      "   ----------------------- ---------------- 159.6/276.5 MB 1.1 MB/s eta 0:01:42\n",
      "   ----------------------- ---------------- 159.9/276.5 MB 1.2 MB/s eta 0:01:42\n",
      "   ----------------------- ---------------- 160.4/276.5 MB 1.2 MB/s eta 0:01:41\n",
      "   ----------------------- ---------------- 160.7/276.5 MB 1.2 MB/s eta 0:01:40\n",
      "   ----------------------- ---------------- 161.2/276.5 MB 1.2 MB/s eta 0:01:40\n",
      "   ----------------------- ---------------- 161.5/276.5 MB 1.2 MB/s eta 0:01:39\n",
      "   ----------------------- ---------------- 162.0/276.5 MB 1.2 MB/s eta 0:01:38\n",
      "   ----------------------- ---------------- 162.3/276.5 MB 1.2 MB/s eta 0:01:38\n",
      "   ----------------------- ---------------- 162.5/276.5 MB 1.2 MB/s eta 0:01:38\n",
      "   ----------------------- ---------------- 162.8/276.5 MB 1.2 MB/s eta 0:01:38\n",
      "   ----------------------- ---------------- 163.3/276.5 MB 1.2 MB/s eta 0:01:37\n",
      "   ----------------------- ---------------- 163.6/276.5 MB 1.2 MB/s eta 0:01:37\n",
      "   ----------------------- ---------------- 164.1/276.5 MB 1.2 MB/s eta 0:01:36\n",
      "   ----------------------- ---------------- 164.4/276.5 MB 1.2 MB/s eta 0:01:34\n",
      "   ----------------------- ---------------- 164.9/276.5 MB 1.2 MB/s eta 0:01:33\n",
      "   ----------------------- ---------------- 165.2/276.5 MB 1.2 MB/s eta 0:01:33\n",
      "   ----------------------- ---------------- 165.7/276.5 MB 1.2 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 165.9/276.5 MB 1.2 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 166.5/276.5 MB 1.2 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 166.7/276.5 MB 1.2 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 167.0/276.5 MB 1.2 MB/s eta 0:01:31\n",
      "   ------------------------ --------------- 167.5/276.5 MB 1.2 MB/s eta 0:01:30\n",
      "   ------------------------ --------------- 167.8/276.5 MB 1.2 MB/s eta 0:01:30\n",
      "   ------------------------ --------------- 168.3/276.5 MB 1.2 MB/s eta 0:01:29\n",
      "   ------------------------ --------------- 168.6/276.5 MB 1.2 MB/s eta 0:01:29\n",
      "   ------------------------ --------------- 168.8/276.5 MB 1.2 MB/s eta 0:01:29\n",
      "   ------------------------ --------------- 169.1/276.5 MB 1.2 MB/s eta 0:01:29\n",
      "   ------------------------ --------------- 169.3/276.5 MB 1.2 MB/s eta 0:01:27\n",
      "   ------------------------ --------------- 169.9/276.5 MB 1.2 MB/s eta 0:01:26\n",
      "   ------------------------ --------------- 170.4/276.5 MB 1.3 MB/s eta 0:01:23\n",
      "   ------------------------ --------------- 170.7/276.5 MB 1.3 MB/s eta 0:01:22\n",
      "   ------------------------ --------------- 170.9/276.5 MB 1.3 MB/s eta 0:01:22\n",
      "   ------------------------ --------------- 171.2/276.5 MB 1.3 MB/s eta 0:01:22\n",
      "   ------------------------ --------------- 171.2/276.5 MB 1.3 MB/s eta 0:01:22\n",
      "   ------------------------ --------------- 171.4/276.5 MB 1.3 MB/s eta 0:01:22\n",
      "   ------------------------ --------------- 171.4/276.5 MB 1.3 MB/s eta 0:01:22\n",
      "   ------------------------ --------------- 171.7/276.5 MB 1.3 MB/s eta 0:01:19\n",
      "   ------------------------ --------------- 171.7/276.5 MB 1.3 MB/s eta 0:01:19\n",
      "   ------------------------ --------------- 172.0/276.5 MB 1.3 MB/s eta 0:01:20\n",
      "   ------------------------ --------------- 172.0/276.5 MB 1.3 MB/s eta 0:01:20\n",
      "   ------------------------ --------------- 172.0/276.5 MB 1.3 MB/s eta 0:01:20\n",
      "   ------------------------ --------------- 172.0/276.5 MB 1.3 MB/s eta 0:01:20\n",
      "   ------------------------ --------------- 172.0/276.5 MB 1.3 MB/s eta 0:01:20\n",
      "   ------------------------ --------------- 172.0/276.5 MB 1.3 MB/s eta 0:01:20\n",
      "   ------------------------ --------------- 172.0/276.5 MB 1.3 MB/s eta 0:01:20\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.2/276.5 MB 1.3 MB/s eta 0:01:21\n",
      "   ------------------------ --------------- 172.5/276.5 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 172.5/276.5 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 172.5/276.5 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 172.5/276.5 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 172.5/276.5 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 172.5/276.5 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 172.5/276.5 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------------ --------------- 172.8/276.5 MB 1.1 MB/s eta 0:01:38\n",
      "   ------------------------- -------------- 173.0/276.5 MB 1.1 MB/s eta 0:01:38\n",
      "   ------------------------- -------------- 173.3/276.5 MB 1.1 MB/s eta 0:01:38\n",
      "   ------------------------- -------------- 173.8/276.5 MB 1.1 MB/s eta 0:01:37\n",
      "   ------------------------- -------------- 174.1/276.5 MB 1.1 MB/s eta 0:01:36\n",
      "   ------------------------- -------------- 174.6/276.5 MB 1.1 MB/s eta 0:01:35\n",
      "   ------------------------- -------------- 174.9/276.5 MB 1.1 MB/s eta 0:01:35\n",
      "   ------------------------- -------------- 175.4/276.5 MB 1.1 MB/s eta 0:01:34\n",
      "   ------------------------- -------------- 175.6/276.5 MB 1.1 MB/s eta 0:01:34\n",
      "   ------------------------- -------------- 175.9/276.5 MB 1.1 MB/s eta 0:01:34\n",
      "   ------------------------- -------------- 176.4/276.5 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------------- -------------- 176.7/276.5 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------------- -------------- 176.9/276.5 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------------- -------------- 177.2/276.5 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------------- -------------- 177.7/276.5 MB 1.1 MB/s eta 0:01:30\n",
      "   ------------------------- -------------- 178.0/276.5 MB 1.1 MB/s eta 0:01:29\n",
      "   ------------------------- -------------- 178.3/276.5 MB 1.1 MB/s eta 0:01:29\n",
      "   ------------------------- -------------- 178.8/276.5 MB 1.1 MB/s eta 0:01:28\n",
      "   ------------------------- -------------- 179.0/276.5 MB 1.2 MB/s eta 0:01:23\n",
      "   ------------------------- -------------- 179.3/276.5 MB 1.2 MB/s eta 0:01:23\n",
      "   -------------------------- ------------- 179.8/276.5 MB 1.2 MB/s eta 0:01:22\n",
      "   -------------------------- ------------- 180.1/276.5 MB 1.2 MB/s eta 0:01:22\n",
      "   -------------------------- ------------- 180.4/276.5 MB 1.2 MB/s eta 0:01:21\n",
      "   -------------------------- ------------- 180.9/276.5 MB 1.2 MB/s eta 0:01:20\n",
      "   -------------------------- ------------- 181.1/276.5 MB 1.2 MB/s eta 0:01:20\n",
      "   -------------------------- ------------- 181.7/276.5 MB 1.2 MB/s eta 0:01:19\n",
      "   -------------------------- ------------- 181.9/276.5 MB 1.2 MB/s eta 0:01:18\n",
      "   -------------------------- ------------- 182.2/276.5 MB 1.2 MB/s eta 0:01:18\n",
      "   -------------------------- ------------- 182.5/276.5 MB 1.2 MB/s eta 0:01:18\n",
      "   -------------------------- ------------- 182.7/276.5 MB 1.2 MB/s eta 0:01:18\n",
      "   -------------------------- ------------- 183.0/276.5 MB 1.2 MB/s eta 0:01:17\n",
      "   -------------------------- ------------- 183.5/276.5 MB 1.2 MB/s eta 0:01:17\n",
      "   -------------------------- ------------- 183.8/276.5 MB 1.2 MB/s eta 0:01:16\n",
      "   -------------------------- ------------- 184.3/276.5 MB 1.2 MB/s eta 0:01:16\n",
      "   -------------------------- ------------- 184.5/276.5 MB 1.2 MB/s eta 0:01:16\n",
      "   -------------------------- ------------- 184.5/276.5 MB 1.2 MB/s eta 0:01:16\n",
      "   -------------------------- ------------- 184.8/276.5 MB 1.2 MB/s eta 0:01:16\n",
      "   -------------------------- ------------- 185.1/276.5 MB 1.2 MB/s eta 0:01:16\n",
      "   -------------------------- ------------- 185.6/276.5 MB 1.2 MB/s eta 0:01:16\n",
      "   -------------------------- ------------- 185.9/276.5 MB 1.3 MB/s eta 0:01:13\n",
      "   -------------------------- ------------- 186.1/276.5 MB 1.3 MB/s eta 0:01:13\n",
      "   -------------------------- ------------- 186.6/276.5 MB 1.3 MB/s eta 0:01:12\n",
      "   --------------------------- ------------ 186.9/276.5 MB 1.3 MB/s eta 0:01:11\n",
      "   --------------------------- ------------ 187.2/276.5 MB 1.3 MB/s eta 0:01:11\n",
      "   --------------------------- ------------ 187.7/276.5 MB 1.3 MB/s eta 0:01:10\n",
      "   --------------------------- ------------ 188.0/276.5 MB 1.3 MB/s eta 0:01:07\n",
      "   --------------------------- ------------ 188.2/276.5 MB 1.3 MB/s eta 0:01:07\n",
      "   --------------------------- ------------ 188.7/276.5 MB 1.3 MB/s eta 0:01:06\n",
      "   --------------------------- ------------ 189.0/276.5 MB 1.3 MB/s eta 0:01:06\n",
      "   --------------------------- ------------ 189.3/276.5 MB 1.3 MB/s eta 0:01:06\n",
      "   --------------------------- ------------ 189.5/276.5 MB 1.3 MB/s eta 0:01:06\n",
      "   --------------------------- ------------ 190.1/276.5 MB 1.3 MB/s eta 0:01:05\n",
      "   --------------------------- ------------ 190.3/276.5 MB 1.3 MB/s eta 0:01:05\n",
      "   --------------------------- ------------ 190.8/276.5 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 191.1/276.5 MB 1.3 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 191.1/276.5 MB 1.3 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 191.4/276.5 MB 1.3 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 191.6/276.5 MB 1.3 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 191.9/276.5 MB 1.3 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 192.4/276.5 MB 1.3 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 192.7/276.5 MB 1.3 MB/s eta 0:01:04\n",
      "   --------------------------- ------------ 193.2/276.5 MB 1.3 MB/s eta 0:01:03\n",
      "   --------------------------- ------------ 193.5/276.5 MB 1.3 MB/s eta 0:01:03\n",
      "   ---------------------------- ----------- 193.7/276.5 MB 1.3 MB/s eta 0:01:03\n",
      "   ---------------------------- ----------- 194.0/276.5 MB 1.3 MB/s eta 0:01:03\n",
      "   ---------------------------- ----------- 194.5/276.5 MB 1.3 MB/s eta 0:01:02\n",
      "   ---------------------------- ----------- 194.8/276.5 MB 1.3 MB/s eta 0:01:02\n",
      "   ---------------------------- ----------- 195.0/276.5 MB 1.3 MB/s eta 0:01:02\n",
      "   ---------------------------- ----------- 195.6/276.5 MB 1.3 MB/s eta 0:01:02\n",
      "   ---------------------------- ----------- 195.8/276.5 MB 1.3 MB/s eta 0:01:02\n",
      "   ---------------------------- ----------- 196.3/276.5 MB 1.3 MB/s eta 0:01:01\n",
      "   ---------------------------- ----------- 196.6/276.5 MB 1.3 MB/s eta 0:01:01\n",
      "   ---------------------------- ----------- 196.9/276.5 MB 1.3 MB/s eta 0:01:01\n",
      "   ---------------------------- ----------- 197.4/276.5 MB 1.3 MB/s eta 0:01:00\n",
      "   ---------------------------- ----------- 197.4/276.5 MB 1.3 MB/s eta 0:01:00\n",
      "   ---------------------------- ----------- 197.9/276.5 MB 1.3 MB/s eta 0:01:00\n",
      "   ---------------------------- ----------- 198.2/276.5 MB 1.3 MB/s eta 0:01:00\n",
      "   ---------------------------- ----------- 198.4/276.5 MB 1.3 MB/s eta 0:01:00\n",
      "   ---------------------------- ----------- 199.0/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ---------------------------- ----------- 199.2/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ---------------------------- ----------- 199.5/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ---------------------------- ----------- 199.8/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ---------------------------- ----------- 199.8/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ---------------------------- ----------- 200.3/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ----------------------------- ---------- 200.5/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ----------------------------- ---------- 200.8/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ----------------------------- ---------- 201.3/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ----------------------------- ---------- 201.3/276.5 MB 1.3 MB/s eta 0:00:59\n",
      "   ----------------------------- ---------- 201.9/276.5 MB 1.3 MB/s eta 0:00:58\n",
      "   ----------------------------- ---------- 202.4/276.5 MB 1.3 MB/s eta 0:00:58\n",
      "   ----------------------------- ---------- 202.6/276.5 MB 1.3 MB/s eta 0:00:57\n",
      "   ----------------------------- ---------- 202.9/276.5 MB 1.3 MB/s eta 0:00:57\n",
      "   ----------------------------- ---------- 203.2/276.5 MB 1.3 MB/s eta 0:00:57\n",
      "   ----------------------------- ---------- 203.4/276.5 MB 1.3 MB/s eta 0:00:57\n",
      "   ----------------------------- ---------- 203.9/276.5 MB 1.3 MB/s eta 0:00:57\n",
      "   ----------------------------- ---------- 204.2/276.5 MB 1.3 MB/s eta 0:00:57\n",
      "   ----------------------------- ---------- 204.7/276.5 MB 1.3 MB/s eta 0:00:56\n",
      "   ----------------------------- ---------- 205.0/276.5 MB 1.3 MB/s eta 0:00:56\n",
      "   ----------------------------- ---------- 205.5/276.5 MB 1.3 MB/s eta 0:00:56\n",
      "   ----------------------------- ---------- 205.5/276.5 MB 1.3 MB/s eta 0:00:56\n",
      "   ----------------------------- ---------- 205.8/276.5 MB 1.3 MB/s eta 0:00:55\n",
      "   ----------------------------- ---------- 206.3/276.5 MB 1.3 MB/s eta 0:00:55\n",
      "   ----------------------------- ---------- 206.6/276.5 MB 1.3 MB/s eta 0:00:55\n",
      "   ----------------------------- ---------- 207.1/276.5 MB 1.3 MB/s eta 0:00:55\n",
      "   ----------------------------- ---------- 207.4/276.5 MB 1.3 MB/s eta 0:00:54\n",
      "   ------------------------------ --------- 207.9/276.5 MB 1.3 MB/s eta 0:00:54\n",
      "   ------------------------------ --------- 208.1/276.5 MB 1.3 MB/s eta 0:00:54\n",
      "   ------------------------------ --------- 208.4/276.5 MB 1.3 MB/s eta 0:00:53\n",
      "   ------------------------------ --------- 208.7/276.5 MB 1.3 MB/s eta 0:00:53\n",
      "   ------------------------------ --------- 208.9/276.5 MB 1.3 MB/s eta 0:00:53\n",
      "   ------------------------------ --------- 209.5/276.5 MB 1.3 MB/s eta 0:00:53\n",
      "   ------------------------------ --------- 209.7/276.5 MB 1.3 MB/s eta 0:00:52\n",
      "   ------------------------------ --------- 210.2/276.5 MB 1.3 MB/s eta 0:00:51\n",
      "   ------------------------------ --------- 210.5/276.5 MB 1.3 MB/s eta 0:00:51\n",
      "   ------------------------------ --------- 210.8/276.5 MB 1.3 MB/s eta 0:00:51\n",
      "   ------------------------------ --------- 211.3/276.5 MB 1.3 MB/s eta 0:00:50\n",
      "   ------------------------------ --------- 211.6/276.5 MB 1.3 MB/s eta 0:00:49\n",
      "   ------------------------------ --------- 211.8/276.5 MB 1.3 MB/s eta 0:00:49\n",
      "   ------------------------------ --------- 212.3/276.5 MB 1.4 MB/s eta 0:00:46\n",
      "   ------------------------------ --------- 212.6/276.5 MB 1.4 MB/s eta 0:00:46\n",
      "   ------------------------------ --------- 212.9/276.5 MB 1.4 MB/s eta 0:00:46\n",
      "   ------------------------------ --------- 213.1/276.5 MB 1.4 MB/s eta 0:00:46\n",
      "   ------------------------------ --------- 213.6/276.5 MB 1.4 MB/s eta 0:00:45\n",
      "   ------------------------------ --------- 213.9/276.5 MB 1.4 MB/s eta 0:00:45\n",
      "   ------------------------------ --------- 214.2/276.5 MB 1.4 MB/s eta 0:00:45\n",
      "   ------------------------------- -------- 214.7/276.5 MB 1.6 MB/s eta 0:00:40\n",
      "   ------------------------------- -------- 215.0/276.5 MB 1.6 MB/s eta 0:00:40\n",
      "   ------------------------------- -------- 215.5/276.5 MB 1.6 MB/s eta 0:00:40\n",
      "   ------------------------------- -------- 215.7/276.5 MB 1.6 MB/s eta 0:00:40\n",
      "   ------------------------------- -------- 216.0/276.5 MB 1.6 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 216.3/276.5 MB 1.6 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 216.3/276.5 MB 1.6 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 216.5/276.5 MB 1.5 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 216.8/276.5 MB 1.5 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 217.1/276.5 MB 1.5 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 217.3/276.5 MB 1.5 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 217.6/276.5 MB 1.5 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 217.8/276.5 MB 1.5 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 218.1/276.5 MB 1.5 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 218.4/276.5 MB 1.6 MB/s eta 0:00:37\n",
      "   ------------------------------- -------- 218.9/276.5 MB 1.6 MB/s eta 0:00:37\n",
      "   ------------------------------- -------- 219.2/276.5 MB 1.6 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 219.7/276.5 MB 1.6 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 219.9/276.5 MB 1.6 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 220.2/276.5 MB 1.6 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 220.5/276.5 MB 1.6 MB/s eta 0:00:35\n",
      "   ------------------------------- -------- 220.7/276.5 MB 1.6 MB/s eta 0:00:35\n",
      "   ------------------------------- -------- 220.7/276.5 MB 1.6 MB/s eta 0:00:35\n",
      "   ------------------------------- -------- 220.7/276.5 MB 1.6 MB/s eta 0:00:35\n",
      "   ------------------------------- -------- 221.0/276.5 MB 1.6 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 221.0/276.5 MB 1.6 MB/s eta 0:00:36\n",
      "   ------------------------------- -------- 221.0/276.5 MB 1.6 MB/s eta 0:00:36\n",
      "   -------------------------------- ------- 221.2/276.5 MB 1.5 MB/s eta 0:00:36\n",
      "   -------------------------------- ------- 221.2/276.5 MB 1.5 MB/s eta 0:00:36\n",
      "   -------------------------------- ------- 221.2/276.5 MB 1.5 MB/s eta 0:00:36\n",
      "   -------------------------------- ------- 221.5/276.5 MB 1.5 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 221.5/276.5 MB 1.5 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 221.5/276.5 MB 1.5 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 221.5/276.5 MB 1.5 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 221.8/276.5 MB 1.5 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 221.8/276.5 MB 1.5 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 221.8/276.5 MB 1.5 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 222.0/276.5 MB 1.5 MB/s eta 0:00:38\n",
      "   -------------------------------- ------- 222.0/276.5 MB 1.5 MB/s eta 0:00:38\n",
      "   -------------------------------- ------- 222.0/276.5 MB 1.5 MB/s eta 0:00:38\n",
      "   -------------------------------- ------- 222.6/276.5 MB 1.4 MB/s eta 0:00:38\n",
      "   -------------------------------- ------- 223.1/276.5 MB 1.4 MB/s eta 0:00:38\n",
      "   -------------------------------- ------- 223.3/276.5 MB 1.4 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 223.9/276.5 MB 1.4 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 224.1/276.5 MB 1.4 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 224.4/276.5 MB 1.4 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 224.7/276.5 MB 1.4 MB/s eta 0:00:37\n",
      "   -------------------------------- ------- 225.2/276.5 MB 1.4 MB/s eta 0:00:36\n",
      "   -------------------------------- ------- 225.4/276.5 MB 1.4 MB/s eta 0:00:36\n",
      "   -------------------------------- ------- 225.7/276.5 MB 1.4 MB/s eta 0:00:36\n",
      "   -------------------------------- ------- 226.2/276.5 MB 1.4 MB/s eta 0:00:35\n",
      "   -------------------------------- ------- 226.8/276.5 MB 1.5 MB/s eta 0:00:35\n",
      "   -------------------------------- ------- 227.0/276.5 MB 1.4 MB/s eta 0:00:35\n",
      "   -------------------------------- ------- 227.5/276.5 MB 1.4 MB/s eta 0:00:34\n",
      "   -------------------------------- ------- 227.8/276.5 MB 1.4 MB/s eta 0:00:34\n",
      "   -------------------------------- ------- 228.1/276.5 MB 1.5 MB/s eta 0:00:34\n",
      "   --------------------------------- ------ 228.3/276.5 MB 1.5 MB/s eta 0:00:34\n",
      "   --------------------------------- ------ 228.6/276.5 MB 1.5 MB/s eta 0:00:33\n",
      "   --------------------------------- ------ 229.1/276.5 MB 1.5 MB/s eta 0:00:33\n",
      "   --------------------------------- ------ 229.4/276.5 MB 1.5 MB/s eta 0:00:33\n",
      "   --------------------------------- ------ 229.9/276.5 MB 1.5 MB/s eta 0:00:32\n",
      "   --------------------------------- ------ 230.2/276.5 MB 1.5 MB/s eta 0:00:32\n",
      "   --------------------------------- ------ 230.4/276.5 MB 1.5 MB/s eta 0:00:32\n",
      "   --------------------------------- ------ 230.7/276.5 MB 1.5 MB/s eta 0:00:32\n",
      "   --------------------------------- ------ 231.2/276.5 MB 1.5 MB/s eta 0:00:32\n",
      "   --------------------------------- ------ 231.5/276.5 MB 1.5 MB/s eta 0:00:31\n",
      "   --------------------------------- ------ 232.0/276.5 MB 1.5 MB/s eta 0:00:31\n",
      "   --------------------------------- ------ 232.3/276.5 MB 1.5 MB/s eta 0:00:31\n",
      "   --------------------------------- ------ 232.5/276.5 MB 1.5 MB/s eta 0:00:31\n",
      "   --------------------------------- ------ 232.8/276.5 MB 1.5 MB/s eta 0:00:30\n",
      "   --------------------------------- ------ 233.0/276.5 MB 1.5 MB/s eta 0:00:30\n",
      "   --------------------------------- ------ 233.6/276.5 MB 1.5 MB/s eta 0:00:30\n",
      "   --------------------------------- ------ 233.8/276.5 MB 1.5 MB/s eta 0:00:30\n",
      "   --------------------------------- ------ 234.4/276.5 MB 1.5 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 234.6/276.5 MB 1.5 MB/s eta 0:00:29\n",
      "   --------------------------------- ------ 234.9/276.5 MB 1.5 MB/s eta 0:00:29\n",
      "   ---------------------------------- ----- 235.1/276.5 MB 1.5 MB/s eta 0:00:29\n",
      "   ---------------------------------- ----- 235.7/276.5 MB 1.5 MB/s eta 0:00:28\n",
      "   ---------------------------------- ----- 235.9/276.5 MB 1.5 MB/s eta 0:00:28\n",
      "   ---------------------------------- ----- 236.5/276.5 MB 1.5 MB/s eta 0:00:28\n",
      "   ---------------------------------- ----- 236.7/276.5 MB 1.5 MB/s eta 0:00:27\n",
      "   ---------------------------------- ----- 237.0/276.5 MB 1.5 MB/s eta 0:00:27\n",
      "   ---------------------------------- ----- 237.2/276.5 MB 1.5 MB/s eta 0:00:27\n",
      "   ---------------------------------- ----- 237.5/276.5 MB 1.5 MB/s eta 0:00:27\n",
      "   ---------------------------------- ----- 238.0/276.5 MB 1.5 MB/s eta 0:00:27\n",
      "   ---------------------------------- ----- 238.3/276.5 MB 1.5 MB/s eta 0:00:27\n",
      "   ---------------------------------- ----- 238.6/276.5 MB 1.5 MB/s eta 0:00:26\n",
      "   ---------------------------------- ----- 238.8/276.5 MB 1.5 MB/s eta 0:00:26\n",
      "   ---------------------------------- ----- 239.3/276.5 MB 1.5 MB/s eta 0:00:26\n",
      "   ---------------------------------- ----- 239.3/276.5 MB 1.5 MB/s eta 0:00:26\n",
      "   ---------------------------------- ----- 239.9/276.5 MB 1.5 MB/s eta 0:00:26\n",
      "   ---------------------------------- ----- 240.4/276.5 MB 1.5 MB/s eta 0:00:25\n",
      "   ---------------------------------- ----- 240.6/276.5 MB 1.5 MB/s eta 0:00:25\n",
      "   ---------------------------------- ----- 240.9/276.5 MB 1.5 MB/s eta 0:00:25\n",
      "   ---------------------------------- ----- 241.4/276.5 MB 1.5 MB/s eta 0:00:24\n",
      "   ---------------------------------- ----- 241.7/276.5 MB 1.5 MB/s eta 0:00:24\n",
      "   ---------------------------------- ----- 242.0/276.5 MB 1.5 MB/s eta 0:00:24\n",
      "   ----------------------------------- ---- 242.5/276.5 MB 1.5 MB/s eta 0:00:24\n",
      "   ----------------------------------- ---- 242.7/276.5 MB 1.5 MB/s eta 0:00:23\n",
      "   ----------------------------------- ---- 243.3/276.5 MB 1.5 MB/s eta 0:00:23\n",
      "   ----------------------------------- ---- 243.5/276.5 MB 1.5 MB/s eta 0:00:23\n",
      "   ----------------------------------- ---- 243.8/276.5 MB 1.5 MB/s eta 0:00:23\n",
      "   ----------------------------------- ---- 244.3/276.5 MB 1.5 MB/s eta 0:00:22\n",
      "   ----------------------------------- ---- 244.6/276.5 MB 1.5 MB/s eta 0:00:22\n",
      "   ----------------------------------- ---- 245.1/276.5 MB 1.5 MB/s eta 0:00:22\n",
      "   ----------------------------------- ---- 245.4/276.5 MB 1.5 MB/s eta 0:00:21\n",
      "   ----------------------------------- ---- 245.9/276.5 MB 1.5 MB/s eta 0:00:21\n",
      "   ----------------------------------- ---- 245.9/276.5 MB 1.5 MB/s eta 0:00:21\n",
      "   ----------------------------------- ---- 246.2/276.5 MB 1.5 MB/s eta 0:00:21\n",
      "   ----------------------------------- ---- 246.7/276.5 MB 1.5 MB/s eta 0:00:21\n",
      "   ----------------------------------- ---- 246.7/276.5 MB 1.5 MB/s eta 0:00:21\n",
      "   ----------------------------------- ---- 247.2/276.5 MB 1.5 MB/s eta 0:00:20\n",
      "   ----------------------------------- ---- 247.7/276.5 MB 1.5 MB/s eta 0:00:20\n",
      "   ----------------------------------- ---- 248.0/276.5 MB 1.5 MB/s eta 0:00:20\n",
      "   ----------------------------------- ---- 248.5/276.5 MB 1.5 MB/s eta 0:00:19\n",
      "   ----------------------------------- ---- 248.8/276.5 MB 1.5 MB/s eta 0:00:19\n",
      "   ------------------------------------ --- 249.3/276.5 MB 1.5 MB/s eta 0:00:19\n",
      "   ------------------------------------ --- 249.6/276.5 MB 1.5 MB/s eta 0:00:19\n",
      "   ------------------------------------ --- 249.8/276.5 MB 1.5 MB/s eta 0:00:18\n",
      "   ------------------------------------ --- 250.3/276.5 MB 1.5 MB/s eta 0:00:18\n",
      "   ------------------------------------ --- 250.6/276.5 MB 1.5 MB/s eta 0:00:18\n",
      "   ------------------------------------ --- 250.9/276.5 MB 1.5 MB/s eta 0:00:18\n",
      "   ------------------------------------ --- 251.4/276.5 MB 1.5 MB/s eta 0:00:17\n",
      "   ------------------------------------ --- 251.7/276.5 MB 1.5 MB/s eta 0:00:17\n",
      "   ------------------------------------ --- 252.2/276.5 MB 1.5 MB/s eta 0:00:17\n",
      "   ------------------------------------ --- 252.4/276.5 MB 1.5 MB/s eta 0:00:17\n",
      "   ------------------------------------ --- 252.7/276.5 MB 1.5 MB/s eta 0:00:16\n",
      "   ------------------------------------ --- 253.0/276.5 MB 1.5 MB/s eta 0:00:16\n",
      "   ------------------------------------ --- 253.5/276.5 MB 1.5 MB/s eta 0:00:16\n",
      "   ------------------------------------ --- 253.8/276.5 MB 1.5 MB/s eta 0:00:16\n",
      "   ------------------------------------ --- 254.3/276.5 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 254.5/276.5 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 254.8/276.5 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 255.1/276.5 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------------------ --- 255.6/276.5 MB 1.5 MB/s eta 0:00:15\n",
      "   ------------------------------------- -- 255.9/276.5 MB 1.5 MB/s eta 0:00:14\n",
      "   ------------------------------------- -- 256.1/276.5 MB 1.5 MB/s eta 0:00:14\n",
      "   ------------------------------------- -- 256.4/276.5 MB 1.5 MB/s eta 0:00:14\n",
      "   ------------------------------------- -- 256.9/276.5 MB 1.5 MB/s eta 0:00:14\n",
      "   ------------------------------------- -- 257.2/276.5 MB 1.5 MB/s eta 0:00:13\n",
      "   ------------------------------------- -- 257.7/276.5 MB 1.5 MB/s eta 0:00:13\n",
      "   ------------------------------------- -- 257.9/276.5 MB 1.5 MB/s eta 0:00:13\n",
      "   ------------------------------------- -- 258.2/276.5 MB 1.5 MB/s eta 0:00:13\n",
      "   ------------------------------------- -- 258.5/276.5 MB 1.5 MB/s eta 0:00:13\n",
      "   ------------------------------------- -- 259.0/276.5 MB 1.5 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 259.3/276.5 MB 1.5 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 259.8/276.5 MB 1.5 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 260.0/276.5 MB 1.5 MB/s eta 0:00:12\n",
      "   ------------------------------------- -- 260.3/276.5 MB 1.5 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 260.6/276.5 MB 1.5 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 261.1/276.5 MB 1.5 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 261.4/276.5 MB 1.5 MB/s eta 0:00:11\n",
      "   ------------------------------------- -- 261.6/276.5 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 262.1/276.5 MB 1.5 MB/s eta 0:00:10\n",
      "   ------------------------------------- -- 262.7/276.5 MB 1.5 MB/s eta 0:00:10\n",
      "   -------------------------------------- - 262.9/276.5 MB 1.5 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 263.2/276.5 MB 1.5 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 263.5/276.5 MB 1.5 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 263.7/276.5 MB 1.5 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 264.2/276.5 MB 1.5 MB/s eta 0:00:09\n",
      "   -------------------------------------- - 264.5/276.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 265.0/276.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 265.3/276.5 MB 1.5 MB/s eta 0:00:08\n",
      "   -------------------------------------- - 265.8/276.5 MB 1.5 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 266.1/276.5 MB 1.5 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 266.3/276.5 MB 1.5 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 266.6/276.5 MB 1.5 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 267.1/276.5 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------------------------------- - 267.4/276.5 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 267.6/276.5 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 268.2/276.5 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 268.4/276.5 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 268.7/276.5 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 269.2/276.5 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 269.5/276.5 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  269.7/276.5 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  269.7/276.5 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------------------------  270.0/276.5 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------------------  270.3/276.5 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------------------  270.5/276.5 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------------------  270.8/276.5 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------------------------------  271.3/276.5 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------------------------------  271.6/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  271.8/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  272.4/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  272.6/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  273.2/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  273.2/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  273.2/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  273.2/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  273.2/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  273.2/276.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  273.4/276.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.4/276.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.4/276.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.4/276.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.4/276.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.4/276.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.4/276.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.7/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.7/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.7/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.7/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.7/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.7/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.9/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.9/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.9/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.9/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.9/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  273.9/276.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  274.2/276.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  274.2/276.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  274.2/276.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  274.5/276.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  274.5/276.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  274.7/276.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  274.7/276.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  275.0/276.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  275.3/276.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  275.5/276.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  275.8/276.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  276.3/276.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  276.3/276.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  276.3/276.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 276.5/276.5 MB 1.3 MB/s  0:03:34\n",
      "Downloading numpy-1.24.3-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.8 MB 2.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.0/14.8 MB 2.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.3/14.8 MB 1.6 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.6/14.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.8/14.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.8/14.8 MB 1.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/14.8 MB 1.4 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.4/14.8 MB 702.6 kB/s eta 0:00:18\n",
      "   ------ --------------------------------- 2.4/14.8 MB 702.6 kB/s eta 0:00:18\n",
      "   ------ --------------------------------- 2.4/14.8 MB 702.6 kB/s eta 0:00:18\n",
      "   ------ --------------------------------- 2.4/14.8 MB 702.6 kB/s eta 0:00:18\n",
      "   ------- -------------------------------- 2.6/14.8 MB 604.0 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 2.6/14.8 MB 604.0 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 2.9/14.8 MB 612.3 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 3.1/14.8 MB 636.4 kB/s eta 0:00:19\n",
      "   --------- ------------------------------ 3.4/14.8 MB 664.4 kB/s eta 0:00:18\n",
      "   --------- ------------------------------ 3.7/14.8 MB 692.4 kB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 3.9/14.8 MB 722.7 kB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 4.2/14.8 MB 751.1 kB/s eta 0:00:15\n",
      "   ------------ --------------------------- 4.7/14.8 MB 801.1 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 5.0/14.8 MB 818.3 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 5.2/14.8 MB 836.7 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 5.5/14.8 MB 858.1 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 5.8/14.8 MB 878.6 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 6.3/14.8 MB 920.9 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.8/14.8 MB 970.9 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 7.1/14.8 MB 984.6 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 7.3/14.8 MB 995.5 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 7.9/14.8 MB 1.0 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 8.1/14.8 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 8.7/14.8 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 8.9/14.8 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 9.2/14.8 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 9.4/14.8 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 10.0/14.8 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 10.2/14.8 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 10.7/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 11.0/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 11.3/14.8 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 11.5/14.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 11.8/14.8 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 12.1/14.8 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.3/14.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 12.8/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.1/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.4/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.6/14.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 13.9/14.8 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.8/14.8 MB 1.2 MB/s  0:00:11\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached grpcio-1.74.0-cp310-cp310-win_amd64.whl (4.5 MB)\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.4 MB/s  0:00:01\n",
      "Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.6 MB 1.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.0/5.6 MB 1.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.3/5.6 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.1/5.6 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.9/5.6 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.9/5.6 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 3.7/5.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 3.9/5.6 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.6 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 1.7 MB/s  0:00:03\n",
      "Downloading google_auth-2.48.0-py3-none-any.whl (236 kB)\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.46.3-py3-none-any.whl (30 kB)\n",
      "Using cached cryptography-45.0.7-cp37-abi3-win_amd64.whl (3.4 MB)\n",
      "Using cached cffi-2.0.0-cp310-cp310-win_amd64.whl (182 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/2.9 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.0/2.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.3/2.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.8/2.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.1/2.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 1.8 MB/s  0:00:01\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading markdown-3.10.1-py3-none-any.whl (107 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Using cached pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wheel, werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pycparser, pyasn1, protobuf, opt_einsum, oauthlib, numpy, markdown, keras, grpcio, google_pasta, gast, rsa, requests-oauthlib, pyasn1-modules, h5py, cffi, astunparse, cryptography, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "\n",
      "   ----------------------------------------  0/32 [libclang]\n",
      "   ----------------------------------------  0/32 [libclang]\n",
      "   --- ------------------------------------  3/32 [werkzeug]\n",
      "  Attempting uninstall: typing-extensions\n",
      "   --- ------------------------------------  3/32 [werkzeug]\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "   --- ------------------------------------  3/32 [werkzeug]\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "   --- ------------------------------------  3/32 [werkzeug]\n",
      "   ----- ----------------------------------  4/32 [typing-extensions]\n",
      "   ----- ----------------------------------  4/32 [typing-extensions]\n",
      "   ----- ----------------------------------  4/32 [typing-extensions]\n",
      "   ----- ----------------------------------  4/32 [typing-extensions]\n",
      "   ----- ----------------------------------  4/32 [typing-extensions]\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "   ----- ----------------------------------  4/32 [typing-extensions]\n",
      "   -------- -------------------------------  7/32 [tensorflow-estimator]\n",
      "   -------- -------------------------------  7/32 [tensorflow-estimator]\n",
      "   -------- -------------------------------  7/32 [tensorflow-estimator]\n",
      "   ------------ --------------------------- 10/32 [pyasn1]\n",
      "  Attempting uninstall: protobuf\n",
      "   ------------ --------------------------- 10/32 [pyasn1]\n",
      "    Found existing installation: protobuf 6.33.5\n",
      "   ------------ --------------------------- 10/32 [pyasn1]\n",
      "    Uninstalling protobuf-6.33.5:\n",
      "   ------------ --------------------------- 10/32 [pyasn1]\n",
      "      Successfully uninstalled protobuf-6.33.5\n",
      "   ------------ --------------------------- 10/32 [pyasn1]\n",
      "   ------------- -------------------------- 11/32 [protobuf]\n",
      "   --------------- ------------------------ 12/32 [opt_einsum]\n",
      "   ---------------- ----------------------- 13/32 [oauthlib]\n",
      "  Attempting uninstall: numpy\n",
      "   ---------------- ----------------------- 13/32 [oauthlib]\n",
      "    Found existing installation: numpy 1.26.4\n",
      "   ---------------- ----------------------- 13/32 [oauthlib]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "    Uninstalling numpy-1.26.4:\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ----------------- ---------------------- 14/32 [numpy]\n",
      "   ------------------ --------------------- 15/32 [markdown]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   -------------------- ------------------- 16/32 [keras]\n",
      "   --------------------- ------------------ 17/32 [grpcio]\n",
      "   --------------------- ------------------ 17/32 [grpcio]\n",
      "  Attempting uninstall: gast\n",
      "   --------------------- ------------------ 17/32 [grpcio]\n",
      "    Found existing installation: gast 0.7.0\n",
      "   --------------------- ------------------ 17/32 [grpcio]\n",
      "    Uninstalling gast-0.7.0:\n",
      "   --------------------- ------------------ 17/32 [grpcio]\n",
      "   ----------------------- ---------------- 19/32 [gast]\n",
      "      Successfully uninstalled gast-0.7.0\n",
      "   ----------------------- ---------------- 19/32 [gast]\n",
      "   -------------------------- ------------- 21/32 [requests-oauthlib]\n",
      "   --------------------------- ------------ 22/32 [pyasn1-modules]\n",
      "   --------------------------- ------------ 22/32 [pyasn1-modules]\n",
      "   --------------------------- ------------ 22/32 [pyasn1-modules]\n",
      "   ---------------------------- ----------- 23/32 [h5py]\n",
      "   ---------------------------- ----------- 23/32 [h5py]\n",
      "   ------------------------------ --------- 24/32 [cffi]\n",
      "   -------------------------------- ------- 26/32 [cryptography]\n",
      "   -------------------------------- ------- 26/32 [cryptography]\n",
      "   --------------------------------- ------ 27/32 [google-auth]\n",
      "   --------------------------------- ------ 27/32 [google-auth]\n",
      "   ----------------------------------- ---- 28/32 [google-auth-oauthlib]\n",
      "   ------------------------------------ --- 29/32 [tensorboard]\n",
      "   ------------------------------------ --- 29/32 [tensorboard]\n",
      "   ------------------------------------ --- 29/32 [tensorboard]\n",
      "   ------------------------------------ --- 29/32 [tensorboard]\n",
      "   ------------------------------------ --- 29/32 [tensorboard]\n",
      "   ------------------------------------ --- 29/32 [tensorboard]\n",
      "   ------------------------------------ --- 29/32 [tensorboard]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ------------------------------------- -- 30/32 [tensorflow-intel]\n",
      "   ---------------------------------------- 32/32 [tensorflow]\n",
      "\n",
      "Successfully installed astunparse-1.6.3 cffi-2.0.0 cryptography-45.0.7 flatbuffers-25.12.19 gast-0.4.0 google-auth-2.48.0 google-auth-oauthlib-1.0.0 google_pasta-0.2.0 grpcio-1.74.0 h5py-3.15.1 keras-2.13.1 libclang-18.1.1 markdown-3.10.1 numpy-1.24.3 oauthlib-3.3.1 opt_einsum-3.4.0 protobuf-4.25.8 pyasn1-0.6.2 pyasn1-modules-0.4.2 pycparser-3.0 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.3.0 typing-extensions-4.5.0 werkzeug-3.1.5 wheel-0.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "exceptiongroup 1.3.1 requires typing-extensions>=4.6.0; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "ipython 8.38.0 requires typing_extensions>=4.6; python_version < \"3.12\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic 2.12.5 requires typing-extensions>=4.14.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.41.5 requires typing-extensions>=4.14.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "torch 2.10.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "typing-inspection 0.4.2 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "wandb 0.24.1 requires typing-extensions<5,>=4.8, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow with GPU support and plotting libraries\n",
    "%pip install \"tensorflow[and-cuda]\" numpy matplotlib scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf86a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from: c:\\Users\\ishaa\\OneDrive\\Desktop\\CrossVission-Attacks\\venv\\Scripts\\python.exe\n",
      "Requirement already satisfied: numpy in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: scipy in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: tensorflow[and-cuda] in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (2.13.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow[and-cuda]) (2.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (25.12.19)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (4.25.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (1.74.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow[and-cuda]) (0.31.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.48.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.32.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.1.5)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (0.46.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (45.0.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2026.1.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (0.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from cryptography>=38.0.3->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from cffi>=1.14->cryptography>=38.0.3->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.3.1)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\ishaa\\onedrive\\desktop\\crossvission-attacks\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow[and-cuda]) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'\n",
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 1. Print where the notebook is actually running from\n",
    "print(f\"Running from: {sys.executable}\")\n",
    "\n",
    "# 2. Force install into THIS specific python\n",
    "!{sys.executable} -m pip install \"tensorflow[and-cuda]\" numpy matplotlib scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc229af",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef795027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.1\n",
      "Keras version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Callable\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, DenseNet121\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Apply a global dark plotting theme so all figures and saved images have a black background\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.facecolor'] = 'black'\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "plt.rcParams['savefig.facecolor'] = 'black'\n",
    "plt.rcParams['text.color'] = 'white'\n",
    "plt.rcParams['axes.labelcolor'] = 'white'\n",
    "plt.rcParams['xtick.color'] = 'white'\n",
    "plt.rcParams['ytick.color'] = 'white'\n",
    "plt.rcParams['legend.facecolor'] = 'black'\n",
    "plt.rcParams['legend.edgecolor'] = 'white'\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.version.VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48bbd0f",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration & Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147e8a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No GPU found. Training will be slow on CPU.\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Mixed precision policy: mixed_float16\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Found {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}\")\n",
    "        print(\"Memory growth enabled for all GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU found. Training will be slow on CPU.\")\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "print(f\"Mixed precision policy: {tf.keras.mixed_precision.global_policy().name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6b271",
   "metadata": {},
   "source": [
    "## 3. Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08563025",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'zip_file': '../caltech-101.zip',  # Path relative to Model Training folder\n",
    "    'extract_dir': 'caltech101_data',\n",
    "    'img_size': (224, 224),\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'train_split': 0.8,\n",
    "    'val_split': 0.1,\n",
    "    'test_split': 0.1,\n",
    "    'seed': 42,\n",
    "    'learning_rate': 1e-4,\n",
    "    'patience_early_stop': 10,\n",
    "    'patience_lr_reduce': 5,\n",
    "}\n",
    "\n",
    "MODELS_TO_TRAIN = ['VGG19', 'ResNet50', 'DenseNet121']\n",
    "CHECKPOINT_DIR = Path('checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "np.random.seed(CONFIG['seed'])\n",
    "tf.random.set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13cc3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already extracted at: caltech101_data\n",
      "Image root directory: caltech101_data\n",
      "\n",
      "Contents of extract directory:\n",
      "  __MACOSX\n",
      "  caltech-101\n"
     ]
    }
   ],
   "source": [
    "def extract_zip_if_needed(zip_path: str, extract_dir: str) -> Path:\n",
    "    \"\"\"Extract zip file if not already extracted.\"\"\"\n",
    "    extract_path = Path(extract_dir)\n",
    "    \n",
    "    if extract_path.exists() and any(extract_path.iterdir()):\n",
    "        print(f\"Data already extracted at: {extract_path}\")\n",
    "        return extract_path\n",
    "    \n",
    "    if not Path(zip_path).exists():\n",
    "        raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n",
    "    \n",
    "    print(f\"Extracting {zip_path}...\")\n",
    "    extract_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path=extract_path)\n",
    "    \n",
    "    print(f\"Extraction complete: {extract_path}\")\n",
    "    return extract_path\n",
    "\n",
    "data_root = extract_zip_if_needed(CONFIG['zip_file'], CONFIG['extract_dir'])\n",
    "\n",
    "def find_image_root(base_path: Path) -> Path:\n",
    "    \"\"\"Find the actual root containing class folders with images.\"\"\"\n",
    "    # First, look for common Caltech-101 directory structures\n",
    "    possible_roots = [\n",
    "        base_path / '101_ObjectCategories',\n",
    "        base_path / 'caltech-101' / '101_ObjectCategories',\n",
    "        base_path / 'caltech101' / '101_ObjectCategories',\n",
    "    ]\n",
    "    \n",
    "    for root in possible_roots:\n",
    "        if root.exists() and root.is_dir():\n",
    "            print(f\"Found image root at: {root}\")\n",
    "            return root\n",
    "    \n",
    "    # Recursive search for 101_ObjectCategories anywhere in the tree\n",
    "    for obj_cat in base_path.rglob('101_ObjectCategories'):\n",
    "        if obj_cat.is_dir():\n",
    "            print(f\"Found 101_ObjectCategories at: {obj_cat}\")\n",
    "            return obj_cat\n",
    "    \n",
    "    # Fallback: search for image files and go up to find class root\n",
    "    for item in base_path.rglob('*.jpg'):\n",
    "        if item.is_file():\n",
    "            # Go up to the class folder's parent (the root with all classes)\n",
    "            class_folder = item.parent\n",
    "            root = class_folder.parent\n",
    "            if root.is_dir() and len(list(root.iterdir())) > 10:  # Should have many class folders\n",
    "                print(f\"Found image root via image search: {root}\")\n",
    "                return root\n",
    "    \n",
    "    # Another fallback: find directory with many subdirectories (class folders)\n",
    "    for subdir in base_path.rglob('*'):\n",
    "        if subdir.is_dir() and subdir.name != '__MACOSX':\n",
    "            subdirs = [s for s in subdir.iterdir() if s.is_dir() and s.name != '__MACOSX']\n",
    "            if len(subdirs) > 50:  # Caltech-101 has 101+ classes\n",
    "                print(f\"Found root with {len(subdirs)} class folders: {subdir}\")\n",
    "                return subdir\n",
    "    \n",
    "    print(f\"WARNING: Could not find image root, using base path: {base_path}\")\n",
    "    return base_path\n",
    "\n",
    "IMAGE_ROOT = find_image_root(data_root)\n",
    "print(f\"Image root directory: {IMAGE_ROOT}\")\n",
    "\n",
    "# List contents to verify\n",
    "print(f\"\\nContents of extract directory:\")\n",
    "for item in sorted(data_root.iterdir())[:10]:\n",
    "    print(f\"  {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf2fb7",
   "metadata": {},
   "source": [
    "## 5. Dataset Discovery & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b51a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Total images: 0\n",
      "\n",
      "Sample classes: ['__MACOSX', 'caltech-101']...\n",
      "\n",
      "Class distribution (first 10):\n",
      "  __MACOSX: 0 images\n",
      "  caltech-101: 0 images\n"
     ]
    }
   ],
   "source": [
    "def get_class_names_and_counts(image_root: Path) -> Tuple[list, dict]:\n",
    "    \"\"\"Get class names and image counts per class.\"\"\"\n",
    "    # Filter out system/hidden folders\n",
    "    exclude_dirs = {'__MACOSX', '.DS_Store', 'BACKGROUND_Google', '__pycache__'}\n",
    "    \n",
    "    class_names = sorted([\n",
    "        d.name for d in image_root.iterdir() \n",
    "        if d.is_dir() and d.name not in exclude_dirs and not d.name.startswith('.')\n",
    "    ])\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = image_root / class_name\n",
    "        count = len([f for f in class_dir.iterdir() \n",
    "                     if f.is_file() and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']])\n",
    "        class_counts[class_name] = count\n",
    "    \n",
    "    return class_names, class_counts\n",
    "\n",
    "CLASS_NAMES, CLASS_COUNTS = get_class_names_and_counts(IMAGE_ROOT)\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "TOTAL_IMAGES = sum(CLASS_COUNTS.values())\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Total images: {TOTAL_IMAGES}\")\n",
    "print(f\"\\nSample classes: {CLASS_NAMES[:10]}...\")\n",
    "print(f\"\\nClass distribution (first 10):\")\n",
    "for cls in list(CLASS_COUNTS.keys())[:10]:\n",
    "    print(f\"  {cls}: {CLASS_COUNTS[cls]} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973d1b5",
   "metadata": {},
   "source": [
    "## 6. Create tf.data.Dataset with Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d588826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 7315\n",
      "Validation samples: 914\n",
      "Test samples: 915\n"
     ]
    }
   ],
   "source": [
    "def collect_all_image_paths_and_labels(image_root: Path, class_names: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Collect all image paths and their corresponding labels.\"\"\"\n",
    "    all_paths = []\n",
    "    all_labels = []\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = image_root / class_name\n",
    "        for img_path in class_dir.iterdir():\n",
    "            if img_path.is_file() and img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:\n",
    "                all_paths.append(str(img_path))\n",
    "                all_labels.append(class_to_idx[class_name])\n",
    "    \n",
    "    return np.array(all_paths), np.array(all_labels)\n",
    "\n",
    "all_paths, all_labels = collect_all_image_paths_and_labels(IMAGE_ROOT, CLASS_NAMES)\n",
    "\n",
    "indices = np.arange(len(all_paths))\n",
    "np.random.shuffle(indices)\n",
    "all_paths = all_paths[indices]\n",
    "all_labels = all_labels[indices]\n",
    "\n",
    "train_size = int(CONFIG['train_split'] * len(all_paths))\n",
    "val_size = int(CONFIG['val_split'] * len(all_paths))\n",
    "\n",
    "train_paths, train_labels = all_paths[:train_size], all_labels[:train_size]\n",
    "val_paths, val_labels = all_paths[train_size:train_size+val_size], all_labels[train_size:train_size+val_size]\n",
    "test_paths, test_labels = all_paths[train_size+val_size:], all_labels[train_size+val_size:]\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)}\")\n",
    "print(f\"Validation samples: {len(val_paths)}\")\n",
    "print(f\"Test samples: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7745c",
   "metadata": {},
   "source": [
    "## 7. Dataset Pipeline Factory (with Model-Specific Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bbf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights computed | Range: [0.112, 3.118]\n",
      "Dataset pipeline factory created with enhanced augmentation and caching.\n"
     ]
    }
   ],
   "source": [
    "# Ensure compute_class_weight is available (install scikit-learn if needed)\n",
    "try:\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    print(\"scikit-learn not found  installing scikit-learn...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "PREPROCESS_FUNCTIONS = {\n",
    "    'VGG19': vgg_preprocess,\n",
    "    'ResNet50': resnet_preprocess,\n",
    "    'DenseNet121': densenet_preprocess,\n",
    "}\n",
    "\n",
    "# Compute class weights to handle imbalanced classes\n",
    "CLASS_WEIGHTS = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "CLASS_WEIGHT_DICT = dict(enumerate(CLASS_WEIGHTS))\n",
    "print(f\"Class weights computed | Range: [{CLASS_WEIGHTS.min():.3f}, {CLASS_WEIGHTS.max():.3f}]\")\n",
    "\n",
    "# Cache for datasets to avoid recreating them multiple times\n",
    "_dataset_cache = {}\n",
    "\n",
    "def create_dataset_pipeline(\n",
    "    paths: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    preprocess_fn: Callable,\n",
    "    img_size: Tuple[int, int],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    augment: bool = False,\n",
    "    cache: bool = False\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Create an optimized tf.data pipeline with model-specific preprocessing.\"\"\"\n",
    "    \n",
    "    def load_and_preprocess(path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = preprocess_fn(img)\n",
    "        return img, label\n",
    "    \n",
    "    def augment_image(img, label):\n",
    "        \"\"\"Enhanced augmentation: flips, brightness, contrast, saturation, random crop.\"\"\"\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.random_brightness(img, 0.2)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        img = tf.image.random_saturation(img, 0.8, 1.2)\n",
    "        # Random crop (simulates zoom) then resize back\n",
    "        crop_frac = tf.random.uniform([], 0.85, 1.0)\n",
    "        h, w = img_size\n",
    "        new_h = tf.cast(tf.cast(h, tf.float32) * crop_frac, tf.int32)\n",
    "        new_w = tf.cast(tf.cast(w, tf.float32) * crop_frac, tf.int32)\n",
    "        img = tf.image.random_crop(img, [new_h, new_w, 3])\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        return img, label\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=min(len(paths), 10000), seed=CONFIG['seed'], reshuffle_each_iteration=True)\n",
    "    \n",
    "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_datasets_for_model(model_name: str, use_cache: bool = True) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"Create train, val, test datasets with model-specific preprocessing (cached).\"\"\"\n",
    "    cache_key = model_name\n",
    "    \n",
    "    if use_cache and cache_key in _dataset_cache:\n",
    "        return _dataset_cache[cache_key]\n",
    "    \n",
    "    preprocess_fn = PREPROCESS_FUNCTIONS[model_name]\n",
    "    \n",
    "    train_ds = create_dataset_pipeline(\n",
    "        train_paths, train_labels, preprocess_fn,\n",
    "        CONFIG['img_size'], CONFIG['batch_size'],\n",
    "        shuffle=True, augment=True, cache=False  # Don't cache augmented data\n",
    "    )\n",
    "    val_ds = create_dataset_pipeline(\n",
    "        val_paths, val_labels, preprocess_fn,\n",
    "        CONFIG['img_size'], CONFIG['batch_size'],\n",
    "        shuffle=False, augment=False, cache=True  # Cache validation data\n",
    "    )\n",
    "    test_ds = create_dataset_pipeline(\n",
    "        test_paths, test_labels, preprocess_fn,\n",
    "        CONFIG['img_size'], CONFIG['batch_size'],\n",
    "        shuffle=False, augment=False, cache=True  # Cache test data\n",
    "    )\n",
    "    \n",
    "    if use_cache:\n",
    "        _dataset_cache[cache_key] = (train_ds, val_ds, test_ds)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def clear_dataset_cache():\n",
    "    \"\"\"Clear the dataset cache to free memory.\"\"\"\n",
    "    global _dataset_cache\n",
    "    _dataset_cache = {}\n",
    "    print(\"Dataset cache cleared.\")\n",
    "\n",
    "print(\"Dataset pipeline factory created with enhanced augmentation and caching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707735a2",
   "metadata": {},
   "source": [
    "## 8. Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model factory created.\n",
      "Models available: ['VGG19', 'ResNet101', 'DenseNet121']\n"
     ]
    }
   ],
   "source": [
    "BASE_MODELS = {\n",
    "    'VGG19': VGG19,\n",
    "    'ResNet50': ResNet50,\n",
    "    'DenseNet121': DenseNet121,\n",
    "}\n",
    "\n",
    "# Number of layers to unfreeze for fine-tuning\n",
    "FINE_TUNE_LAYERS = 20\n",
    "\n",
    "def create_model(model_name: str, num_classes: int, img_size: Tuple[int, int], fine_tune: bool = True) -> Model:\n",
    "    \"\"\"Create a transfer learning model with fine-tuning of last 20 layers.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the base model ('VGG19', 'ResNet50', 'DenseNet121')\n",
    "        num_classes: Number of output classes\n",
    "        img_size: Input image size (height, width)\n",
    "        fine_tune: If True, unfreeze last FINE_TUNE_LAYERS layers for training\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model with fine-tuning enabled\n",
    "    \"\"\"\n",
    "    \n",
    "    base_model_class = BASE_MODELS[model_name]\n",
    "    \n",
    "    base_model = base_model_class(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(*img_size, 3),\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze all layers first\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Unfreeze last FINE_TUNE_LAYERS layers for genuine training on Caltech-101\n",
    "    if fine_tune:\n",
    "        for layer in base_model.layers[-FINE_TUNE_LAYERS:]:\n",
    "            layer.trainable = True\n",
    "        print(f\"  Fine-tuning enabled: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "    \n",
    "    inputs = keras.Input(shape=(*img_size, 3))\n",
    "    x = base_model(inputs, training=fine_tune)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=f\"{model_name}_transfer\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Model factory created with fine-tuning support.\")\n",
    "print(f\"Models available: {list(BASE_MODELS.keys())}\")\n",
    "print(f\"Fine-tune layers: Last {FINE_TUNE_LAYERS} layers will be unfrozen for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3164611",
   "metadata": {},
   "source": [
    "## 9. Callbacks Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks factory created.\n"
     ]
    }
   ],
   "source": [
    "# Minimum epochs before early stopping can trigger\n",
    "MIN_EPOCHS_BEFORE_EARLY_STOP = 20\n",
    "\n",
    "def get_callbacks(model_name: str) -> list:\n",
    "    \"\"\"Create callbacks for training with minimum epoch threshold.\n",
    "    \n",
    "    Early stopping won't trigger until at least MIN_EPOCHS_BEFORE_EARLY_STOP\n",
    "    epochs have completed, ensuring sufficient training on Caltech-101.\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.keras\"\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=CONFIG['patience_early_stop'],\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            start_from_epoch=MIN_EPOCHS_BEFORE_EARLY_STOP  # Don't stop before epoch 20\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(checkpoint_path),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=CONFIG['patience_lr_reduce'],\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "print(\"Callbacks factory created.\")\n",
    "print(f\"Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa03106",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training storage initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store results\n",
    "training_histories: Dict[str, keras.callbacks.History] = {}\n",
    "trained_models: Dict[str, Model] = {}\n",
    "\n",
    "print(\"Training storage initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae593887",
   "metadata": {},
   "source": [
    "## 10.1 Train VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9975da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training VGG19\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770072071.045398  118378 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3398 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No checkpoint found. Creating new VGG19 model...\n",
      "\n",
      "VGG19 Summary:\n",
      "  Total params: 20,446,630\n",
      "  Trainable params: 421,222\n",
      "\n",
      "--- Phase 1: Training classification head ---\n",
      "Early Stopping: patience=10, monitor='val_loss'\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:11:13.318048: I external/local_xla/xla/service/service.cc:163] XLA service 0x716290010d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2026-02-03 04:11:13.318073: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2026-02-03 04:11:13.551643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2026-02-03 04:11:13.876280: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91801\n",
      "2026-02-03 04:11:13.954477: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954503: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954548: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954559: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954565: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:14.523011: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:14.957133: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1886', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.014894: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.246000: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1778', 332 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.726707: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1834', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:19.056332: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:11:20.040428: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2026-02-03 04:11:22.024797: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:11:24.686833: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "I0000 00:00:1770072090.042138  118908 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.0175 - loss: 5.0671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:12:06.974217: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:06.974246: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:07.140540: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.698903: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1759', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.792575: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.802529: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 364 bytes spill stores, 364 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:09.977328: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.0176 - loss: 5.0662"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:12:18.807080: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:19.289817: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:19.528417: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 324 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:25.016513: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:25.489088: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:25.760911: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 372 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:27.578850: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.16521, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 1: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 283ms/step - accuracy: 0.0271 - loss: 4.8654 - val_accuracy: 0.1652 - val_loss: 3.9945 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.0751 - loss: 4.2922\n",
      "Epoch 2: val_accuracy improved from 0.16521 to 0.44748, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 2: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 188ms/step - accuracy: 0.1012 - loss: 4.2041 - val_accuracy: 0.4475 - val_loss: 3.2475 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.1892 - loss: 3.7958\n",
      "Epoch 3: val_accuracy improved from 0.44748 to 0.59081, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 3: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.2224 - loss: 3.6200 - val_accuracy: 0.5908 - val_loss: 2.3353 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.3180 - loss: 3.1739\n",
      "Epoch 4: val_accuracy improved from 0.59081 to 0.68600, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 4: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.3357 - loss: 3.0777 - val_accuracy: 0.6860 - val_loss: 1.6646 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4115 - loss: 2.6430\n",
      "Epoch 5: val_accuracy improved from 0.68600 to 0.75492, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 5: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.4171 - loss: 2.6182 - val_accuracy: 0.7549 - val_loss: 1.2716 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4703 - loss: 2.2797\n",
      "Epoch 6: val_accuracy improved from 0.75492 to 0.78665, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 6: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.4726 - loss: 2.2538 - val_accuracy: 0.7867 - val_loss: 1.0130 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.5234 - loss: 2.0230\n",
      "Epoch 7: val_accuracy improved from 0.78665 to 0.80744, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 7: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 185ms/step - accuracy: 0.5277 - loss: 2.0076 - val_accuracy: 0.8074 - val_loss: 0.8755 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5479 - loss: 1.8607\n",
      "Epoch 8: val_accuracy improved from 0.80744 to 0.82166, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 8: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.5545 - loss: 1.8373 - val_accuracy: 0.8217 - val_loss: 0.7833 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5776 - loss: 1.6751\n",
      "Epoch 9: val_accuracy improved from 0.82166 to 0.82823, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 9: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.5844 - loss: 1.6671 - val_accuracy: 0.8282 - val_loss: 0.7129 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6123 - loss: 1.5647\n",
      "Epoch 10: val_accuracy improved from 0.82823 to 0.84573, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.6168 - loss: 1.5359 - val_accuracy: 0.8457 - val_loss: 0.6686 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6205 - loss: 1.4827\n",
      "Epoch 11: val_accuracy did not improve from 0.84573\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.6276 - loss: 1.4301 - val_accuracy: 0.8435 - val_loss: 0.6404 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6562 - loss: 1.3431\n",
      "Epoch 12: val_accuracy did not improve from 0.84573\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.6485 - loss: 1.3616 - val_accuracy: 0.8457 - val_loss: 0.6060 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6676 - loss: 1.2603\n",
      "Epoch 13: val_accuracy improved from 0.84573 to 0.85449, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 13: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6651 - loss: 1.2792 - val_accuracy: 0.8545 - val_loss: 0.5856 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.6646 - loss: 1.2279\n",
      "Epoch 14: val_accuracy did not improve from 0.85449\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6700 - loss: 1.2435 - val_accuracy: 0.8534 - val_loss: 0.5722 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6895 - loss: 1.1857\n",
      "Epoch 15: val_accuracy improved from 0.85449 to 0.85667, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 15: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6850 - loss: 1.1818 - val_accuracy: 0.8567 - val_loss: 0.5624 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6919 - loss: 1.1099\n",
      "Epoch 16: val_accuracy improved from 0.85667 to 0.86214, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 16: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6965 - loss: 1.0878 - val_accuracy: 0.8621 - val_loss: 0.5351 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7153 - loss: 1.0528\n",
      "Epoch 17: val_accuracy improved from 0.86214 to 0.86433, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 17: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.7106 - loss: 1.0730 - val_accuracy: 0.8643 - val_loss: 0.5359 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7109 - loss: 1.0560\n",
      "Epoch 18: val_accuracy improved from 0.86433 to 0.86980, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 18: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7169 - loss: 1.0324 - val_accuracy: 0.8698 - val_loss: 0.5131 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7331 - loss: 0.9986\n",
      "Epoch 19: val_accuracy did not improve from 0.86980\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.7347 - loss: 0.9769 - val_accuracy: 0.8698 - val_loss: 0.5035 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7432 - loss: 0.9378\n",
      "Epoch 20: val_accuracy improved from 0.86980 to 0.87418, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 20: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7373 - loss: 0.9712 - val_accuracy: 0.8742 - val_loss: 0.5003 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7494 - loss: 0.9141\n",
      "Epoch 21: val_accuracy improved from 0.87418 to 0.87637, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 21: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.7486 - loss: 0.9227 - val_accuracy: 0.8764 - val_loss: 0.4935 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7483 - loss: 0.8808\n",
      "Epoch 22: val_accuracy improved from 0.87637 to 0.87965, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 22: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7430 - loss: 0.9231 - val_accuracy: 0.8796 - val_loss: 0.4855 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7553 - loss: 0.8726\n",
      "Epoch 23: val_accuracy did not improve from 0.87965\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7565 - loss: 0.8482 - val_accuracy: 0.8796 - val_loss: 0.4738 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7712 - loss: 0.8117\n",
      "Epoch 24: val_accuracy improved from 0.87965 to 0.88184, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 24: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7646 - loss: 0.8451 - val_accuracy: 0.8818 - val_loss: 0.4776 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7735 - loss: 0.7874\n",
      "Epoch 25: val_accuracy improved from 0.88184 to 0.88512, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 25: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7729 - loss: 0.7879 - val_accuracy: 0.8851 - val_loss: 0.4656 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7653 - loss: 0.8088\n",
      "Epoch 26: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.7702 - loss: 0.7968 - val_accuracy: 0.8840 - val_loss: 0.4653 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7780 - loss: 0.7625\n",
      "Epoch 27: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7776 - loss: 0.7566 - val_accuracy: 0.8807 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7867 - loss: 0.7270\n",
      "Epoch 28: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7813 - loss: 0.7542 - val_accuracy: 0.8840 - val_loss: 0.4552 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7947 - loss: 0.7123\n",
      "Epoch 29: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7975 - loss: 0.7098 - val_accuracy: 0.8829 - val_loss: 0.4563 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7939 - loss: 0.7052\n",
      "Epoch 30: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7904 - loss: 0.7121 - val_accuracy: 0.8829 - val_loss: 0.4474 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7973 - loss: 0.6798\n",
      "Epoch 31: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7917 - loss: 0.6916 - val_accuracy: 0.8807 - val_loss: 0.4428 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8131 - loss: 0.6363\n",
      "Epoch 32: val_accuracy improved from 0.88512 to 0.88621, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 32: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8066 - loss: 0.6662 - val_accuracy: 0.8862 - val_loss: 0.4538 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8031 - loss: 0.6399\n",
      "Epoch 33: val_accuracy did not improve from 0.88621\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8014 - loss: 0.6668 - val_accuracy: 0.8818 - val_loss: 0.4389 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8039 - loss: 0.6542\n",
      "Epoch 34: val_accuracy improved from 0.88621 to 0.88950, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 34: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8040 - loss: 0.6449 - val_accuracy: 0.8895 - val_loss: 0.4341 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8153 - loss: 0.6051\n",
      "Epoch 35: val_accuracy improved from 0.88950 to 0.89059, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 35: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8116 - loss: 0.6207 - val_accuracy: 0.8906 - val_loss: 0.4390 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8078 - loss: 0.6354\n",
      "Epoch 36: val_accuracy improved from 0.89059 to 0.89168, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 36: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.8068 - loss: 0.6479 - val_accuracy: 0.8917 - val_loss: 0.4364 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8147 - loss: 0.5991\n",
      "Epoch 37: val_accuracy improved from 0.89168 to 0.89278, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 37: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8172 - loss: 0.6032 - val_accuracy: 0.8928 - val_loss: 0.4368 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8162 - loss: 0.5970\n",
      "Epoch 38: val_accuracy improved from 0.89278 to 0.89497, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 38: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8179 - loss: 0.5940 - val_accuracy: 0.8950 - val_loss: 0.4289 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8303 - loss: 0.5635\n",
      "Epoch 39: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8231 - loss: 0.5834 - val_accuracy: 0.8939 - val_loss: 0.4325 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8318 - loss: 0.5417\n",
      "Epoch 40: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8246 - loss: 0.5549 - val_accuracy: 0.8928 - val_loss: 0.4312 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8239 - loss: 0.5576\n",
      "Epoch 41: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8284 - loss: 0.5508 - val_accuracy: 0.8906 - val_loss: 0.4362 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8344 - loss: 0.5139\n",
      "Epoch 42: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8298 - loss: 0.5330 - val_accuracy: 0.8917 - val_loss: 0.4256 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8312 - loss: 0.5267\n",
      "Epoch 43: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8346 - loss: 0.5349 - val_accuracy: 0.8950 - val_loss: 0.4196 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8363 - loss: 0.5132\n",
      "Epoch 44: val_accuracy improved from 0.89497 to 0.89606, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 44: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8334 - loss: 0.5211 - val_accuracy: 0.8961 - val_loss: 0.4267 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8376 - loss: 0.5050\n",
      "Epoch 45: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8316 - loss: 0.5287 - val_accuracy: 0.8917 - val_loss: 0.4254 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8361 - loss: 0.4915\n",
      "Epoch 46: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8409 - loss: 0.5028 - val_accuracy: 0.8917 - val_loss: 0.4263 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8414 - loss: 0.5256\n",
      "Epoch 47: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8390 - loss: 0.5069 - val_accuracy: 0.8928 - val_loss: 0.4202 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8362 - loss: 0.5069\n",
      "Epoch 48: val_accuracy improved from 0.89606 to 0.89716, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 48: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.8402 - loss: 0.5071 - val_accuracy: 0.8972 - val_loss: 0.4186 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8423 - loss: 0.4724\n",
      "Epoch 49: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8416 - loss: 0.4951 - val_accuracy: 0.8917 - val_loss: 0.4109 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8484 - loss: 0.4619\n",
      "Epoch 50: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8498 - loss: 0.4568 - val_accuracy: 0.8972 - val_loss: 0.4175 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "--- Phase 2: Fine-tuning last 20 layers ---\n",
      "  Trainable params after unfreezing: 20,443,814\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:45:39.504251: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:39.504281: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:39.504289: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:40.322591: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1910', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.505592: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1858', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.516915: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1910', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.580995: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1802', 344 bytes spill stores, 344 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.681628: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1858', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:42.850510: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:45:46.157659: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:46:02.927408: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.41GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:46:17.397755: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569ms/step - accuracy: 0.8447 - loss: 0.4653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:48:29.113003: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681ms/step - accuracy: 0.8448 - loss: 0.4652\n",
      "Epoch 1: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 706ms/step - accuracy: 0.8476 - loss: 0.4549 - val_accuracy: 0.8939 - val_loss: 0.4132 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8542 - loss: 0.4510\n",
      "Epoch 2: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 568ms/step - accuracy: 0.8591 - loss: 0.4387 - val_accuracy: 0.8961 - val_loss: 0.4034 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8497 - loss: 0.4305\n",
      "Epoch 3: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8559 - loss: 0.4280 - val_accuracy: 0.8972 - val_loss: 0.4026 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8610 - loss: 0.3934\n",
      "Epoch 4: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 568ms/step - accuracy: 0.8578 - loss: 0.4062 - val_accuracy: 0.8972 - val_loss: 0.3908 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8639 - loss: 0.4180\n",
      "Epoch 5: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8584 - loss: 0.4162 - val_accuracy: 0.8961 - val_loss: 0.3927 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8641 - loss: 0.4126\n",
      "Epoch 6: val_accuracy improved from 0.89716 to 0.90044, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 6: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 569ms/step - accuracy: 0.8637 - loss: 0.4108 - val_accuracy: 0.9004 - val_loss: 0.3860 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553ms/step - accuracy: 0.8692 - loss: 0.3921\n",
      "Epoch 7: val_accuracy did not improve from 0.90044\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8625 - loss: 0.3970 - val_accuracy: 0.8982 - val_loss: 0.3906 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8705 - loss: 0.3976\n",
      "Epoch 8: val_accuracy did not improve from 0.90044\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 567ms/step - accuracy: 0.8662 - loss: 0.4006 - val_accuracy: 0.8993 - val_loss: 0.3860 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8627 - loss: 0.3916\n",
      "Epoch 9: val_accuracy improved from 0.90044 to 0.90263, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 9: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 575ms/step - accuracy: 0.8662 - loss: 0.3924 - val_accuracy: 0.9026 - val_loss: 0.3833 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8697 - loss: 0.3687\n",
      "Epoch 10: val_accuracy improved from 0.90263 to 0.90591, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 569ms/step - accuracy: 0.8684 - loss: 0.3651 - val_accuracy: 0.9059 - val_loss: 0.3794 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "VGG19 training complete!\n",
      "Total epochs: 60\n"
     ]
    }
   ],
   "source": [
    "# Train VGG19\n",
    "model_name = 'VGG19'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training {model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "train_ds, val_ds, test_ds = get_datasets_for_model(model_name)\n",
    "\n",
    "# Check if checkpoint exists and load it, otherwise create new model\n",
    "checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.keras\"\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"\\nLoading existing model from: {checkpoint_path}\")\n",
    "    model = keras.models.load_model(checkpoint_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"\\nNo checkpoint found. Creating new {model_name} model...\")\n",
    "    model = create_model(model_name, NUM_CLASSES, CONFIG['img_size'])\n",
    "\n",
    "print(f\"\\n{model_name} Summary:\")\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.reduce_prod(v.shape).numpy() for v in model.trainable_variables])\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Fine-tuned layers: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "print(f\"  Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")\n",
    "\n",
    "callbacks = get_callbacks(model_name)\n",
    "\n",
    "print(f\"\\n--- Training with fine-tuning (last {FINE_TUNE_LAYERS} layers) ---\")\n",
    "print(f\"Early Stopping: patience={CONFIG['patience_early_stop']}, monitor='val_loss'\")\n",
    "print(f\"Early Stopping: start_from_epoch={MIN_EPOCHS_BEFORE_EARLY_STOP} (minimum training guaranteed)\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHT_DICT,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_histories[model_name] = history\n",
    "trained_models[model_name] = model\n",
    "\n",
    "print(f\"\\n{model_name} training complete!\")\n",
    "print(f\"Total epochs: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c286d",
   "metadata": {},
   "source": [
    "## 10.2 Train ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training ResNet101\n",
      "================================================================================\n",
      "\n",
      "No checkpoint found. Creating new ResNet101 model...\n",
      "\n",
      "ResNet101 Summary:\n",
      "  Total params: 43,872,998\n",
      "  Trainable params: 1,210,726\n",
      "\n",
      "Early Stopping: patience=10, monitor='val_loss', restore_best_weights=True\n",
      "LR Reduction: patience=5, factor=0.5, min_lr=1e-7\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 05:11:27.489362: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 05:11:27.918270: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10319', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2026-02-03 05:11:28.307889: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10757', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.0297 - loss: 5.0476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 05:12:04.978442: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10742', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2026-02-03 05:12:05.126289: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10319', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.0298 - loss: 5.0459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 05:12:17.183926: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3317', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.47812, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 1: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 206ms/step - accuracy: 0.0637 - loss: 4.6594 - val_accuracy: 0.4781 - val_loss: 3.2524 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.2128 - loss: 3.5421\n",
      "Epoch 2: val_accuracy improved from 0.47812 to 0.71772, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 2: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.2708 - loss: 3.3707 - val_accuracy: 0.7177 - val_loss: 1.9173 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.4246 - loss: 2.6231\n",
      "Epoch 3: val_accuracy improved from 0.71772 to 0.80197, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 3: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.4515 - loss: 2.4744 - val_accuracy: 0.8020 - val_loss: 1.1202 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.5265 - loss: 1.9916\n",
      "Epoch 4: val_accuracy improved from 0.80197 to 0.84464, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 4: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - accuracy: 0.5485 - loss: 1.8839 - val_accuracy: 0.8446 - val_loss: 0.7844 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6052 - loss: 1.6093\n",
      "Epoch 5: val_accuracy improved from 0.84464 to 0.85886, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 5: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.6146 - loss: 1.5749 - val_accuracy: 0.8589 - val_loss: 0.6429 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6642 - loss: 1.3325\n",
      "Epoch 6: val_accuracy improved from 0.85886 to 0.86980, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 6: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.6688 - loss: 1.3181 - val_accuracy: 0.8698 - val_loss: 0.5594 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7038 - loss: 1.1118\n",
      "Epoch 7: val_accuracy did not improve from 0.86980\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - accuracy: 0.7012 - loss: 1.1347 - val_accuracy: 0.8654 - val_loss: 0.5217 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7153 - loss: 1.0524\n",
      "Epoch 8: val_accuracy improved from 0.86980 to 0.87637, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 8: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7198 - loss: 1.0439 - val_accuracy: 0.8764 - val_loss: 0.4904 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7497 - loss: 0.9227\n",
      "Epoch 9: val_accuracy improved from 0.87637 to 0.87965, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 9: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7468 - loss: 0.9330 - val_accuracy: 0.8796 - val_loss: 0.4594 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7576 - loss: 0.8774\n",
      "Epoch 10: val_accuracy improved from 0.87965 to 0.88184, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7572 - loss: 0.8721 - val_accuracy: 0.8818 - val_loss: 0.4311 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7649 - loss: 0.8402\n",
      "Epoch 11: val_accuracy did not improve from 0.88184\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 131ms/step - accuracy: 0.7705 - loss: 0.8006 - val_accuracy: 0.8818 - val_loss: 0.4148 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7772 - loss: 0.7887\n",
      "Epoch 12: val_accuracy improved from 0.88184 to 0.88731, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 12: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7861 - loss: 0.7475 - val_accuracy: 0.8873 - val_loss: 0.3990 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7970 - loss: 0.6880\n",
      "Epoch 13: val_accuracy improved from 0.88731 to 0.89387, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 13: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7944 - loss: 0.7025 - val_accuracy: 0.8939 - val_loss: 0.3895 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8143 - loss: 0.6416\n",
      "Epoch 14: val_accuracy improved from 0.89387 to 0.89497, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 14: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.8100 - loss: 0.6575 - val_accuracy: 0.8950 - val_loss: 0.3836 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8188 - loss: 0.6160\n",
      "Epoch 15: val_accuracy improved from 0.89497 to 0.89934, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 15: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.8159 - loss: 0.6112 - val_accuracy: 0.8993 - val_loss: 0.3791 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8171 - loss: 0.5947\n",
      "Epoch 16: val_accuracy improved from 0.89934 to 0.90372, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 16: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.8165 - loss: 0.5834 - val_accuracy: 0.9037 - val_loss: 0.3535 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8371 - loss: 0.5239\n",
      "Epoch 17: val_accuracy did not improve from 0.90372\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 131ms/step - accuracy: 0.8334 - loss: 0.5379 - val_accuracy: 0.9004 - val_loss: 0.3700 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8337 - loss: 0.5440\n",
      "Epoch 18: val_accuracy did not improve from 0.90372\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 131ms/step - accuracy: 0.8353 - loss: 0.5348 - val_accuracy: 0.9037 - val_loss: 0.3635 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8390 - loss: 0.5155\n",
      "Epoch 19: val_accuracy improved from 0.90372 to 0.90481, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 19: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 139ms/step - accuracy: 0.8370 - loss: 0.5062 - val_accuracy: 0.9048 - val_loss: 0.3504 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8532 - loss: 0.4428\n",
      "Epoch 20: val_accuracy improved from 0.90481 to 0.90810, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 20: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.8499 - loss: 0.4668 - val_accuracy: 0.9081 - val_loss: 0.3457 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8504 - loss: 0.4668\n",
      "Epoch 21: val_accuracy did not improve from 0.90810\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - accuracy: 0.8540 - loss: 0.4662 - val_accuracy: 0.9037 - val_loss: 0.3552 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8665 - loss: 0.4038\n",
      "Epoch 22: val_accuracy improved from 0.90810 to 0.90919, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 22: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 132ms/step - accuracy: 0.8606 - loss: 0.4289 - val_accuracy: 0.9092 - val_loss: 0.3579 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8623 - loss: 0.4235\n",
      "Epoch 23: val_accuracy did not improve from 0.90919\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 128ms/step - accuracy: 0.8627 - loss: 0.4079 - val_accuracy: 0.9070 - val_loss: 0.3495 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8618 - loss: 0.4186\n",
      "Epoch 24: val_accuracy improved from 0.90919 to 0.91028, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 24: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 131ms/step - accuracy: 0.8599 - loss: 0.4134 - val_accuracy: 0.9103 - val_loss: 0.3552 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8742 - loss: 0.3598\n",
      "Epoch 25: val_accuracy did not improve from 0.91028\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - accuracy: 0.8720 - loss: 0.3721 - val_accuracy: 0.9092 - val_loss: 0.3414 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8704 - loss: 0.3779\n",
      "Epoch 26: val_accuracy improved from 0.91028 to 0.91685, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 26: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 133ms/step - accuracy: 0.8740 - loss: 0.3690 - val_accuracy: 0.9168 - val_loss: 0.3390 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8783 - loss: 0.3403\n",
      "Epoch 27: val_accuracy did not improve from 0.91685\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.8789 - loss: 0.3540 - val_accuracy: 0.9103 - val_loss: 0.3487 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8817 - loss: 0.3318\n",
      "Epoch 28: val_accuracy did not improve from 0.91685\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.8802 - loss: 0.3387 - val_accuracy: 0.9136 - val_loss: 0.3395 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8769 - loss: 0.3264\n",
      "Epoch 29: val_accuracy improved from 0.91685 to 0.91794, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 29: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 137ms/step - accuracy: 0.8794 - loss: 0.3312 - val_accuracy: 0.9179 - val_loss: 0.3416 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8846 - loss: 0.3320\n",
      "Epoch 30: val_accuracy improved from 0.91794 to 0.91904, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 30: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 133ms/step - accuracy: 0.8841 - loss: 0.3311 - val_accuracy: 0.9190 - val_loss: 0.3336 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8838 - loss: 0.3129\n",
      "Epoch 31: val_accuracy improved from 0.91904 to 0.92341, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 31: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 133ms/step - accuracy: 0.8811 - loss: 0.3097 - val_accuracy: 0.9234 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9037 - loss: 0.2567\n",
      "Epoch 32: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.9040 - loss: 0.2725 - val_accuracy: 0.9190 - val_loss: 0.3453 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9012 - loss: 0.2893\n",
      "Epoch 33: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 130ms/step - accuracy: 0.8983 - loss: 0.3005 - val_accuracy: 0.9223 - val_loss: 0.3393 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8935 - loss: 0.2908\n",
      "Epoch 34: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.8927 - loss: 0.2962 - val_accuracy: 0.9147 - val_loss: 0.3386 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8980 - loss: 0.2711\n",
      "Epoch 35: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9005 - loss: 0.2660 - val_accuracy: 0.9158 - val_loss: 0.3412 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9078 - loss: 0.2911\n",
      "Epoch 36: val_accuracy did not improve from 0.92341\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9017 - loss: 0.2948 - val_accuracy: 0.9168 - val_loss: 0.3382 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8970 - loss: 0.2601\n",
      "Epoch 37: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9031 - loss: 0.2511 - val_accuracy: 0.9190 - val_loss: 0.3337 - learning_rate: 5.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9166 - loss: 0.2243\n",
      "Epoch 38: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9191 - loss: 0.2239 - val_accuracy: 0.9179 - val_loss: 0.3317 - learning_rate: 5.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9130 - loss: 0.2325\n",
      "Epoch 39: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9090 - loss: 0.2399 - val_accuracy: 0.9201 - val_loss: 0.3322 - learning_rate: 5.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9085 - loss: 0.2141\n",
      "Epoch 40: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9132 - loss: 0.2182 - val_accuracy: 0.9212 - val_loss: 0.3263 - learning_rate: 5.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9039 - loss: 0.2384\n",
      "Epoch 41: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9094 - loss: 0.2293 - val_accuracy: 0.9179 - val_loss: 0.3363 - learning_rate: 5.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9165 - loss: 0.2115\n",
      "Epoch 42: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9146 - loss: 0.2193 - val_accuracy: 0.9212 - val_loss: 0.3218 - learning_rate: 5.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9168 - loss: 0.2109\n",
      "Epoch 43: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9147 - loss: 0.2248 - val_accuracy: 0.9201 - val_loss: 0.3230 - learning_rate: 5.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9207 - loss: 0.1938\n",
      "Epoch 44: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 129ms/step - accuracy: 0.9224 - loss: 0.1863 - val_accuracy: 0.9223 - val_loss: 0.3291 - learning_rate: 5.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9254 - loss: 0.1893\n",
      "Epoch 45: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9195 - loss: 0.2095 - val_accuracy: 0.9201 - val_loss: 0.3337 - learning_rate: 5.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9309 - loss: 0.1800\n",
      "Epoch 46: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9271 - loss: 0.1951 - val_accuracy: 0.9179 - val_loss: 0.3273 - learning_rate: 5.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9257 - loss: 0.1966\n",
      "Epoch 47: val_accuracy did not improve from 0.92341\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9234 - loss: 0.1966 - val_accuracy: 0.9179 - val_loss: 0.3271 - learning_rate: 5.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9214 - loss: 0.1747\n",
      "Epoch 48: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9211 - loss: 0.1901 - val_accuracy: 0.9190 - val_loss: 0.3244 - learning_rate: 2.5000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9285 - loss: 0.2035\n",
      "Epoch 49: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 129ms/step - accuracy: 0.9241 - loss: 0.2000 - val_accuracy: 0.9190 - val_loss: 0.3264 - learning_rate: 2.5000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9210 - loss: 0.1959\n",
      "Epoch 50: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9213 - loss: 0.1913 - val_accuracy: 0.9234 - val_loss: 0.3195 - learning_rate: 2.5000e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "ResNet101 training complete!\n",
      "Stopped at epoch: 50/50\n"
     ]
    }
   ],
   "source": [
    "# Train ResNet50\n",
    "model_name = 'ResNet50'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training {model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "train_ds, val_ds, test_ds = get_datasets_for_model(model_name)\n",
    "\n",
    "# Check if checkpoint exists and load it, otherwise create new model\n",
    "checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.keras\"\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"\\nLoading existing model from: {checkpoint_path}\")\n",
    "    model = keras.models.load_model(checkpoint_path)\n",
    "    print(\"Model loaded successfully! Resuming training...\")\n",
    "else:\n",
    "    print(f\"\\nNo checkpoint found. Creating new {model_name} model...\")\n",
    "    model = create_model(model_name, NUM_CLASSES, CONFIG['img_size'])\n",
    "\n",
    "print(f\"\\n{model_name} Summary:\")\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.reduce_prod(v.shape).numpy() for v in model.trainable_variables])\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Fine-tuned layers: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "print(f\"  Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")\n",
    "\n",
    "callbacks = get_callbacks(model_name)\n",
    "\n",
    "print(f\"\\nEarly Stopping: patience={CONFIG['patience_early_stop']}, monitor='val_loss', restore_best_weights=True\")\n",
    "print(f\"Early Stopping: start_from_epoch={MIN_EPOCHS_BEFORE_EARLY_STOP} (minimum training guaranteed)\")\n",
    "print(f\"LR Reduction: patience={CONFIG['patience_lr_reduce']}, factor=0.5, min_lr=1e-7\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHT_DICT,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_histories[model_name] = history\n",
    "trained_models[model_name] = model\n",
    "\n",
    "print(f\"Stopped at epoch: {len(history.history['loss'])}/{CONFIG['epochs']}\")\n",
    "print(f\"\\n{model_name} training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10073d40",
   "metadata": {},
   "source": [
    "## 10.3 Train DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a23a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DenseNet121\n",
    "model_name = 'DenseNet121'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training {model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "train_ds, val_ds, test_ds = get_datasets_for_model(model_name)\n",
    "\n",
    "# Check if checkpoint exists and load it, otherwise create new model\n",
    "checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.keras\"\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"\\nLoading existing model from: {checkpoint_path}\")\n",
    "    model = keras.models.load_model(checkpoint_path)\n",
    "    print(\"Model loaded successfully! Resuming training...\")\n",
    "else:\n",
    "    print(f\"\\nNo checkpoint found. Creating new {model_name} model...\")\n",
    "    model = create_model(model_name, NUM_CLASSES, CONFIG['img_size'])\n",
    "\n",
    "print(f\"\\n{model_name} Summary:\")\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.reduce_prod(v.shape).numpy() for v in model.trainable_variables])\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Fine-tuned layers: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "print(f\"  Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")\n",
    "\n",
    "callbacks = get_callbacks(model_name)\n",
    "\n",
    "print(f\"\\nEarly Stopping: patience={CONFIG['patience_early_stop']}, monitor='val_loss', restore_best_weights=True\")\n",
    "print(f\"Early Stopping: start_from_epoch={MIN_EPOCHS_BEFORE_EARLY_STOP} (minimum training guaranteed)\")\n",
    "print(f\"LR Reduction: patience={CONFIG['patience_lr_reduce']}, factor=0.5, min_lr=1e-7\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHT_DICT,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_histories[model_name] = history\n",
    "trained_models[model_name] = model\n",
    "\n",
    "print(f\"Stopped at epoch: {len(history.history['loss'])}/{CONFIG['epochs']}\")\n",
    "print(f\"\\n{model_name} training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b405e",
   "metadata": {},
   "source": [
    "## 11. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6997f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_results = {}\n",
    "model_predictions = {}  # Cache predictions to avoid recomputing\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    _, _, test_ds = get_datasets_for_model(model_name)\n",
    "    model = trained_models[model_name]\n",
    "    \n",
    "    # Get predictions once and reuse\n",
    "    print(f\"  Computing predictions...\", end=\" \", flush=True)\n",
    "    y_pred_probs = model.predict(test_ds, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = test_labels.astype(int)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    # Cache for later use\n",
    "    model_predictions[model_name] = {\n",
    "        'probs': y_pred_probs,\n",
    "        'pred': y_pred,\n",
    "        'true': y_true\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics from predictions\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    loss, _ = model.evaluate(test_ds, verbose=0)\n",
    "    \n",
    "    test_results[model_name] = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"  Test Loss: {loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Model':<15} {'Test Loss':<12} {'Test Accuracy':<15}\")\n",
    "print(\"-\" * 42)\n",
    "for model_name, results in test_results.items():\n",
    "    print(f\"{model_name:<15} {results['loss']:<12.4f} {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "best_model = max(test_results.keys(), key=lambda k: test_results[k]['accuracy'])\n",
    "print(f\"\\nBest performing model: {best_model} with {test_results[best_model]['accuracy']*100:.2f}% accuracy\")\n",
    "\n",
    "# --- Detailed diagnostics for each model (using cached predictions) ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    \n",
    "    # Use cached predictions\n",
    "    y_pred = model_predictions[model_name]['pred']\n",
    "    y_true = model_predictions[model_name]['true']\n",
    "    \n",
    "    # Confusion matrix - find most confused pairs\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_no_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_no_diag, 0)\n",
    "    \n",
    "    print(\"Top 5 most confused class pairs (true  pred):\")\n",
    "    flat_idx = np.argsort(cm_no_diag.ravel())[-5:][::-1]\n",
    "    for idx in flat_idx:\n",
    "        i, j = divmod(idx, cm.shape[1])\n",
    "        if cm_no_diag[i, j] > 0:\n",
    "            print(f\"  {CLASS_NAMES[i]}  {CLASS_NAMES[j]}: {cm_no_diag[i, j]}\")\n",
    "    \n",
    "    # Worst classes by F1\n",
    "    report = classification_report(y_true, y_pred, target_names=CLASS_NAMES, output_dict=True, zero_division=0)\n",
    "    worst = sorted([(c, report[c]['f1-score']) for c in CLASS_NAMES], key=lambda x: x[1])[:5]\n",
    "    print(\"Worst 5 classes by F1-score:\")\n",
    "    for cls, f1 in worst:\n",
    "        print(f\"  {cls}: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e289627",
   "metadata": {},
   "source": [
    "## 12. Misclassification Analysis & Grad-CAM\n",
    "\n",
    "Visualize misclassified samples and use Grad-CAM to understand where the model focuses its attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misclassified_samples(model_name: str):\n",
    "    \"\"\"Get misclassified test samples for a model (uses cached predictions from evaluation).\"\"\"\n",
    "    # Reuse predictions from evaluation cell if available\n",
    "    if model_name in model_predictions:\n",
    "        y_pred_probs = model_predictions[model_name]['probs']\n",
    "        y_pred = model_predictions[model_name]['pred']\n",
    "        y_true = model_predictions[model_name]['true']\n",
    "    else:\n",
    "        # Fallback: compute predictions\n",
    "        print(f\"  Computing predictions for {model_name}...\", end=\" \", flush=True)\n",
    "        model = trained_models[model_name]\n",
    "        _, _, test_ds = get_datasets_for_model(model_name)\n",
    "        y_pred_probs = model.predict(test_ds, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        y_true = test_labels.astype(int)\n",
    "        model_predictions[model_name] = {'probs': y_pred_probs, 'pred': y_pred, 'true': y_true}\n",
    "        print(\"Done!\")\n",
    "    \n",
    "    misclassified_idx = np.where(y_pred != y_true)[0]\n",
    "    return misclassified_idx, y_pred, y_pred_probs, y_true\n",
    "\n",
    "def plot_misclassified(model_name: str, n_show: int = 8):\n",
    "    \"\"\"Display misclassified images for a model.\"\"\"\n",
    "    misclassified_idx, y_pred, y_pred_probs, y_true = get_misclassified_samples(model_name)\n",
    "    \n",
    "    n_show = min(n_show, len(misclassified_idx))\n",
    "    if n_show == 0:\n",
    "        print(f\"No misclassified samples for {model_name}!\")\n",
    "        return\n",
    "    \n",
    "    cols = 4\n",
    "    rows = (n_show + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 4 * rows))\n",
    "    axes = axes.flatten() if n_show > 1 else [axes]\n",
    "    \n",
    "    for i, idx in enumerate(misclassified_idx[:n_show]):\n",
    "        img = plt.imread(test_paths[idx])\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        conf = y_pred_probs[idx, y_pred[idx]] * 100\n",
    "        axes[i].set_title(f\"True: {CLASS_NAMES[y_true[idx]]}\\nPred: {CLASS_NAMES[y_pred[idx]]} ({conf:.1f}%)\", fontsize=9)\n",
    "    \n",
    "    for i in range(n_show, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"{model_name} - Misclassified Samples ({len(misclassified_idx)} total)\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name.lower()}_misclassified.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Cache for feature models to avoid recreating\n",
    "_feature_model_cache = {}\n",
    "\n",
    "def visualize_gradcam(model_name: str, img_idx: int = None):\n",
    "    \"\"\"Visualize activation heatmap for a misclassified image (optimized).\"\"\"\n",
    "    print(f\"  Generating activation map for {model_name}...\", end=\" \", flush=True)\n",
    "    \n",
    "    misclassified_idx, y_pred, y_pred_probs, y_true = get_misclassified_samples(model_name)\n",
    "    \n",
    "    if len(misclassified_idx) == 0:\n",
    "        print(f\"No misclassified samples for {model_name}\")\n",
    "        return\n",
    "    \n",
    "    idx = misclassified_idx[0] if img_idx is None else img_idx\n",
    "    img_path = test_paths[idx]\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, CONFIG['img_size'])\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img_preprocessed = PREPROCESS_FUNCTIONS[model_name](img)\n",
    "    img_array = tf.expand_dims(img_preprocessed, 0)\n",
    "    \n",
    "    # Get or create feature model (cached)\n",
    "    if model_name not in _feature_model_cache:\n",
    "        model = trained_models[model_name]\n",
    "        base = model.layers[1]\n",
    "        last_conv_name = [l.name for l in base.layers if 'conv' in l.name][-1]\n",
    "        _feature_model_cache[model_name] = Model(inputs=base.input, outputs=base.get_layer(last_conv_name).output)\n",
    "    \n",
    "    feature_model = _feature_model_cache[model_name]\n",
    "    \n",
    "    # Get features and create heatmap from activations\n",
    "    features = feature_model(img_array)\n",
    "    heatmap = tf.reduce_mean(features, axis=-1)[0]\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)\n",
    "    heatmap = heatmap.numpy()\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "    # Load original image and create overlay\n",
    "    orig_img = plt.imread(img_path)\n",
    "    heatmap_resized = np.array(tf.image.resize(heatmap[..., np.newaxis], orig_img.shape[:2]))[:, :, 0]\n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized)\n",
    "    \n",
    "    cmap = plt.cm.jet(heatmap_resized)[:, :, :3]\n",
    "    overlay = 0.5 * cmap + 0.5 * (orig_img / 255.0 if orig_img.max() > 1 else orig_img)\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title(f\"Original\\nTrue: {CLASS_NAMES[y_true[idx]]}\", fontsize=11)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(heatmap, cmap='jet')\n",
    "    axes[1].set_title(\"Activation Heatmap\", fontsize=11)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(f\"Overlay\\nPred: {CLASS_NAMES[y_pred[idx]]} ({y_pred_probs[idx, y_pred[idx]]*100:.1f}%)\", fontsize=11)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"{model_name} Activation Analysis\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name.lower()}_gradcam.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Show misclassified samples and activation map for VGG19\n",
    "print(\"=\"*80)\n",
    "print(\"VGG19 MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "plot_misclassified('VGG19', n_show=8)\n",
    "visualize_gradcam('VGG19')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6e90a",
   "metadata": {},
   "source": [
    "## 13. Training Curves Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ba0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories: Dict[str, keras.callbacks.History], metric: str = 'accuracy'):\n",
    "    \"\"\"Plot training curves for all models.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for idx, (model_name, history) in enumerate(histories.items()):\n",
    "        epochs = range(1, len(history.history[metric]) + 1)\n",
    "        \n",
    "        axes[0].plot(epochs, history.history[metric], \n",
    "                     color=colors[idx], linestyle='-', linewidth=2,\n",
    "                     label=f'{model_name} (Train)')\n",
    "        axes[0].plot(epochs, history.history[f'val_{metric}'], \n",
    "                     color=colors[idx], linestyle='--', linewidth=2,\n",
    "                     label=f'{model_name} (Val)')\n",
    "    \n",
    "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].legend(loc='lower right', fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    \n",
    "    for idx, (model_name, history) in enumerate(histories.items()):\n",
    "        epochs = range(1, len(history.history['loss']) + 1)\n",
    "        \n",
    "        axes[1].plot(epochs, history.history['loss'], \n",
    "                     color=colors[idx], linestyle='-', linewidth=2,\n",
    "                     label=f'{model_name} (Train)')\n",
    "        axes[1].plot(epochs, history.history['val_loss'], \n",
    "                     color=colors[idx], linestyle='--', linewidth=2,\n",
    "                     label=f'{model_name} (Val)')\n",
    "    \n",
    "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].legend(loc='upper right', fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Training curves saved to: training_curves.png\")\n",
    "\n",
    "plot_training_history(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472b2a3",
   "metadata": {},
   "source": [
    "## 14. Individual Model Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cff4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual_model_curves(histories: Dict[str, keras.callbacks.History]):\n",
    "    \"\"\"Plot individual training curves for each model.\"\"\"\n",
    "    n_models = len(histories)\n",
    "    fig, axes = plt.subplots(n_models, 2, figsize=(14, 5 * n_models))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (model_name, history) in enumerate(histories.items()):\n",
    "        epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "        \n",
    "        axes[idx, 0].plot(epochs, history.history['accuracy'], 'b-', linewidth=2, label='Train')\n",
    "        axes[idx, 0].plot(epochs, history.history['val_accuracy'], 'r--', linewidth=2, label='Validation')\n",
    "        axes[idx, 0].set_title(f'{model_name} - Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[idx, 0].set_xlabel('Epoch')\n",
    "        axes[idx, 0].set_ylabel('Accuracy')\n",
    "        axes[idx, 0].legend()\n",
    "        axes[idx, 0].grid(True, alpha=0.3)\n",
    "        axes[idx, 0].set_ylim([0, 1])\n",
    "        \n",
    "        axes[idx, 1].plot(epochs, history.history['loss'], 'b-', linewidth=2, label='Train')\n",
    "        axes[idx, 1].plot(epochs, history.history['val_loss'], 'r--', linewidth=2, label='Validation')\n",
    "        axes[idx, 1].set_title(f'{model_name} - Loss', fontsize=12, fontweight='bold')\n",
    "        axes[idx, 1].set_xlabel('Epoch')\n",
    "        axes[idx, 1].set_ylabel('Loss')\n",
    "        axes[idx, 1].legend()\n",
    "        axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('individual_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Individual curves saved to: individual_training_curves.png\")\n",
    "\n",
    "plot_individual_model_curves(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aeed05",
   "metadata": {},
   "source": [
    "## 15. Model Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(test_results: dict):\n",
    "    \"\"\"Bar chart comparing test accuracy across models.\"\"\"\n",
    "    models = list(test_results.keys())\n",
    "    accuracies = [test_results[m]['accuracy'] * 100 for m in models]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "    bars = ax.bar(models, accuracies, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{acc:.2f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_title('Model Comparison - Test Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Model comparison saved to: model_comparison.png\")\n",
    "\n",
    "plot_model_comparison(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3365",
   "metadata": {},
   "source": [
    "## 16. Save Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODELS_DIR = Path('final_models')\n",
    "FINAL_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    save_path = FINAL_MODELS_DIR / f\"{model_name}_final.keras\"\n",
    "    model.save(save_path)\n",
    "    print(f\"Saved {model_name} to: {save_path}\")\n",
    "\n",
    "class_names_path = FINAL_MODELS_DIR / 'class_names.txt'\n",
    "with open(class_names_path, 'w') as f:\n",
    "    for name in CLASS_NAMES:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"Saved class names to: {class_names_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCheckpoints directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Final models directory: {FINAL_MODELS_DIR}\")\n",
    "print(f\"Visualization files: training_curves.png, individual_training_curves.png, model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbe735",
   "metadata": {},
   "source": [
    "## 16.5 Export Models for Adversarial Attacks\n",
    "\n",
    "Export trained models in TensorFlow SavedModel format with preprocessing metadata for ART/Foolbox compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create attack models directory\n",
    "ATTACK_MODELS_DIR = Path('attack_models')\n",
    "ATTACK_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Preprocessing metadata for each model (required for ART/Foolbox)\n",
    "PREPROCESSING_INFO = {\n",
    "    'VGG19': {\n",
    "        'mode': 'caffe',  # BGR, mean subtraction [103.939, 116.779, 123.68]\n",
    "        'mean': [103.939, 116.779, 123.68],\n",
    "        'std': [1.0, 1.0, 1.0],\n",
    "        'input_range': [0, 255],\n",
    "        'channel_order': 'BGR'\n",
    "    },\n",
    "    'ResNet50': {\n",
    "        'mode': 'caffe',  # BGR, mean subtraction\n",
    "        'mean': [103.939, 116.779, 123.68],\n",
    "        'std': [1.0, 1.0, 1.0],\n",
    "        'input_range': [0, 255],\n",
    "        'channel_order': 'BGR'\n",
    "    },\n",
    "    'DenseNet121': {\n",
    "        'mode': 'torch',  # RGB, normalize to [0,1] then subtract mean/std\n",
    "        'mean': [0.485, 0.456, 0.406],\n",
    "        'std': [0.229, 0.224, 0.225],\n",
    "        'input_range': [0, 1],\n",
    "        'channel_order': 'RGB'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPORTING MODELS FOR ADVERSARIAL ATTACKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    model_dir = ATTACK_MODELS_DIR / model_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Save as TensorFlow SavedModel format (best for ART/Foolbox)\n",
    "    savedmodel_path = model_dir / 'saved_model'\n",
    "    model.export(str(savedmodel_path))\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  SavedModel: {savedmodel_path}\")\n",
    "    \n",
    "    # 2. Also save as .keras for easy loading\n",
    "    keras_path = model_dir / f'{model_name}_attack.keras'\n",
    "    model.save(keras_path)\n",
    "    print(f\"  Keras format: {keras_path}\")\n",
    "    \n",
    "    # 3. Save preprocessing metadata\n",
    "    metadata = {\n",
    "        'model_name': model_name,\n",
    "        'input_shape': [224, 224, 3],\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'class_names_file': '../final_models/class_names.txt',\n",
    "        'preprocessing': PREPROCESSING_INFO[model_name],\n",
    "        'training_info': {\n",
    "            'dataset': 'Caltech-101',\n",
    "            'total_images': TOTAL_IMAGES,\n",
    "            'fine_tuned_layers': FINE_TUNE_LAYERS,\n",
    "            'batch_size': CONFIG['batch_size'],\n",
    "            'epochs_trained': len(training_histories[model_name].history['loss']),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = model_dir / 'preprocessing.json'\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"  Metadata: {metadata_path}\")\n",
    "\n",
    "# Save training configuration for reproducibility\n",
    "config_path = ATTACK_MODELS_DIR / 'training_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'config': CONFIG,\n",
    "        'fine_tune_layers': FINE_TUNE_LAYERS,\n",
    "        'min_epochs_before_early_stop': MIN_EPOCHS_BEFORE_EARLY_STOP,\n",
    "        'models': list(trained_models.keys()),\n",
    "        'num_classes': NUM_CLASSES,\n",
    "    }, f, indent=2)\n",
    "print(f\"\\nTraining config: {config_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORT COMPLETE - Models ready for adversarial attacks\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAttack models directory: {ATTACK_MODELS_DIR}\")\n",
    "print(\"\\nTo use with ART (Adversarial Robustness Toolbox):\")\n",
    "print(\"  from art.estimators.classification import TensorFlowV2Classifier\")\n",
    "print(\"  import tensorflow as tf\")\n",
    "print(\"  model = tf.saved_model.load('attack_models/VGG19/saved_model')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e87ff",
   "metadata": {},
   "source": [
    "## 17. Individual Model Predictions\n",
    "\n",
    "Use these cells to make predictions with each model separately. You can provide an image path or use a random test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for single image prediction\n",
    "def predict_single_image(model, model_name: str, image_path: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Make a prediction on a single image using a specific model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Keras model\n",
    "        model_name: Name of the model (for preprocessing selection)\n",
    "        image_path: Path to the image file\n",
    "        top_k: Number of top predictions to show\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, CONFIG['img_size'])\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    \n",
    "    # Apply model-specific preprocessing\n",
    "    preprocess_fn = PREPROCESS_FUNCTIONS[model_name]\n",
    "    img = preprocess_fn(img)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(img, verbose=0)[0]\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
    "    \n",
    "    results = {\n",
    "        'image_path': image_path,\n",
    "        'model': model_name,\n",
    "        'predictions': []\n",
    "    }\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        results['predictions'].append({\n",
    "            'class': CLASS_NAMES[idx],\n",
    "            'confidence': float(predictions[idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_prediction(results: dict, show_image: bool = True):\n",
    "    \"\"\"Display prediction results with optional image.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {results['model']}\")\n",
    "    print(f\"Image: {results['image_path']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if show_image:\n",
    "        img = plt.imread(results['image_path'])\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Predicted: {results['predictions'][0]['class']}\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\nTop {len(results['predictions'])} Predictions:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, pred in enumerate(results['predictions'], 1):\n",
    "        confidence = pred['confidence'] * 100\n",
    "        bar = '' * int(confidence / 5) + '' * (20 - int(confidence / 5))\n",
    "        print(f\"{i}. {pred['class']:<25} {bar} {confidence:.2f}%\")\n",
    "\n",
    "print(\"Prediction helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca023a3",
   "metadata": {},
   "source": [
    "## 16.1 Predict with VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14495d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICT WITH VGG19\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Use a random test image\n",
    "random_idx = np.random.randint(0, len(test_paths))\n",
    "sample_image_path = test_paths[random_idx]\n",
    "true_label = CLASS_NAMES[test_labels[random_idx]]\n",
    "\n",
    "# Option 2: Specify your own image path (uncomment and modify)\n",
    "# sample_image_path = \"path/to/your/image.jpg\"\n",
    "\n",
    "# Load model (from memory or checkpoint)\n",
    "if 'VGG19' in trained_models:\n",
    "    vgg19_model = trained_models['VGG19']\n",
    "else:\n",
    "    vgg19_checkpoint = CHECKPOINT_DIR / \"VGG19_best.keras\"\n",
    "    vgg19_model = keras.models.load_model(vgg19_checkpoint)\n",
    "    print(f\"Loaded VGG19 from: {vgg19_checkpoint}\")\n",
    "\n",
    "# Make prediction\n",
    "vgg19_results = predict_single_image(vgg19_model, 'VGG19', sample_image_path, top_k=5)\n",
    "display_prediction(vgg19_results)\n",
    "\n",
    "print(f\"\\nTrue label: {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae5ee0",
   "metadata": {},
   "source": [
    "## 16.2 Predict with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4890162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICT WITH ResNet50\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Use a random test image\n",
    "# random_idx = np.random.randint(0, len(test_paths))\n",
    "# sample_image_path = test_paths[random_idx]\n",
    "# true_label = CLASS_NAMES[test_labels[random_idx]]\n",
    "\n",
    "# Option 2: Specify your own image path (uncomment and modify)\n",
    "sample_image_path = \"caltech101_data/caltech-101/101_ObjectCategories/lotus/image_0006.jpg\"\n",
    "\n",
    "# Load model (from memory or checkpoint)\n",
    "if 'ResNet50' in trained_models:\n",
    "    resnet_model = trained_models['ResNet50']\n",
    "else:\n",
    "    resnet_checkpoint = CHECKPOINT_DIR / \"ResNet50_best.keras\"\n",
    "    resnet_model = keras.models.load_model(resnet_checkpoint)\n",
    "    print(f\"Loaded ResNet50 from: {resnet_checkpoint}\")\n",
    "\n",
    "# Make prediction\n",
    "resnet_results = predict_single_image(resnet_model, 'ResNet50', sample_image_path, top_k=5)\n",
    "display_prediction(resnet_results)\n",
    "\n",
    "print(f\"\\nTrue label: {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed55ba",
   "metadata": {},
   "source": [
    "## 16.3 Predict with DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11839215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICT WITH DenseNet121\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Use a random test image\n",
    "# random_idx = np.random.randint(0, len(test_paths))\n",
    "# sample_image_path = test_paths[random_idx]\n",
    "# true_label = CLASS_NAMES[test_labels[random_idx]]\n",
    "\n",
    "# Option 2: Specify your own image path (uncomment and modify)\n",
    "sample_image_path = \"caltech101_data/caltech-101/101_ObjectCategories/lotus/image_0006.jpg\"\n",
    "\n",
    "# Load model (from memory or checkpoint)\n",
    "if 'DenseNet121' in trained_models:\n",
    "    densenet_model = trained_models['DenseNet121']\n",
    "else:\n",
    "    densenet_checkpoint = CHECKPOINT_DIR / \"DenseNet121_best.keras\"\n",
    "    densenet_model = keras.models.load_model(densenet_checkpoint)\n",
    "    print(f\"Loaded DenseNet121 from: {densenet_checkpoint}\")\n",
    "\n",
    "# Make prediction\n",
    "densenet_results = predict_single_image(densenet_model, 'DenseNet121', sample_image_path, top_k=5)\n",
    "display_prediction(densenet_results)\n",
    "\n",
    "print(f\"\\nTrue label: {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c35073",
   "metadata": {},
   "source": [
    "## 16.4 Compare All Models on Same Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE ALL MODELS ON THE SAME IMAGE\n",
    "# ============================================\n",
    "\n",
    "# Pick a random test image\n",
    "random_idx = np.random.randint(0, len(test_paths))\n",
    "comparison_image_path = test_paths[random_idx]\n",
    "true_label = CLASS_NAMES[test_labels[random_idx]]\n",
    "\n",
    "# comparison_image_path = \"caltech101_data/caltech-101/101_ObjectCategories/lotus/image_0006.jpg\"\n",
    "\n",
    "print(f\"Comparing all models on: {comparison_image_path}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "\n",
    "# Show the image\n",
    "img = plt.imread(comparison_image_path)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"True: {true_label}\")\n",
    "plt.show()\n",
    "\n",
    "# Predict with each model\n",
    "all_predictions = {}\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "    else:\n",
    "        checkpoint = CHECKPOINT_DIR / f\"{model_name}_best.keras\"\n",
    "        model = keras.models.load_model(checkpoint)\n",
    "    \n",
    "    results = predict_single_image(model, model_name, comparison_image_path, top_k=3)\n",
    "    all_predictions[model_name] = results['predictions']\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Model':<15} {'Top-1 Prediction':<25} {'Confidence':<12} {'Correct?'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name, preds in all_predictions.items():\n",
    "    top_pred = preds[0]['class']\n",
    "    confidence = preds[0]['confidence'] * 100\n",
    "    is_correct = \"\" if top_pred == true_label else \"\"\n",
    "    print(f\"{model_name:<15} {top_pred:<25} {confidence:>8.2f}%    {is_correct}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
