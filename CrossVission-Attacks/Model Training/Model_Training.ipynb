{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac67ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.10.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (2.10.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (25.12.19)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (3.15.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (26.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (80.10.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (1.78.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.10.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.48.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.32.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.1.5)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.46.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (46.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2026.1.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from cryptography>=38.0.3->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.3.1)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow 2.10.1 (last version with native Windows GPU support) and dependencies\n",
    "%pip install \"tensorflow==2.10.1\" \"numpy>=1.22,<1.24\" matplotlib scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf86a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from: c:\\Users\\ishaa\\Desktop\\Reasearch_paper\\project\\venv\\Scripts\\python.exe\n",
      "CUDA_PATH set to: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\n",
      "Requirement already satisfied: tensorflow==2.10.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (25.12.19)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (3.15.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (26.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (80.10.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (1.78.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorflow==2.10.1) (2.10.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.48.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.32.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.1.5)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.46.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (46.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2026.1.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from cryptography>=38.0.3->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.3.1)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\ishaa\\desktop\\reasearch_paper\\project\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Print where the notebook is actually running from\n",
    "print(f\"Running from: {sys.executable}\")\n",
    "\n",
    "# 2. Set CUDA_PATH to 11.8 (required for TF 2.10 GPU support on Windows)\n",
    "os.environ['CUDA_PATH'] = r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8'\n",
    "cuda_bin = r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin'\n",
    "cuda_libnvvp = r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\libnvvp'\n",
    "if cuda_bin not in os.environ.get('PATH', ''):\n",
    "    os.environ['PATH'] = cuda_bin + ';' + cuda_libnvvp + ';' + os.environ['PATH']\n",
    "print(f\"CUDA_PATH set to: {os.environ['CUDA_PATH']}\")\n",
    "\n",
    "# 3. Force install into THIS specific python\n",
    "!{sys.executable} -m pip install \"tensorflow==2.10.1\" \"numpy>=1.22,<1.24\" matplotlib scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc229af",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c6b847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.10.1\n",
      "TF file: c:\\Users\\ishaa\\Desktop\\Reasearch_paper\\project\\venv\\lib\\site-packages\\tensorflow\\__init__.py\n",
      "TF keras: <module 'tensorflow.keras' from 'c:\\\\Users\\\\ishaa\\\\Desktop\\\\Reasearch_paper\\\\project\\\\venv\\\\lib\\\\site-packages\\\\keras\\\\api\\\\_v2\\\\keras\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"TF file:\", tf.__file__)\n",
    "print(\"TF keras:\", tf.keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef795027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "Keras version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Callable\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, DenseNet121\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Apply a global dark plotting theme so all figures and saved images have a black background\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.facecolor'] = 'black'\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "plt.rcParams['savefig.facecolor'] = 'black'\n",
    "plt.rcParams['text.color'] = 'white'\n",
    "plt.rcParams['axes.labelcolor'] = 'white'\n",
    "plt.rcParams['xtick.color'] = 'white'\n",
    "plt.rcParams['ytick.color'] = 'white'\n",
    "plt.rcParams['legend.facecolor'] = 'black'\n",
    "plt.rcParams['legend.edgecolor'] = 'white'\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.version.VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48bbd0f",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration & Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "147e8a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPU(s): ['/physical_device:GPU:0']\n",
      "Memory growth enabled for all GPUs\n",
      "Mixed precision policy: mixed_float16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure CUDA 11.8 is being used (required for TF 2.10 on Windows)\n",
    "os.environ['CUDA_PATH'] = r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8'\n",
    "cuda_bin = r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin'\n",
    "cuda_libnvvp = r'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\libnvvp'\n",
    "if cuda_bin not in os.environ.get('PATH', ''):\n",
    "    os.environ['PATH'] = cuda_bin + ';' + cuda_libnvvp + ';' + os.environ['PATH']\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Found {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}\")\n",
    "        print(\"Memory growth enabled for all GPUs\")\n",
    "        \n",
    "        # Enable mixed precision for faster training on GPU (RTX 3060 has compute capability 8.6)\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(f\"Mixed precision policy: {tf.keras.mixed_precision.global_policy().name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU found. Training will be slow on CPU.\")\n",
    "    print(\"Make sure CUDA 11.8 and cuDNN 8 are installed.\")\n",
    "    print(f\"Current CUDA_PATH: {os.environ.get('CUDA_PATH', 'NOT SET')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6b271",
   "metadata": {},
   "source": [
    "## 3. Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08563025",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'zip_file': '../caltech-101.zip',  # Path relative to Model Training folder (one level up)\n",
    "    'extract_dir': 'caltech101_data',\n",
    "    'img_size': (224, 224),\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'train_split': 0.8,\n",
    "    'val_split': 0.1,\n",
    "    'test_split': 0.1,\n",
    "    'seed': 42,\n",
    "    'learning_rate': 1e-4,\n",
    "    'patience_early_stop': 10,\n",
    "    'patience_lr_reduce': 5,\n",
    "}\n",
    "\n",
    "MODELS_TO_TRAIN = ['VGG19', 'ResNet50', 'DenseNet121']\n",
    "CHECKPOINT_DIR = Path('checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "np.random.seed(CONFIG['seed'])\n",
    "tf.random.set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a13cc3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already extracted at: caltech101_data\\caltech-101\\101_ObjectCategories\n",
      "Found 101_ObjectCategories at: caltech101_data\\caltech-101\\101_ObjectCategories (102 class folders)\n",
      "Image root directory: caltech101_data\\caltech-101\\101_ObjectCategories\n",
      "\n",
      "Found 102 class folders\n",
      "First 10: ['BACKGROUND_Google', 'Faces', 'Faces_easy', 'Leopards', 'Motorbikes', 'accordion', 'airplanes', 'anchor', 'ant', 'barrel']\n"
     ]
    }
   ],
   "source": [
    "def extract_zip_if_needed(zip_path: str, extract_dir: str) -> Path:\n",
    "    \"\"\"Extract zip file if not already extracted, handling nested tar.gz archives.\"\"\"\n",
    "    extract_path = Path(extract_dir)\n",
    "    \n",
    "    # Check if already fully extracted (has 101_ObjectCategories with class folders)\n",
    "    for obj_cat in extract_path.rglob('101_ObjectCategories'):\n",
    "        if obj_cat.is_dir():\n",
    "            subdirs = [d for d in obj_cat.iterdir() if d.is_dir() and d.name != '__MACOSX']\n",
    "            if len(subdirs) > 50:\n",
    "                print(f\"Data already extracted at: {obj_cat}\")\n",
    "                return extract_path\n",
    "    \n",
    "    # Step 1: Extract the outer zip if needed\n",
    "    if not extract_path.exists() or not any(extract_path.iterdir()):\n",
    "        if not Path(zip_path).exists():\n",
    "            raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n",
    "        \n",
    "        print(f\"Extracting {zip_path}...\")\n",
    "        extract_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=extract_path)\n",
    "        \n",
    "        print(f\"Zip extraction complete: {extract_path}\")\n",
    "    \n",
    "    # Step 2: Look for and extract any nested .tar.gz files (skip __MACOSX)\n",
    "    for tar_gz in extract_path.rglob('*.tar.gz'):\n",
    "        # Skip macOS metadata files\n",
    "        if '__MACOSX' in str(tar_gz):\n",
    "            continue\n",
    "        print(f\"Found nested archive: {tar_gz}\")\n",
    "        print(f\"Extracting {tar_gz.name} (this may take a moment)...\")\n",
    "        with tarfile.open(tar_gz, 'r:gz') as tar:\n",
    "            tar.extractall(path=tar_gz.parent)\n",
    "        print(f\"  Extracted to: {tar_gz.parent}\")\n",
    "    \n",
    "    print(f\"Full extraction complete: {extract_path}\")\n",
    "    return extract_path\n",
    "\n",
    "data_root = extract_zip_if_needed(CONFIG['zip_file'], CONFIG['extract_dir'])\n",
    "\n",
    "def find_image_root(base_path: Path) -> Path:\n",
    "    \"\"\"Find the actual root containing class folders with images.\"\"\"\n",
    "    # Recursive search for 101_ObjectCategories anywhere in the tree\n",
    "    for obj_cat in base_path.rglob('101_ObjectCategories'):\n",
    "        if obj_cat.is_dir():\n",
    "            subdirs = [d for d in obj_cat.iterdir() if d.is_dir() and d.name != '__MACOSX']\n",
    "            if len(subdirs) > 10:\n",
    "                print(f\"Found 101_ObjectCategories at: {obj_cat} ({len(subdirs)} class folders)\")\n",
    "                return obj_cat\n",
    "    \n",
    "    # Fallback: find directory with many subdirectories (class folders)\n",
    "    for subdir in base_path.rglob('*'):\n",
    "        if subdir.is_dir() and subdir.name != '__MACOSX':\n",
    "            subdirs = [s for s in subdir.iterdir() if s.is_dir() and s.name != '__MACOSX']\n",
    "            if len(subdirs) > 50:\n",
    "                print(f\"Found root with {len(subdirs)} class folders: {subdir}\")\n",
    "                return subdir\n",
    "    \n",
    "    # Fallback: search for image files and go up to find class root\n",
    "    for item in base_path.rglob('*.jpg'):\n",
    "        if item.is_file():\n",
    "            class_folder = item.parent\n",
    "            root = class_folder.parent\n",
    "            if root.is_dir() and len(list(root.iterdir())) > 10:\n",
    "                print(f\"Found image root via image search: {root}\")\n",
    "                return root\n",
    "    \n",
    "    print(f\"WARNING: Could not find image root, using base path: {base_path}\")\n",
    "    return base_path\n",
    "\n",
    "IMAGE_ROOT = find_image_root(data_root)\n",
    "print(f\"Image root directory: {IMAGE_ROOT}\")\n",
    "\n",
    "# List some class folders to verify\n",
    "class_dirs = sorted([d.name for d in IMAGE_ROOT.iterdir() if d.is_dir() and d.name != '__MACOSX'])\n",
    "print(f\"\\nFound {len(class_dirs)} class folders\")\n",
    "print(f\"First 10: {class_dirs[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf2fb7",
   "metadata": {},
   "source": [
    "## 5. Dataset Discovery & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65b51a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 1\n",
      "Total images: 0\n",
      "\n",
      "Sample classes: ['caltech-101']...\n",
      "\n",
      "Class distribution (first 10):\n",
      "  caltech-101: 0 images\n"
     ]
    }
   ],
   "source": [
    "def get_class_names_and_counts(image_root: Path) -> Tuple[list, dict]:\n",
    "    \"\"\"Get class names and image counts per class.\"\"\"\n",
    "    # Filter out system/hidden folders\n",
    "    exclude_dirs = {'__MACOSX', '.DS_Store', 'BACKGROUND_Google', '__pycache__'}\n",
    "    \n",
    "    class_names = sorted([\n",
    "        d.name for d in image_root.iterdir() \n",
    "        if d.is_dir() and d.name not in exclude_dirs and not d.name.startswith('.')\n",
    "    ])\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = image_root / class_name\n",
    "        count = len([f for f in class_dir.iterdir() \n",
    "                     if f.is_file() and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']])\n",
    "        class_counts[class_name] = count\n",
    "    \n",
    "    return class_names, class_counts\n",
    "\n",
    "CLASS_NAMES, CLASS_COUNTS = get_class_names_and_counts(IMAGE_ROOT)\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "TOTAL_IMAGES = sum(CLASS_COUNTS.values())\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Total images: {TOTAL_IMAGES}\")\n",
    "print(f\"\\nSample classes: {CLASS_NAMES[:10]}...\")\n",
    "print(f\"\\nClass distribution (first 10):\")\n",
    "for cls in list(CLASS_COUNTS.keys())[:10]:\n",
    "    print(f\"  {cls}: {CLASS_COUNTS[cls]} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973d1b5",
   "metadata": {},
   "source": [
    "## 6. Create tf.data.Dataset with Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d588826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 0\n",
      "Validation samples: 0\n",
      "Test samples: 0\n"
     ]
    }
   ],
   "source": [
    "def collect_all_image_paths_and_labels(image_root: Path, class_names: list) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Collect all image paths and their corresponding labels.\"\"\"\n",
    "    all_paths = []\n",
    "    all_labels = []\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = image_root / class_name\n",
    "        for img_path in class_dir.iterdir():\n",
    "            if img_path.is_file() and img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:\n",
    "                all_paths.append(str(img_path))\n",
    "                all_labels.append(class_to_idx[class_name])\n",
    "    \n",
    "    return np.array(all_paths), np.array(all_labels)\n",
    "\n",
    "all_paths, all_labels = collect_all_image_paths_and_labels(IMAGE_ROOT, CLASS_NAMES)\n",
    "\n",
    "indices = np.arange(len(all_paths))\n",
    "np.random.shuffle(indices)\n",
    "all_paths = all_paths[indices]\n",
    "all_labels = all_labels[indices]\n",
    "\n",
    "train_size = int(CONFIG['train_split'] * len(all_paths))\n",
    "val_size = int(CONFIG['val_split'] * len(all_paths))\n",
    "\n",
    "train_paths, train_labels = all_paths[:train_size], all_labels[:train_size]\n",
    "val_paths, val_labels = all_paths[train_size:train_size+val_size], all_labels[train_size:train_size+val_size]\n",
    "test_paths, test_labels = all_paths[train_size+val_size:], all_labels[train_size+val_size:]\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)}\")\n",
    "print(f\"Validation samples: {len(val_paths)}\")\n",
    "print(f\"Test samples: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7745c",
   "metadata": {},
   "source": [
    "## 7. Dataset Pipeline Factory (with Model-Specific Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28bbf96",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m PREPROCESS_FUNCTIONS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVGG19\u001b[39m\u001b[38;5;124m'\u001b[39m: vgg_preprocess,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResNet50\u001b[39m\u001b[38;5;124m'\u001b[39m: resnet_preprocess,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDenseNet121\u001b[39m\u001b[38;5;124m'\u001b[39m: densenet_preprocess,\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Compute class weights to handle imbalanced classes\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m CLASS_WEIGHTS \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_labels\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m CLASS_WEIGHT_DICT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(CLASS_WEIGHTS))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass weights computed | Range: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCLASS_WEIGHTS\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCLASS_WEIGHTS\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ishaa\\Desktop\\Reasearch_paper\\project\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ishaa\\Desktop\\Reasearch_paper\\project\\venv\\lib\\site-packages\\sklearn\\utils\\class_weight.py:84\u001b[0m, in \u001b[0;36mcompute_class_weight\u001b[1;34m(class_weight, classes, y, sample_weight)\u001b[0m\n\u001b[0;32m     80\u001b[0m     weighted_class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_ind, weights\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m     81\u001b[0m     recip_freq \u001b[38;5;241m=\u001b[39m weighted_class_counts\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m (\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28mlen\u001b[39m(le\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m*\u001b[39m weighted_class_counts\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[43mrecip_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# user-defined dictionary\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(classes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Ensure compute_class_weight is available (install scikit-learn if needed)\n",
    "try:\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    print(\"scikit-learn not found â€” installing scikit-learn...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "PREPROCESS_FUNCTIONS = {\n",
    "    'VGG19': vgg_preprocess,\n",
    "    'ResNet50': resnet_preprocess,\n",
    "    'DenseNet121': densenet_preprocess,\n",
    "}\n",
    "\n",
    "# Compute class weights to handle imbalanced classes\n",
    "CLASS_WEIGHTS = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "CLASS_WEIGHT_DICT = dict(enumerate(CLASS_WEIGHTS))\n",
    "print(f\"Class weights computed | Range: [{CLASS_WEIGHTS.min():.3f}, {CLASS_WEIGHTS.max():.3f}]\")\n",
    "\n",
    "# Cache for datasets to avoid recreating them multiple times\n",
    "_dataset_cache = {}\n",
    "\n",
    "def create_dataset_pipeline(\n",
    "    paths: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    preprocess_fn: Callable,\n",
    "    img_size: Tuple[int, int],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    augment: bool = False,\n",
    "    cache: bool = False\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Create an optimized tf.data pipeline with model-specific preprocessing.\"\"\"\n",
    "    \n",
    "    def load_and_preprocess(path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = preprocess_fn(img)\n",
    "        return img, label\n",
    "    \n",
    "    def augment_image(img, label):\n",
    "        \"\"\"Enhanced augmentation: flips, brightness, contrast, saturation, random crop.\"\"\"\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.random_brightness(img, 0.2)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        img = tf.image.random_saturation(img, 0.8, 1.2)\n",
    "        # Random crop (simulates zoom) then resize back\n",
    "        crop_frac = tf.random.uniform([], 0.85, 1.0)\n",
    "        h, w = img_size\n",
    "        new_h = tf.cast(tf.cast(h, tf.float32) * crop_frac, tf.int32)\n",
    "        new_w = tf.cast(tf.cast(w, tf.float32) * crop_frac, tf.int32)\n",
    "        img = tf.image.random_crop(img, [new_h, new_w, 3])\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        return img, label\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=min(len(paths), 10000), seed=CONFIG['seed'], reshuffle_each_iteration=True)\n",
    "    \n",
    "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_datasets_for_model(model_name: str, use_cache: bool = True) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"Create train, val, test datasets with model-specific preprocessing (cached).\"\"\"\n",
    "    cache_key = model_name\n",
    "    \n",
    "    if use_cache and cache_key in _dataset_cache:\n",
    "        return _dataset_cache[cache_key]\n",
    "    \n",
    "    preprocess_fn = PREPROCESS_FUNCTIONS[model_name]\n",
    "    \n",
    "    train_ds = create_dataset_pipeline(\n",
    "        train_paths, train_labels, preprocess_fn,\n",
    "        CONFIG['img_size'], CONFIG['batch_size'],\n",
    "        shuffle=True, augment=True, cache=False  # Don't cache augmented data\n",
    "    )\n",
    "    val_ds = create_dataset_pipeline(\n",
    "        val_paths, val_labels, preprocess_fn,\n",
    "        CONFIG['img_size'], CONFIG['batch_size'],\n",
    "        shuffle=False, augment=False, cache=True  # Cache validation data\n",
    "    )\n",
    "    test_ds = create_dataset_pipeline(\n",
    "        test_paths, test_labels, preprocess_fn,\n",
    "        CONFIG['img_size'], CONFIG['batch_size'],\n",
    "        shuffle=False, augment=False, cache=True  # Cache test data\n",
    "    )\n",
    "    \n",
    "    if use_cache:\n",
    "        _dataset_cache[cache_key] = (train_ds, val_ds, test_ds)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def clear_dataset_cache():\n",
    "    \"\"\"Clear the dataset cache to free memory.\"\"\"\n",
    "    global _dataset_cache\n",
    "    _dataset_cache = {}\n",
    "    print(\"Dataset cache cleared.\")\n",
    "\n",
    "print(\"Dataset pipeline factory created with enhanced augmentation and caching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707735a2",
   "metadata": {},
   "source": [
    "## 8. Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model factory created.\n",
      "Models available: ['VGG19', 'ResNet101', 'DenseNet121']\n"
     ]
    }
   ],
   "source": [
    "BASE_MODELS = {\n",
    "    'VGG19': VGG19,\n",
    "    'ResNet50': ResNet50,\n",
    "    'DenseNet121': DenseNet121,\n",
    "}\n",
    "\n",
    "# Number of layers to unfreeze for fine-tuning\n",
    "FINE_TUNE_LAYERS = 20\n",
    "\n",
    "def create_model(model_name: str, num_classes: int, img_size: Tuple[int, int], fine_tune: bool = True) -> Model:\n",
    "    \"\"\"Create a transfer learning model with fine-tuning of last 20 layers.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the base model ('VGG19', 'ResNet50', 'DenseNet121')\n",
    "        num_classes: Number of output classes\n",
    "        img_size: Input image size (height, width)\n",
    "        fine_tune: If True, unfreeze last FINE_TUNE_LAYERS layers for training\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model with fine-tuning enabled\n",
    "    \"\"\"\n",
    "    \n",
    "    base_model_class = BASE_MODELS[model_name]\n",
    "    \n",
    "    base_model = base_model_class(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(*img_size, 3),\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze all layers first\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Unfreeze last FINE_TUNE_LAYERS layers for genuine training on Caltech-101\n",
    "    if fine_tune:\n",
    "        for layer in base_model.layers[-FINE_TUNE_LAYERS:]:\n",
    "            layer.trainable = True\n",
    "        print(f\"  Fine-tuning enabled: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "    \n",
    "    inputs = keras.Input(shape=(*img_size, 3))\n",
    "    x = base_model(inputs, training=fine_tune)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=f\"{model_name}_transfer\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Model factory created with fine-tuning support.\")\n",
    "print(f\"Models available: {list(BASE_MODELS.keys())}\")\n",
    "print(f\"Fine-tune layers: Last {FINE_TUNE_LAYERS} layers will be unfrozen for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3164611",
   "metadata": {},
   "source": [
    "## 9. Callbacks Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks factory created.\n"
     ]
    }
   ],
   "source": [
    "# Minimum epochs before early stopping can trigger\n",
    "MIN_EPOCHS_BEFORE_EARLY_STOP = 20\n",
    "\n",
    "class MinEpochEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Early stopping that only activates after a minimum number of epochs.\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor='val_loss', patience=10, min_epochs=20, restore_best_weights=True, verbose=1):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.min_epochs = min_epochs\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "        self.best_weights = None\n",
    "        self.best_value = None\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        if self.best_value is None or current < self.best_value:\n",
    "            self.best_value = current\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            # Only count patience after min_epochs\n",
    "            if epoch >= self.min_epochs:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    self.stopped_epoch = epoch\n",
    "                    self.model.stop_training = True\n",
    "                    if self.restore_best_weights and self.best_weights is not None:\n",
    "                        if self.verbose:\n",
    "                            print(f\"\\nRestoring model weights from the end of the best epoch.\")\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose:\n",
    "            print(f\"Epoch {self.stopped_epoch + 1}: early stopping (after min {self.min_epochs} epochs)\")\n",
    "\n",
    "def get_callbacks(model_name: str) -> list:\n",
    "    \"\"\"Create callbacks for training with minimum epoch threshold.\n",
    "    \n",
    "    Early stopping won't trigger until at least MIN_EPOCHS_BEFORE_EARLY_STOP\n",
    "    epochs have completed, ensuring sufficient training on Caltech-101.\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.h5\"\n",
    "    \n",
    "    callbacks = [\n",
    "        MinEpochEarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=CONFIG['patience_early_stop'],\n",
    "            min_epochs=MIN_EPOCHS_BEFORE_EARLY_STOP,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(checkpoint_path),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=CONFIG['patience_lr_reduce'],\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "print(\"Callbacks factory created.\")\n",
    "print(f\"Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa03106",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training storage initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store results\n",
    "training_histories: Dict[str, keras.callbacks.History] = {}\n",
    "trained_models: Dict[str, Model] = {}\n",
    "\n",
    "print(\"Training storage initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae593887",
   "metadata": {},
   "source": [
    "## 10.1 Train VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9975da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training VGG19\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770072071.045398  118378 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3398 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No checkpoint found. Creating new VGG19 model...\n",
      "\n",
      "VGG19 Summary:\n",
      "  Total params: 20,446,630\n",
      "  Trainable params: 421,222\n",
      "\n",
      "--- Phase 1: Training classification head ---\n",
      "Early Stopping: patience=10, monitor='val_loss'\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:11:13.318048: I external/local_xla/xla/service/service.cc:163] XLA service 0x716290010d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2026-02-03 04:11:13.318073: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2026-02-03 04:11:13.551643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2026-02-03 04:11:13.876280: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91801\n",
      "2026-02-03 04:11:13.954477: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954503: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954548: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954559: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954565: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:14.523011: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:14.957133: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1886', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.014894: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.246000: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1778', 332 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.726707: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1834', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:19.056332: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:11:20.040428: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2026-02-03 04:11:22.024797: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:11:24.686833: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "I0000 00:00:1770072090.042138  118908 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.0175 - loss: 5.0671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:12:06.974217: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:06.974246: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:07.140540: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.698903: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1759', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.792575: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.802529: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 364 bytes spill stores, 364 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:09.977328: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.0176 - loss: 5.0662"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:12:18.807080: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:19.289817: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:19.528417: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 324 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:25.016513: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:25.489088: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:25.760911: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 372 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:27.578850: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.16521, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 1: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 283ms/step - accuracy: 0.0271 - loss: 4.8654 - val_accuracy: 0.1652 - val_loss: 3.9945 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.0751 - loss: 4.2922\n",
      "Epoch 2: val_accuracy improved from 0.16521 to 0.44748, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 2: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 188ms/step - accuracy: 0.1012 - loss: 4.2041 - val_accuracy: 0.4475 - val_loss: 3.2475 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.1892 - loss: 3.7958\n",
      "Epoch 3: val_accuracy improved from 0.44748 to 0.59081, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 3: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.2224 - loss: 3.6200 - val_accuracy: 0.5908 - val_loss: 2.3353 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.3180 - loss: 3.1739\n",
      "Epoch 4: val_accuracy improved from 0.59081 to 0.68600, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 4: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.3357 - loss: 3.0777 - val_accuracy: 0.6860 - val_loss: 1.6646 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4115 - loss: 2.6430\n",
      "Epoch 5: val_accuracy improved from 0.68600 to 0.75492, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 5: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.4171 - loss: 2.6182 - val_accuracy: 0.7549 - val_loss: 1.2716 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4703 - loss: 2.2797\n",
      "Epoch 6: val_accuracy improved from 0.75492 to 0.78665, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 6: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.4726 - loss: 2.2538 - val_accuracy: 0.7867 - val_loss: 1.0130 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.5234 - loss: 2.0230\n",
      "Epoch 7: val_accuracy improved from 0.78665 to 0.80744, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 7: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 185ms/step - accuracy: 0.5277 - loss: 2.0076 - val_accuracy: 0.8074 - val_loss: 0.8755 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5479 - loss: 1.8607\n",
      "Epoch 8: val_accuracy improved from 0.80744 to 0.82166, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 8: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.5545 - loss: 1.8373 - val_accuracy: 0.8217 - val_loss: 0.7833 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5776 - loss: 1.6751\n",
      "Epoch 9: val_accuracy improved from 0.82166 to 0.82823, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 9: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.5844 - loss: 1.6671 - val_accuracy: 0.8282 - val_loss: 0.7129 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6123 - loss: 1.5647\n",
      "Epoch 10: val_accuracy improved from 0.82823 to 0.84573, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.6168 - loss: 1.5359 - val_accuracy: 0.8457 - val_loss: 0.6686 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6205 - loss: 1.4827\n",
      "Epoch 11: val_accuracy did not improve from 0.84573\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.6276 - loss: 1.4301 - val_accuracy: 0.8435 - val_loss: 0.6404 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6562 - loss: 1.3431\n",
      "Epoch 12: val_accuracy did not improve from 0.84573\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.6485 - loss: 1.3616 - val_accuracy: 0.8457 - val_loss: 0.6060 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6676 - loss: 1.2603\n",
      "Epoch 13: val_accuracy improved from 0.84573 to 0.85449, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 13: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6651 - loss: 1.2792 - val_accuracy: 0.8545 - val_loss: 0.5856 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.6646 - loss: 1.2279\n",
      "Epoch 14: val_accuracy did not improve from 0.85449\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6700 - loss: 1.2435 - val_accuracy: 0.8534 - val_loss: 0.5722 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6895 - loss: 1.1857\n",
      "Epoch 15: val_accuracy improved from 0.85449 to 0.85667, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 15: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6850 - loss: 1.1818 - val_accuracy: 0.8567 - val_loss: 0.5624 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6919 - loss: 1.1099\n",
      "Epoch 16: val_accuracy improved from 0.85667 to 0.86214, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 16: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6965 - loss: 1.0878 - val_accuracy: 0.8621 - val_loss: 0.5351 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7153 - loss: 1.0528\n",
      "Epoch 17: val_accuracy improved from 0.86214 to 0.86433, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 17: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.7106 - loss: 1.0730 - val_accuracy: 0.8643 - val_loss: 0.5359 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7109 - loss: 1.0560\n",
      "Epoch 18: val_accuracy improved from 0.86433 to 0.86980, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 18: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7169 - loss: 1.0324 - val_accuracy: 0.8698 - val_loss: 0.5131 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7331 - loss: 0.9986\n",
      "Epoch 19: val_accuracy did not improve from 0.86980\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.7347 - loss: 0.9769 - val_accuracy: 0.8698 - val_loss: 0.5035 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7432 - loss: 0.9378\n",
      "Epoch 20: val_accuracy improved from 0.86980 to 0.87418, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 20: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7373 - loss: 0.9712 - val_accuracy: 0.8742 - val_loss: 0.5003 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7494 - loss: 0.9141\n",
      "Epoch 21: val_accuracy improved from 0.87418 to 0.87637, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 21: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.7486 - loss: 0.9227 - val_accuracy: 0.8764 - val_loss: 0.4935 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7483 - loss: 0.8808\n",
      "Epoch 22: val_accuracy improved from 0.87637 to 0.87965, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 22: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7430 - loss: 0.9231 - val_accuracy: 0.8796 - val_loss: 0.4855 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7553 - loss: 0.8726\n",
      "Epoch 23: val_accuracy did not improve from 0.87965\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7565 - loss: 0.8482 - val_accuracy: 0.8796 - val_loss: 0.4738 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7712 - loss: 0.8117\n",
      "Epoch 24: val_accuracy improved from 0.87965 to 0.88184, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 24: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7646 - loss: 0.8451 - val_accuracy: 0.8818 - val_loss: 0.4776 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7735 - loss: 0.7874\n",
      "Epoch 25: val_accuracy improved from 0.88184 to 0.88512, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 25: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7729 - loss: 0.7879 - val_accuracy: 0.8851 - val_loss: 0.4656 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7653 - loss: 0.8088\n",
      "Epoch 26: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.7702 - loss: 0.7968 - val_accuracy: 0.8840 - val_loss: 0.4653 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7780 - loss: 0.7625\n",
      "Epoch 27: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7776 - loss: 0.7566 - val_accuracy: 0.8807 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7867 - loss: 0.7270\n",
      "Epoch 28: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7813 - loss: 0.7542 - val_accuracy: 0.8840 - val_loss: 0.4552 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7947 - loss: 0.7123\n",
      "Epoch 29: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7975 - loss: 0.7098 - val_accuracy: 0.8829 - val_loss: 0.4563 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7939 - loss: 0.7052\n",
      "Epoch 30: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7904 - loss: 0.7121 - val_accuracy: 0.8829 - val_loss: 0.4474 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7973 - loss: 0.6798\n",
      "Epoch 31: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7917 - loss: 0.6916 - val_accuracy: 0.8807 - val_loss: 0.4428 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8131 - loss: 0.6363\n",
      "Epoch 32: val_accuracy improved from 0.88512 to 0.88621, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 32: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8066 - loss: 0.6662 - val_accuracy: 0.8862 - val_loss: 0.4538 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8031 - loss: 0.6399\n",
      "Epoch 33: val_accuracy did not improve from 0.88621\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8014 - loss: 0.6668 - val_accuracy: 0.8818 - val_loss: 0.4389 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8039 - loss: 0.6542\n",
      "Epoch 34: val_accuracy improved from 0.88621 to 0.88950, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 34: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8040 - loss: 0.6449 - val_accuracy: 0.8895 - val_loss: 0.4341 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8153 - loss: 0.6051\n",
      "Epoch 35: val_accuracy improved from 0.88950 to 0.89059, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 35: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8116 - loss: 0.6207 - val_accuracy: 0.8906 - val_loss: 0.4390 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8078 - loss: 0.6354\n",
      "Epoch 36: val_accuracy improved from 0.89059 to 0.89168, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 36: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.8068 - loss: 0.6479 - val_accuracy: 0.8917 - val_loss: 0.4364 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8147 - loss: 0.5991\n",
      "Epoch 37: val_accuracy improved from 0.89168 to 0.89278, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 37: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8172 - loss: 0.6032 - val_accuracy: 0.8928 - val_loss: 0.4368 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8162 - loss: 0.5970\n",
      "Epoch 38: val_accuracy improved from 0.89278 to 0.89497, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 38: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8179 - loss: 0.5940 - val_accuracy: 0.8950 - val_loss: 0.4289 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8303 - loss: 0.5635\n",
      "Epoch 39: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8231 - loss: 0.5834 - val_accuracy: 0.8939 - val_loss: 0.4325 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8318 - loss: 0.5417\n",
      "Epoch 40: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8246 - loss: 0.5549 - val_accuracy: 0.8928 - val_loss: 0.4312 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8239 - loss: 0.5576\n",
      "Epoch 41: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8284 - loss: 0.5508 - val_accuracy: 0.8906 - val_loss: 0.4362 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8344 - loss: 0.5139\n",
      "Epoch 42: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8298 - loss: 0.5330 - val_accuracy: 0.8917 - val_loss: 0.4256 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8312 - loss: 0.5267\n",
      "Epoch 43: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8346 - loss: 0.5349 - val_accuracy: 0.8950 - val_loss: 0.4196 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8363 - loss: 0.5132\n",
      "Epoch 44: val_accuracy improved from 0.89497 to 0.89606, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 44: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8334 - loss: 0.5211 - val_accuracy: 0.8961 - val_loss: 0.4267 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8376 - loss: 0.5050\n",
      "Epoch 45: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8316 - loss: 0.5287 - val_accuracy: 0.8917 - val_loss: 0.4254 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8361 - loss: 0.4915\n",
      "Epoch 46: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8409 - loss: 0.5028 - val_accuracy: 0.8917 - val_loss: 0.4263 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8414 - loss: 0.5256\n",
      "Epoch 47: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8390 - loss: 0.5069 - val_accuracy: 0.8928 - val_loss: 0.4202 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8362 - loss: 0.5069\n",
      "Epoch 48: val_accuracy improved from 0.89606 to 0.89716, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 48: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.8402 - loss: 0.5071 - val_accuracy: 0.8972 - val_loss: 0.4186 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8423 - loss: 0.4724\n",
      "Epoch 49: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8416 - loss: 0.4951 - val_accuracy: 0.8917 - val_loss: 0.4109 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8484 - loss: 0.4619\n",
      "Epoch 50: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8498 - loss: 0.4568 - val_accuracy: 0.8972 - val_loss: 0.4175 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "--- Phase 2: Fine-tuning last 20 layers ---\n",
      "  Trainable params after unfreezing: 20,443,814\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:45:39.504251: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:39.504281: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:39.504289: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:40.322591: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1910', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.505592: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1858', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.516915: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1910', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.580995: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1802', 344 bytes spill stores, 344 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.681628: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1858', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:42.850510: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:45:46.157659: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:46:02.927408: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.41GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:46:17.397755: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 569ms/step - accuracy: 0.8447 - loss: 0.4653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:48:29.113003: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681ms/step - accuracy: 0.8448 - loss: 0.4652\n",
      "Epoch 1: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 706ms/step - accuracy: 0.8476 - loss: 0.4549 - val_accuracy: 0.8939 - val_loss: 0.4132 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8542 - loss: 0.4510\n",
      "Epoch 2: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 568ms/step - accuracy: 0.8591 - loss: 0.4387 - val_accuracy: 0.8961 - val_loss: 0.4034 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8497 - loss: 0.4305\n",
      "Epoch 3: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8559 - loss: 0.4280 - val_accuracy: 0.8972 - val_loss: 0.4026 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8610 - loss: 0.3934\n",
      "Epoch 4: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 568ms/step - accuracy: 0.8578 - loss: 0.4062 - val_accuracy: 0.8972 - val_loss: 0.3908 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8639 - loss: 0.4180\n",
      "Epoch 5: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8584 - loss: 0.4162 - val_accuracy: 0.8961 - val_loss: 0.3927 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8641 - loss: 0.4126\n",
      "Epoch 6: val_accuracy improved from 0.89716 to 0.90044, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 6: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 569ms/step - accuracy: 0.8637 - loss: 0.4108 - val_accuracy: 0.9004 - val_loss: 0.3860 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553ms/step - accuracy: 0.8692 - loss: 0.3921\n",
      "Epoch 7: val_accuracy did not improve from 0.90044\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8625 - loss: 0.3970 - val_accuracy: 0.8982 - val_loss: 0.3906 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8705 - loss: 0.3976\n",
      "Epoch 8: val_accuracy did not improve from 0.90044\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 567ms/step - accuracy: 0.8662 - loss: 0.4006 - val_accuracy: 0.8993 - val_loss: 0.3860 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8627 - loss: 0.3916\n",
      "Epoch 9: val_accuracy improved from 0.90044 to 0.90263, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 9: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 575ms/step - accuracy: 0.8662 - loss: 0.3924 - val_accuracy: 0.9026 - val_loss: 0.3833 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8697 - loss: 0.3687\n",
      "Epoch 10: val_accuracy improved from 0.90263 to 0.90591, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 569ms/step - accuracy: 0.8684 - loss: 0.3651 - val_accuracy: 0.9059 - val_loss: 0.3794 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "VGG19 training complete!\n",
      "Total epochs: 60\n"
     ]
    }
   ],
   "source": [
    "# Train VGG19\n",
    "model_name = 'VGG19'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training {model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "train_ds, val_ds, test_ds = get_datasets_for_model(model_name)\n",
    "\n",
    "# Check if checkpoint exists and load it, otherwise create new model\n",
    "checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.h5\"\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"\\nLoading existing model from: {checkpoint_path}\")\n",
    "    model = keras.models.load_model(checkpoint_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"\\nNo checkpoint found. Creating new {model_name} model...\")\n",
    "    model = create_model(model_name, NUM_CLASSES, CONFIG['img_size'])\n",
    "\n",
    "print(f\"\\n{model_name} Summary:\")\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.reduce_prod(v.shape).numpy() for v in model.trainable_variables])\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Fine-tuned layers: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "print(f\"  Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")\n",
    "\n",
    "callbacks = get_callbacks(model_name)\n",
    "\n",
    "print(f\"\\n--- Training with fine-tuning (last {FINE_TUNE_LAYERS} layers) ---\")\n",
    "print(f\"Early Stopping: patience={CONFIG['patience_early_stop']}, monitor='val_loss'\")\n",
    "print(f\"Early Stopping: min_epochs={MIN_EPOCHS_BEFORE_EARLY_STOP} (minimum training guaranteed)\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHT_DICT,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_histories[model_name] = history\n",
    "trained_models[model_name] = model\n",
    "\n",
    "print(f\"\\n{model_name} training complete!\")\n",
    "print(f\"Total epochs: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50967084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training VGG19\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770072071.045398  118378 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3398 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No checkpoint found. Creating new VGG19 model...\n",
      "\n",
      "VGG19 Summary:\n",
      "  Total params: 20,446,630\n",
      "  Trainable params: 421,222\n",
      "\n",
      "--- Phase 1: Training classification head ---\n",
      "Early Stopping: patience=10, monitor='val_loss'\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:11:13.318048: I external/local_xla/xla/service/service.cc:163] XLA service 0x716290010d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2026-02-03 04:11:13.318073: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2026-02-03 04:11:13.551643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2026-02-03 04:11:13.876280: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91801\n",
      "2026-02-03 04:11:13.954477: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954503: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954548: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954559: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:13.954565: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:11:14.523011: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:14.957133: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1886', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.014894: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.246000: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1778', 332 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:15.726707: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1834', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:11:19.056332: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:11:20.040428: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2026-02-03 04:11:22.024797: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:11:24.686833: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "I0000 00:00:1770072090.042138  118908 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.0175 - loss: 5.0671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:12:06.974217: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:06.974246: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:07.140540: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.698903: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1759', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.792575: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:07.802529: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1550', 364 bytes spill stores, 364 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:09.977328: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.0176 - loss: 5.0662"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:12:18.807080: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:19.289817: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:19.528417: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 324 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:25.016513: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:12:25.489088: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:25.760911: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_284', 372 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "2026-02-03 04:12:27.578850: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.16521, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 1: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 283ms/step - accuracy: 0.0271 - loss: 4.8654 - val_accuracy: 0.1652 - val_loss: 3.9945 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.0751 - loss: 4.2922\n",
      "Epoch 2: val_accuracy improved from 0.16521 to 0.44748, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 2: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 188ms/step - accuracy: 0.1012 - loss: 4.2041 - val_accuracy: 0.4475 - val_loss: 3.2475 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.1892 - loss: 3.7958\n",
      "Epoch 3: val_accuracy improved from 0.44748 to 0.59081, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 3: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.2224 - loss: 3.6200 - val_accuracy: 0.5908 - val_loss: 2.3353 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.3180 - loss: 3.1739\n",
      "Epoch 4: val_accuracy improved from 0.59081 to 0.68600, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 4: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.3357 - loss: 3.0777 - val_accuracy: 0.6860 - val_loss: 1.6646 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4115 - loss: 2.6430\n",
      "Epoch 5: val_accuracy improved from 0.68600 to 0.75492, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 5: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.4171 - loss: 2.6182 - val_accuracy: 0.7549 - val_loss: 1.2716 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4703 - loss: 2.2797\n",
      "Epoch 6: val_accuracy improved from 0.75492 to 0.78665, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 6: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.4726 - loss: 2.2538 - val_accuracy: 0.7867 - val_loss: 1.0130 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.5234 - loss: 2.0230\n",
      "Epoch 7: val_accuracy improved from 0.78665 to 0.80744, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 7: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 185ms/step - accuracy: 0.5277 - loss: 2.0076 - val_accuracy: 0.8074 - val_loss: 0.8755 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5479 - loss: 1.8607\n",
      "Epoch 8: val_accuracy improved from 0.80744 to 0.82166, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 8: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.5545 - loss: 1.8373 - val_accuracy: 0.8217 - val_loss: 0.7833 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5776 - loss: 1.6751\n",
      "Epoch 9: val_accuracy improved from 0.82166 to 0.82823, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 9: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.5844 - loss: 1.6671 - val_accuracy: 0.8282 - val_loss: 0.7129 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6123 - loss: 1.5647\n",
      "Epoch 10: val_accuracy improved from 0.82823 to 0.84573, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.6168 - loss: 1.5359 - val_accuracy: 0.8457 - val_loss: 0.6686 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6205 - loss: 1.4827\n",
      "Epoch 11: val_accuracy did not improve from 0.84573\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.6276 - loss: 1.4301 - val_accuracy: 0.8435 - val_loss: 0.6404 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6562 - loss: 1.3431\n",
      "Epoch 12: val_accuracy did not improve from 0.84573\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.6485 - loss: 1.3616 - val_accuracy: 0.8457 - val_loss: 0.6060 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6676 - loss: 1.2603\n",
      "Epoch 13: val_accuracy improved from 0.84573 to 0.85449, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 13: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6651 - loss: 1.2792 - val_accuracy: 0.8545 - val_loss: 0.5856 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.6646 - loss: 1.2279\n",
      "Epoch 14: val_accuracy did not improve from 0.85449\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6700 - loss: 1.2435 - val_accuracy: 0.8534 - val_loss: 0.5722 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6895 - loss: 1.1857\n",
      "Epoch 15: val_accuracy improved from 0.85449 to 0.85667, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 15: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6850 - loss: 1.1818 - val_accuracy: 0.8567 - val_loss: 0.5624 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.6919 - loss: 1.1099\n",
      "Epoch 16: val_accuracy improved from 0.85667 to 0.86214, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 16: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.6965 - loss: 1.0878 - val_accuracy: 0.8621 - val_loss: 0.5351 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7153 - loss: 1.0528\n",
      "Epoch 17: val_accuracy improved from 0.86214 to 0.86433, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 17: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.7106 - loss: 1.0730 - val_accuracy: 0.8643 - val_loss: 0.5359 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7109 - loss: 1.0560\n",
      "Epoch 18: val_accuracy improved from 0.86433 to 0.86980, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 18: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7169 - loss: 1.0324 - val_accuracy: 0.8698 - val_loss: 0.5131 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7331 - loss: 0.9986\n",
      "Epoch 19: val_accuracy did not improve from 0.86980\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.7347 - loss: 0.9769 - val_accuracy: 0.8698 - val_loss: 0.5035 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7432 - loss: 0.9378\n",
      "Epoch 20: val_accuracy improved from 0.86980 to 0.87418, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 20: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7373 - loss: 0.9712 - val_accuracy: 0.8742 - val_loss: 0.5003 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7494 - loss: 0.9141\n",
      "Epoch 21: val_accuracy improved from 0.87418 to 0.87637, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 21: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.7486 - loss: 0.9227 - val_accuracy: 0.8764 - val_loss: 0.4935 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7483 - loss: 0.8808\n",
      "Epoch 22: val_accuracy improved from 0.87637 to 0.87965, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 22: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7430 - loss: 0.9231 - val_accuracy: 0.8796 - val_loss: 0.4855 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7553 - loss: 0.8726\n",
      "Epoch 23: val_accuracy did not improve from 0.87965\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7565 - loss: 0.8482 - val_accuracy: 0.8796 - val_loss: 0.4738 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7712 - loss: 0.8117\n",
      "Epoch 24: val_accuracy improved from 0.87965 to 0.88184, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 24: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.7646 - loss: 0.8451 - val_accuracy: 0.8818 - val_loss: 0.4776 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7735 - loss: 0.7874\n",
      "Epoch 25: val_accuracy improved from 0.88184 to 0.88512, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 25: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7729 - loss: 0.7879 - val_accuracy: 0.8851 - val_loss: 0.4656 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7653 - loss: 0.8088\n",
      "Epoch 26: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.7702 - loss: 0.7968 - val_accuracy: 0.8840 - val_loss: 0.4653 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7780 - loss: 0.7625\n",
      "Epoch 27: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7776 - loss: 0.7566 - val_accuracy: 0.8807 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7867 - loss: 0.7270\n",
      "Epoch 28: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7813 - loss: 0.7542 - val_accuracy: 0.8840 - val_loss: 0.4552 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7947 - loss: 0.7123\n",
      "Epoch 29: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7975 - loss: 0.7098 - val_accuracy: 0.8829 - val_loss: 0.4563 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7939 - loss: 0.7052\n",
      "Epoch 30: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7904 - loss: 0.7121 - val_accuracy: 0.8829 - val_loss: 0.4474 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7973 - loss: 0.6798\n",
      "Epoch 31: val_accuracy did not improve from 0.88512\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7917 - loss: 0.6916 - val_accuracy: 0.8807 - val_loss: 0.4428 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8131 - loss: 0.6363\n",
      "Epoch 32: val_accuracy improved from 0.88512 to 0.88621, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 32: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8066 - loss: 0.6662 - val_accuracy: 0.8862 - val_loss: 0.4538 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8031 - loss: 0.6399\n",
      "Epoch 33: val_accuracy did not improve from 0.88621\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8014 - loss: 0.6668 - val_accuracy: 0.8818 - val_loss: 0.4389 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8039 - loss: 0.6542\n",
      "Epoch 34: val_accuracy improved from 0.88621 to 0.88950, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 34: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8040 - loss: 0.6449 - val_accuracy: 0.8895 - val_loss: 0.4341 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8153 - loss: 0.6051\n",
      "Epoch 35: val_accuracy improved from 0.88950 to 0.89059, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 35: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8116 - loss: 0.6207 - val_accuracy: 0.8906 - val_loss: 0.4390 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8078 - loss: 0.6354\n",
      "Epoch 36: val_accuracy improved from 0.89059 to 0.89168, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 36: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - accuracy: 0.8068 - loss: 0.6479 - val_accuracy: 0.8917 - val_loss: 0.4364 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8147 - loss: 0.5991\n",
      "Epoch 37: val_accuracy improved from 0.89168 to 0.89278, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 37: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8172 - loss: 0.6032 - val_accuracy: 0.8928 - val_loss: 0.4368 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8162 - loss: 0.5970\n",
      "Epoch 38: val_accuracy improved from 0.89278 to 0.89497, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 38: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8179 - loss: 0.5940 - val_accuracy: 0.8950 - val_loss: 0.4289 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8303 - loss: 0.5635\n",
      "Epoch 39: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8231 - loss: 0.5834 - val_accuracy: 0.8939 - val_loss: 0.4325 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8318 - loss: 0.5417\n",
      "Epoch 40: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8246 - loss: 0.5549 - val_accuracy: 0.8928 - val_loss: 0.4312 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8239 - loss: 0.5576\n",
      "Epoch 41: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8284 - loss: 0.5508 - val_accuracy: 0.8906 - val_loss: 0.4362 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8344 - loss: 0.5139\n",
      "Epoch 42: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8298 - loss: 0.5330 - val_accuracy: 0.8917 - val_loss: 0.4256 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8312 - loss: 0.5267\n",
      "Epoch 43: val_accuracy did not improve from 0.89497\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8346 - loss: 0.5349 - val_accuracy: 0.8950 - val_loss: 0.4196 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8363 - loss: 0.5132\n",
      "Epoch 44: val_accuracy improved from 0.89497 to 0.89606, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 44: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8334 - loss: 0.5211 - val_accuracy: 0.8961 - val_loss: 0.4267 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8376 - loss: 0.5050\n",
      "Epoch 45: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8316 - loss: 0.5287 - val_accuracy: 0.8917 - val_loss: 0.4254 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8361 - loss: 0.4915\n",
      "Epoch 46: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8409 - loss: 0.5028 - val_accuracy: 0.8917 - val_loss: 0.4263 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8414 - loss: 0.5256\n",
      "Epoch 47: val_accuracy did not improve from 0.89606\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - accuracy: 0.8390 - loss: 0.5069 - val_accuracy: 0.8928 - val_loss: 0.4202 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8362 - loss: 0.5069\n",
      "Epoch 48: val_accuracy improved from 0.89606 to 0.89716, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 48: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.8402 - loss: 0.5071 - val_accuracy: 0.8972 - val_loss: 0.4186 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8423 - loss: 0.4724\n",
      "Epoch 49: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8416 - loss: 0.4951 - val_accuracy: 0.8917 - val_loss: 0.4109 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8484 - loss: 0.4619\n",
      "Epoch 50: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8498 - loss: 0.4568 - val_accuracy: 0.8972 - val_loss: 0.4175 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "--- Phase 2: Fine-tuning last 20 layers ---\n",
      "  Trainable params after unfreezing: 20,443,814\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:45:39.504251: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:39.504281: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:39.504289: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 04:45:40.322591: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1910', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.505592: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1858', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.516915: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1910', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.580995: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1802', 344 bytes spill stores, 344 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:40.681628: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1858', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2026-02-03 04:45:42.850510: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:45:46.157659: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:46:02.927408: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.41GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2026-02-03 04:46:17.397755: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 569ms/step - accuracy: 0.8447 - loss: 0.4653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 04:48:29.113003: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681ms/step - accuracy: 0.8448 - loss: 0.4652\n",
      "Epoch 1: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 706ms/step - accuracy: 0.8476 - loss: 0.4549 - val_accuracy: 0.8939 - val_loss: 0.4132 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8542 - loss: 0.4510\n",
      "Epoch 2: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 568ms/step - accuracy: 0.8591 - loss: 0.4387 - val_accuracy: 0.8961 - val_loss: 0.4034 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8497 - loss: 0.4305\n",
      "Epoch 3: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8559 - loss: 0.4280 - val_accuracy: 0.8972 - val_loss: 0.4026 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8610 - loss: 0.3934\n",
      "Epoch 4: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 568ms/step - accuracy: 0.8578 - loss: 0.4062 - val_accuracy: 0.8972 - val_loss: 0.3908 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8639 - loss: 0.4180\n",
      "Epoch 5: val_accuracy did not improve from 0.89716\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8584 - loss: 0.4162 - val_accuracy: 0.8961 - val_loss: 0.3927 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8641 - loss: 0.4126\n",
      "Epoch 6: val_accuracy improved from 0.89716 to 0.90044, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 6: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 569ms/step - accuracy: 0.8637 - loss: 0.4108 - val_accuracy: 0.9004 - val_loss: 0.3860 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553ms/step - accuracy: 0.8692 - loss: 0.3921\n",
      "Epoch 7: val_accuracy did not improve from 0.90044\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 573ms/step - accuracy: 0.8625 - loss: 0.3970 - val_accuracy: 0.8982 - val_loss: 0.3906 - learning_rate: 1.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8705 - loss: 0.3976\n",
      "Epoch 8: val_accuracy did not improve from 0.90044\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 567ms/step - accuracy: 0.8662 - loss: 0.4006 - val_accuracy: 0.8993 - val_loss: 0.3860 - learning_rate: 1.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - accuracy: 0.8627 - loss: 0.3916\n",
      "Epoch 9: val_accuracy improved from 0.90044 to 0.90263, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 9: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 575ms/step - accuracy: 0.8662 - loss: 0.3924 - val_accuracy: 0.9026 - val_loss: 0.3833 - learning_rate: 1.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.8697 - loss: 0.3687\n",
      "Epoch 10: val_accuracy improved from 0.90263 to 0.90591, saving model to checkpoints/VGG19_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to checkpoints/VGG19_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 569ms/step - accuracy: 0.8684 - loss: 0.3651 - val_accuracy: 0.9059 - val_loss: 0.3794 - learning_rate: 1.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "VGG19 training complete!\n",
      "Total epochs: 60\n"
     ]
    }
   ],
   "source": [
    "# Train VGG19\n",
    "model_name = 'VGG19'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training {model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "train_ds, val_ds, test_ds = get_datasets_for_model(model_name)\n",
    "\n",
    "# Check if checkpoint exists and load it, otherwise create new model\n",
    "checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.h5\"\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"\\nLoading existing model from: {checkpoint_path}\")\n",
    "    model = keras.models.load_model(checkpoint_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"\\nNo checkpoint found. Creating new {model_name} model...\")\n",
    "    model = create_model(model_name, NUM_CLASSES, CONFIG['img_size'])\n",
    "\n",
    "print(f\"\\n{model_name} Summary:\")\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.reduce_prod(v.shape).numpy() for v in model.trainable_variables])\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Fine-tuned layers: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "print(f\"  Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")\n",
    "\n",
    "callbacks = get_callbacks(model_name)\n",
    "\n",
    "print(f\"\\n--- Training with fine-tuning (last {FINE_TUNE_LAYERS} layers) ---\")\n",
    "print(f\"Early Stopping: patience={CONFIG['patience_early_stop']}, monitor='val_loss'\")\n",
    "print(f\"Early Stopping: min_epochs={MIN_EPOCHS_BEFORE_EARLY_STOP} (minimum training guaranteed)\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHT_DICT,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_histories[model_name] = history\n",
    "trained_models[model_name] = model\n",
    "\n",
    "print(f\"\\n{model_name} training complete!\")\n",
    "print(f\"Total epochs: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c286d",
   "metadata": {},
   "source": [
    "## 10.2 Train ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training ResNet101\n",
      "================================================================================\n",
      "\n",
      "No checkpoint found. Creating new ResNet101 model...\n",
      "\n",
      "ResNet101 Summary:\n",
      "  Total params: 43,872,998\n",
      "  Trainable params: 1,210,726\n",
      "\n",
      "Early Stopping: patience=10, monitor='val_loss', restore_best_weights=True\n",
      "LR Reduction: patience=5, factor=0.5, min_lr=1e-7\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 05:11:27.489362: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-02-03 05:11:27.918270: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10319', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2026-02-03 05:11:28.307889: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10757', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.0297 - loss: 5.0476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 05:12:04.978442: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10742', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2026-02-03 05:12:05.126289: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10319', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.0298 - loss: 5.0459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 05:12:17.183926: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3317', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.47812, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 1: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 206ms/step - accuracy: 0.0637 - loss: 4.6594 - val_accuracy: 0.4781 - val_loss: 3.2524 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.2128 - loss: 3.5421\n",
      "Epoch 2: val_accuracy improved from 0.47812 to 0.71772, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 2: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.2708 - loss: 3.3707 - val_accuracy: 0.7177 - val_loss: 1.9173 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.4246 - loss: 2.6231\n",
      "Epoch 3: val_accuracy improved from 0.71772 to 0.80197, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 3: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.4515 - loss: 2.4744 - val_accuracy: 0.8020 - val_loss: 1.1202 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.5265 - loss: 1.9916\n",
      "Epoch 4: val_accuracy improved from 0.80197 to 0.84464, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 4: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - accuracy: 0.5485 - loss: 1.8839 - val_accuracy: 0.8446 - val_loss: 0.7844 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6052 - loss: 1.6093\n",
      "Epoch 5: val_accuracy improved from 0.84464 to 0.85886, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 5: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.6146 - loss: 1.5749 - val_accuracy: 0.8589 - val_loss: 0.6429 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6642 - loss: 1.3325\n",
      "Epoch 6: val_accuracy improved from 0.85886 to 0.86980, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 6: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.6688 - loss: 1.3181 - val_accuracy: 0.8698 - val_loss: 0.5594 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7038 - loss: 1.1118\n",
      "Epoch 7: val_accuracy did not improve from 0.86980\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - accuracy: 0.7012 - loss: 1.1347 - val_accuracy: 0.8654 - val_loss: 0.5217 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7153 - loss: 1.0524\n",
      "Epoch 8: val_accuracy improved from 0.86980 to 0.87637, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 8: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7198 - loss: 1.0439 - val_accuracy: 0.8764 - val_loss: 0.4904 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7497 - loss: 0.9227\n",
      "Epoch 9: val_accuracy improved from 0.87637 to 0.87965, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 9: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7468 - loss: 0.9330 - val_accuracy: 0.8796 - val_loss: 0.4594 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7576 - loss: 0.8774\n",
      "Epoch 10: val_accuracy improved from 0.87965 to 0.88184, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7572 - loss: 0.8721 - val_accuracy: 0.8818 - val_loss: 0.4311 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7649 - loss: 0.8402\n",
      "Epoch 11: val_accuracy did not improve from 0.88184\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 131ms/step - accuracy: 0.7705 - loss: 0.8006 - val_accuracy: 0.8818 - val_loss: 0.4148 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7772 - loss: 0.7887\n",
      "Epoch 12: val_accuracy improved from 0.88184 to 0.88731, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 12: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7861 - loss: 0.7475 - val_accuracy: 0.8873 - val_loss: 0.3990 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7970 - loss: 0.6880\n",
      "Epoch 13: val_accuracy improved from 0.88731 to 0.89387, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 13: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.7944 - loss: 0.7025 - val_accuracy: 0.8939 - val_loss: 0.3895 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8143 - loss: 0.6416\n",
      "Epoch 14: val_accuracy improved from 0.89387 to 0.89497, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 14: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.8100 - loss: 0.6575 - val_accuracy: 0.8950 - val_loss: 0.3836 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8188 - loss: 0.6160\n",
      "Epoch 15: val_accuracy improved from 0.89497 to 0.89934, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 15: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 134ms/step - accuracy: 0.8159 - loss: 0.6112 - val_accuracy: 0.8993 - val_loss: 0.3791 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8171 - loss: 0.5947\n",
      "Epoch 16: val_accuracy improved from 0.89934 to 0.90372, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 16: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.8165 - loss: 0.5834 - val_accuracy: 0.9037 - val_loss: 0.3535 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8371 - loss: 0.5239\n",
      "Epoch 17: val_accuracy did not improve from 0.90372\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 131ms/step - accuracy: 0.8334 - loss: 0.5379 - val_accuracy: 0.9004 - val_loss: 0.3700 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8337 - loss: 0.5440\n",
      "Epoch 18: val_accuracy did not improve from 0.90372\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 131ms/step - accuracy: 0.8353 - loss: 0.5348 - val_accuracy: 0.9037 - val_loss: 0.3635 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8390 - loss: 0.5155\n",
      "Epoch 19: val_accuracy improved from 0.90372 to 0.90481, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 19: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 139ms/step - accuracy: 0.8370 - loss: 0.5062 - val_accuracy: 0.9048 - val_loss: 0.3504 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8532 - loss: 0.4428\n",
      "Epoch 20: val_accuracy improved from 0.90481 to 0.90810, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 20: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.8499 - loss: 0.4668 - val_accuracy: 0.9081 - val_loss: 0.3457 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8504 - loss: 0.4668\n",
      "Epoch 21: val_accuracy did not improve from 0.90810\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - accuracy: 0.8540 - loss: 0.4662 - val_accuracy: 0.9037 - val_loss: 0.3552 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8665 - loss: 0.4038\n",
      "Epoch 22: val_accuracy improved from 0.90810 to 0.90919, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 22: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 132ms/step - accuracy: 0.8606 - loss: 0.4289 - val_accuracy: 0.9092 - val_loss: 0.3579 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8623 - loss: 0.4235\n",
      "Epoch 23: val_accuracy did not improve from 0.90919\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 128ms/step - accuracy: 0.8627 - loss: 0.4079 - val_accuracy: 0.9070 - val_loss: 0.3495 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8618 - loss: 0.4186\n",
      "Epoch 24: val_accuracy improved from 0.90919 to 0.91028, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 24: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 131ms/step - accuracy: 0.8599 - loss: 0.4134 - val_accuracy: 0.9103 - val_loss: 0.3552 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8742 - loss: 0.3598\n",
      "Epoch 25: val_accuracy did not improve from 0.91028\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - accuracy: 0.8720 - loss: 0.3721 - val_accuracy: 0.9092 - val_loss: 0.3414 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8704 - loss: 0.3779\n",
      "Epoch 26: val_accuracy improved from 0.91028 to 0.91685, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 26: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 133ms/step - accuracy: 0.8740 - loss: 0.3690 - val_accuracy: 0.9168 - val_loss: 0.3390 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8783 - loss: 0.3403\n",
      "Epoch 27: val_accuracy did not improve from 0.91685\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.8789 - loss: 0.3540 - val_accuracy: 0.9103 - val_loss: 0.3487 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8817 - loss: 0.3318\n",
      "Epoch 28: val_accuracy did not improve from 0.91685\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.8802 - loss: 0.3387 - val_accuracy: 0.9136 - val_loss: 0.3395 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8769 - loss: 0.3264\n",
      "Epoch 29: val_accuracy improved from 0.91685 to 0.91794, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 29: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 137ms/step - accuracy: 0.8794 - loss: 0.3312 - val_accuracy: 0.9179 - val_loss: 0.3416 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8846 - loss: 0.3320\n",
      "Epoch 30: val_accuracy improved from 0.91794 to 0.91904, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 30: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 133ms/step - accuracy: 0.8841 - loss: 0.3311 - val_accuracy: 0.9190 - val_loss: 0.3336 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8838 - loss: 0.3129\n",
      "Epoch 31: val_accuracy improved from 0.91904 to 0.92341, saving model to checkpoints/ResNet101_best.keras\n",
      "\n",
      "Epoch 31: finished saving model to checkpoints/ResNet101_best.keras\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 133ms/step - accuracy: 0.8811 - loss: 0.3097 - val_accuracy: 0.9234 - val_loss: 0.3300 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9037 - loss: 0.2567\n",
      "Epoch 32: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.9040 - loss: 0.2725 - val_accuracy: 0.9190 - val_loss: 0.3453 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9012 - loss: 0.2893\n",
      "Epoch 33: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 130ms/step - accuracy: 0.8983 - loss: 0.3005 - val_accuracy: 0.9223 - val_loss: 0.3393 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8935 - loss: 0.2908\n",
      "Epoch 34: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.8927 - loss: 0.2962 - val_accuracy: 0.9147 - val_loss: 0.3386 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8980 - loss: 0.2711\n",
      "Epoch 35: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9005 - loss: 0.2660 - val_accuracy: 0.9158 - val_loss: 0.3412 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9078 - loss: 0.2911\n",
      "Epoch 36: val_accuracy did not improve from 0.92341\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9017 - loss: 0.2948 - val_accuracy: 0.9168 - val_loss: 0.3382 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8970 - loss: 0.2601\n",
      "Epoch 37: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9031 - loss: 0.2511 - val_accuracy: 0.9190 - val_loss: 0.3337 - learning_rate: 5.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9166 - loss: 0.2243\n",
      "Epoch 38: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9191 - loss: 0.2239 - val_accuracy: 0.9179 - val_loss: 0.3317 - learning_rate: 5.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9130 - loss: 0.2325\n",
      "Epoch 39: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9090 - loss: 0.2399 - val_accuracy: 0.9201 - val_loss: 0.3322 - learning_rate: 5.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9085 - loss: 0.2141\n",
      "Epoch 40: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9132 - loss: 0.2182 - val_accuracy: 0.9212 - val_loss: 0.3263 - learning_rate: 5.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9039 - loss: 0.2384\n",
      "Epoch 41: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9094 - loss: 0.2293 - val_accuracy: 0.9179 - val_loss: 0.3363 - learning_rate: 5.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9165 - loss: 0.2115\n",
      "Epoch 42: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9146 - loss: 0.2193 - val_accuracy: 0.9212 - val_loss: 0.3218 - learning_rate: 5.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9168 - loss: 0.2109\n",
      "Epoch 43: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9147 - loss: 0.2248 - val_accuracy: 0.9201 - val_loss: 0.3230 - learning_rate: 5.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9207 - loss: 0.1938\n",
      "Epoch 44: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 129ms/step - accuracy: 0.9224 - loss: 0.1863 - val_accuracy: 0.9223 - val_loss: 0.3291 - learning_rate: 5.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9254 - loss: 0.1893\n",
      "Epoch 45: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9195 - loss: 0.2095 - val_accuracy: 0.9201 - val_loss: 0.3337 - learning_rate: 5.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9309 - loss: 0.1800\n",
      "Epoch 46: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9271 - loss: 0.1951 - val_accuracy: 0.9179 - val_loss: 0.3273 - learning_rate: 5.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9257 - loss: 0.1966\n",
      "Epoch 47: val_accuracy did not improve from 0.92341\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9234 - loss: 0.1966 - val_accuracy: 0.9179 - val_loss: 0.3271 - learning_rate: 5.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9214 - loss: 0.1747\n",
      "Epoch 48: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9211 - loss: 0.1901 - val_accuracy: 0.9190 - val_loss: 0.3244 - learning_rate: 2.5000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9285 - loss: 0.2035\n",
      "Epoch 49: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 129ms/step - accuracy: 0.9241 - loss: 0.2000 - val_accuracy: 0.9190 - val_loss: 0.3264 - learning_rate: 2.5000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9210 - loss: 0.1959\n",
      "Epoch 50: val_accuracy did not improve from 0.92341\n",
      "\u001b[1m229/229\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 129ms/step - accuracy: 0.9213 - loss: 0.1913 - val_accuracy: 0.9234 - val_loss: 0.3195 - learning_rate: 2.5000e-05\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "ResNet101 training complete!\n",
      "Stopped at epoch: 50/50\n"
     ]
    }
   ],
   "source": [
    "# Train ResNet50\n",
    "model_name = 'ResNet50'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training {model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "train_ds, val_ds, test_ds = get_datasets_for_model(model_name)\n",
    "\n",
    "# Check if checkpoint exists and load it, otherwise create new model\n",
    "checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.h5\"\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"\\nLoading existing model from: {checkpoint_path}\")\n",
    "    model = keras.models.load_model(checkpoint_path)\n",
    "    print(\"Model loaded successfully! Resuming training...\")\n",
    "else:\n",
    "    print(f\"\\nNo checkpoint found. Creating new {model_name} model...\")\n",
    "    model = create_model(model_name, NUM_CLASSES, CONFIG['img_size'])\n",
    "\n",
    "print(f\"\\n{model_name} Summary:\")\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.reduce_prod(v.shape).numpy() for v in model.trainable_variables])\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Fine-tuned layers: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "print(f\"  Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")\n",
    "\n",
    "callbacks = get_callbacks(model_name)\n",
    "\n",
    "print(f\"\\nEarly Stopping: patience={CONFIG['patience_early_stop']}, monitor='val_loss', restore_best_weights=True\")\n",
    "print(f\"Early Stopping: min_epochs={MIN_EPOCHS_BEFORE_EARLY_STOP} (minimum training guaranteed)\")\n",
    "print(f\"LR Reduction: patience={CONFIG['patience_lr_reduce']}, factor=0.5, min_lr=1e-7\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHT_DICT,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_histories[model_name] = history\n",
    "trained_models[model_name] = model\n",
    "\n",
    "print(f\"Stopped at epoch: {len(history.history['loss'])}/{CONFIG['epochs']}\")\n",
    "print(f\"\\n{model_name} training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10073d40",
   "metadata": {},
   "source": [
    "## 10.3 Train DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a23a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DenseNet121\n",
    "model_name = 'DenseNet121'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training {model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "train_ds, val_ds, test_ds = get_datasets_for_model(model_name)\n",
    "\n",
    "# Check if checkpoint exists and load it, otherwise create new model\n",
    "checkpoint_path = CHECKPOINT_DIR / f\"{model_name}_best.h5\"\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"\\nLoading existing model from: {checkpoint_path}\")\n",
    "    model = keras.models.load_model(checkpoint_path)\n",
    "    print(\"Model loaded successfully! Resuming training...\")\n",
    "else:\n",
    "    print(f\"\\nNo checkpoint found. Creating new {model_name} model...\")\n",
    "    model = create_model(model_name, NUM_CLASSES, CONFIG['img_size'])\n",
    "\n",
    "print(f\"\\n{model_name} Summary:\")\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.reduce_prod(v.shape).numpy() for v in model.trainable_variables])\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  Fine-tuned layers: Last {FINE_TUNE_LAYERS} layers unfrozen\")\n",
    "print(f\"  Minimum epochs before early stopping: {MIN_EPOCHS_BEFORE_EARLY_STOP}\")\n",
    "\n",
    "callbacks = get_callbacks(model_name)\n",
    "\n",
    "print(f\"\\nEarly Stopping: patience={CONFIG['patience_early_stop']}, monitor='val_loss', restore_best_weights=True\")\n",
    "print(f\"Early Stopping: min_epochs={MIN_EPOCHS_BEFORE_EARLY_STOP} (minimum training guaranteed)\")\n",
    "print(f\"LR Reduction: patience={CONFIG['patience_lr_reduce']}, factor=0.5, min_lr=1e-7\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=CLASS_WEIGHT_DICT,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_histories[model_name] = history\n",
    "trained_models[model_name] = model\n",
    "\n",
    "print(f\"Stopped at epoch: {len(history.history['loss'])}/{CONFIG['epochs']}\")\n",
    "print(f\"\\n{model_name} training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b405e",
   "metadata": {},
   "source": [
    "## 11. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6997f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_results = {}\n",
    "model_predictions = {}  # Cache predictions to avoid recomputing\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    _, _, test_ds = get_datasets_for_model(model_name)\n",
    "    model = trained_models[model_name]\n",
    "    \n",
    "    # Get predictions once and reuse\n",
    "    print(f\"  Computing predictions...\", end=\" \", flush=True)\n",
    "    y_pred_probs = model.predict(test_ds, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = test_labels.astype(int)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    # Cache for later use\n",
    "    model_predictions[model_name] = {\n",
    "        'probs': y_pred_probs,\n",
    "        'pred': y_pred,\n",
    "        'true': y_true\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics from predictions\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "    loss, _ = model.evaluate(test_ds, verbose=0)\n",
    "    \n",
    "    test_results[model_name] = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"  Test Loss: {loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Model':<15} {'Test Loss':<12} {'Test Accuracy':<15}\")\n",
    "print(\"-\" * 42)\n",
    "for model_name, results in test_results.items():\n",
    "    print(f\"{model_name:<15} {results['loss']:<12.4f} {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "best_model = max(test_results.keys(), key=lambda k: test_results[k]['accuracy'])\n",
    "print(f\"\\nBest performing model: {best_model} with {test_results[best_model]['accuracy']*100:.2f}% accuracy\")\n",
    "\n",
    "# --- Detailed diagnostics for each model (using cached predictions) ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    \n",
    "    # Use cached predictions\n",
    "    y_pred = model_predictions[model_name]['pred']\n",
    "    y_true = model_predictions[model_name]['true']\n",
    "    \n",
    "    # Confusion matrix - find most confused pairs\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_no_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_no_diag, 0)\n",
    "    \n",
    "    print(\"Top 5 most confused class pairs (true â†’ pred):\")\n",
    "    flat_idx = np.argsort(cm_no_diag.ravel())[-5:][::-1]\n",
    "    for idx in flat_idx:\n",
    "        i, j = divmod(idx, cm.shape[1])\n",
    "        if cm_no_diag[i, j] > 0:\n",
    "            print(f\"  {CLASS_NAMES[i]} â†’ {CLASS_NAMES[j]}: {cm_no_diag[i, j]}\")\n",
    "    \n",
    "    # Worst classes by F1\n",
    "    report = classification_report(y_true, y_pred, target_names=CLASS_NAMES, output_dict=True, zero_division=0)\n",
    "    worst = sorted([(c, report[c]['f1-score']) for c in CLASS_NAMES], key=lambda x: x[1])[:5]\n",
    "    print(\"Worst 5 classes by F1-score:\")\n",
    "    for cls, f1 in worst:\n",
    "        print(f\"  {cls}: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e289627",
   "metadata": {},
   "source": [
    "## 12. Misclassification Analysis & Grad-CAM\n",
    "\n",
    "Visualize misclassified samples and use Grad-CAM to understand where the model focuses its attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misclassified_samples(model_name: str):\n",
    "    \"\"\"Get misclassified test samples for a model (uses cached predictions from evaluation).\"\"\"\n",
    "    # Reuse predictions from evaluation cell if available\n",
    "    if model_name in model_predictions:\n",
    "        y_pred_probs = model_predictions[model_name]['probs']\n",
    "        y_pred = model_predictions[model_name]['pred']\n",
    "        y_true = model_predictions[model_name]['true']\n",
    "    else:\n",
    "        # Fallback: compute predictions\n",
    "        print(f\"  Computing predictions for {model_name}...\", end=\" \", flush=True)\n",
    "        model = trained_models[model_name]\n",
    "        _, _, test_ds = get_datasets_for_model(model_name)\n",
    "        y_pred_probs = model.predict(test_ds, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        y_true = test_labels.astype(int)\n",
    "        model_predictions[model_name] = {'probs': y_pred_probs, 'pred': y_pred, 'true': y_true}\n",
    "        print(\"Done!\")\n",
    "    \n",
    "    misclassified_idx = np.where(y_pred != y_true)[0]\n",
    "    return misclassified_idx, y_pred, y_pred_probs, y_true\n",
    "\n",
    "def plot_misclassified(model_name: str, n_show: int = 8):\n",
    "    \"\"\"Display misclassified images for a model.\"\"\"\n",
    "    misclassified_idx, y_pred, y_pred_probs, y_true = get_misclassified_samples(model_name)\n",
    "    \n",
    "    n_show = min(n_show, len(misclassified_idx))\n",
    "    if n_show == 0:\n",
    "        print(f\"No misclassified samples for {model_name}!\")\n",
    "        return\n",
    "    \n",
    "    cols = 4\n",
    "    rows = (n_show + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 4 * rows))\n",
    "    axes = axes.flatten() if n_show > 1 else [axes]\n",
    "    \n",
    "    for i, idx in enumerate(misclassified_idx[:n_show]):\n",
    "        img = plt.imread(test_paths[idx])\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        conf = y_pred_probs[idx, y_pred[idx]] * 100\n",
    "        axes[i].set_title(f\"True: {CLASS_NAMES[y_true[idx]]}\\nPred: {CLASS_NAMES[y_pred[idx]]} ({conf:.1f}%)\", fontsize=9)\n",
    "    \n",
    "    for i in range(n_show, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"{model_name} - Misclassified Samples ({len(misclassified_idx)} total)\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name.lower()}_misclassified.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Cache for feature models to avoid recreating\n",
    "_feature_model_cache = {}\n",
    "\n",
    "def visualize_gradcam(model_name: str, img_idx: int = None):\n",
    "    \"\"\"Visualize activation heatmap for a misclassified image (optimized).\"\"\"\n",
    "    print(f\"  Generating activation map for {model_name}...\", end=\" \", flush=True)\n",
    "    \n",
    "    misclassified_idx, y_pred, y_pred_probs, y_true = get_misclassified_samples(model_name)\n",
    "    \n",
    "    if len(misclassified_idx) == 0:\n",
    "        print(f\"No misclassified samples for {model_name}\")\n",
    "        return\n",
    "    \n",
    "    idx = misclassified_idx[0] if img_idx is None else img_idx\n",
    "    img_path = test_paths[idx]\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, CONFIG['img_size'])\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img_preprocessed = PREPROCESS_FUNCTIONS[model_name](img)\n",
    "    img_array = tf.expand_dims(img_preprocessed, 0)\n",
    "    \n",
    "    # Get or create feature model (cached)\n",
    "    if model_name not in _feature_model_cache:\n",
    "        model = trained_models[model_name]\n",
    "        base = model.layers[1]\n",
    "        last_conv_name = [l.name for l in base.layers if 'conv' in l.name][-1]\n",
    "        _feature_model_cache[model_name] = Model(inputs=base.input, outputs=base.get_layer(last_conv_name).output)\n",
    "    \n",
    "    feature_model = _feature_model_cache[model_name]\n",
    "    \n",
    "    # Get features and create heatmap from activations\n",
    "    features = feature_model(img_array)\n",
    "    heatmap = tf.reduce_mean(features, axis=-1)[0]\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)\n",
    "    heatmap = heatmap.numpy()\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "    # Load original image and create overlay\n",
    "    orig_img = plt.imread(img_path)\n",
    "    heatmap_resized = np.array(tf.image.resize(heatmap[..., np.newaxis], orig_img.shape[:2]))[:, :, 0]\n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized)\n",
    "    \n",
    "    cmap = plt.cm.jet(heatmap_resized)[:, :, :3]\n",
    "    overlay = 0.5 * cmap + 0.5 * (orig_img / 255.0 if orig_img.max() > 1 else orig_img)\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title(f\"Original\\nTrue: {CLASS_NAMES[y_true[idx]]}\", fontsize=11)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(heatmap, cmap='jet')\n",
    "    axes[1].set_title(\"Activation Heatmap\", fontsize=11)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(f\"Overlay\\nPred: {CLASS_NAMES[y_pred[idx]]} ({y_pred_probs[idx, y_pred[idx]]*100:.1f}%)\", fontsize=11)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"{model_name} Activation Analysis\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name.lower()}_gradcam.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Show misclassified samples and activation map for VGG19\n",
    "print(\"=\"*80)\n",
    "print(\"VGG19 MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "plot_misclassified('VGG19', n_show=8)\n",
    "visualize_gradcam('VGG19')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6e90a",
   "metadata": {},
   "source": [
    "## 13. Training Curves Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ba0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories: Dict[str, keras.callbacks.History], metric: str = 'accuracy'):\n",
    "    \"\"\"Plot training curves for all models.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for idx, (model_name, history) in enumerate(histories.items()):\n",
    "        epochs = range(1, len(history.history[metric]) + 1)\n",
    "        \n",
    "        axes[0].plot(epochs, history.history[metric], \n",
    "                     color=colors[idx], linestyle='-', linewidth=2,\n",
    "                     label=f'{model_name} (Train)')\n",
    "        axes[0].plot(epochs, history.history[f'val_{metric}'], \n",
    "                     color=colors[idx], linestyle='--', linewidth=2,\n",
    "                     label=f'{model_name} (Val)')\n",
    "    \n",
    "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].legend(loc='lower right', fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    \n",
    "    for idx, (model_name, history) in enumerate(histories.items()):\n",
    "        epochs = range(1, len(history.history['loss']) + 1)\n",
    "        \n",
    "        axes[1].plot(epochs, history.history['loss'], \n",
    "                     color=colors[idx], linestyle='-', linewidth=2,\n",
    "                     label=f'{model_name} (Train)')\n",
    "        axes[1].plot(epochs, history.history['val_loss'], \n",
    "                     color=colors[idx], linestyle='--', linewidth=2,\n",
    "                     label=f'{model_name} (Val)')\n",
    "    \n",
    "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].legend(loc='upper right', fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Training curves saved to: training_curves.png\")\n",
    "\n",
    "plot_training_history(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472b2a3",
   "metadata": {},
   "source": [
    "## 14. Individual Model Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cff4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual_model_curves(histories: Dict[str, keras.callbacks.History]):\n",
    "    \"\"\"Plot individual training curves for each model.\"\"\"\n",
    "    n_models = len(histories)\n",
    "    fig, axes = plt.subplots(n_models, 2, figsize=(14, 5 * n_models))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (model_name, history) in enumerate(histories.items()):\n",
    "        epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "        \n",
    "        axes[idx, 0].plot(epochs, history.history['accuracy'], 'b-', linewidth=2, label='Train')\n",
    "        axes[idx, 0].plot(epochs, history.history['val_accuracy'], 'r--', linewidth=2, label='Validation')\n",
    "        axes[idx, 0].set_title(f'{model_name} - Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[idx, 0].set_xlabel('Epoch')\n",
    "        axes[idx, 0].set_ylabel('Accuracy')\n",
    "        axes[idx, 0].legend()\n",
    "        axes[idx, 0].grid(True, alpha=0.3)\n",
    "        axes[idx, 0].set_ylim([0, 1])\n",
    "        \n",
    "        axes[idx, 1].plot(epochs, history.history['loss'], 'b-', linewidth=2, label='Train')\n",
    "        axes[idx, 1].plot(epochs, history.history['val_loss'], 'r--', linewidth=2, label='Validation')\n",
    "        axes[idx, 1].set_title(f'{model_name} - Loss', fontsize=12, fontweight='bold')\n",
    "        axes[idx, 1].set_xlabel('Epoch')\n",
    "        axes[idx, 1].set_ylabel('Loss')\n",
    "        axes[idx, 1].legend()\n",
    "        axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('individual_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Individual curves saved to: individual_training_curves.png\")\n",
    "\n",
    "plot_individual_model_curves(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aeed05",
   "metadata": {},
   "source": [
    "## 15. Model Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(test_results: dict):\n",
    "    \"\"\"Bar chart comparing test accuracy across models.\"\"\"\n",
    "    models = list(test_results.keys())\n",
    "    accuracies = [test_results[m]['accuracy'] * 100 for m in models]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "    bars = ax.bar(models, accuracies, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{acc:.2f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_title('Model Comparison - Test Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Model comparison saved to: model_comparison.png\")\n",
    "\n",
    "plot_model_comparison(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3365",
   "metadata": {},
   "source": [
    "## 16. Save Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODELS_DIR = Path('final_models')\n",
    "FINAL_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    save_path = FINAL_MODELS_DIR / f\"{model_name}_final.h5\"\n",
    "    model.save(save_path)\n",
    "    print(f\"Saved {model_name} to: {save_path}\")\n",
    "\n",
    "class_names_path = FINAL_MODELS_DIR / 'class_names.txt'\n",
    "with open(class_names_path, 'w') as f:\n",
    "    for name in CLASS_NAMES:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"Saved class names to: {class_names_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCheckpoints directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Final models directory: {FINAL_MODELS_DIR}\")\n",
    "print(f\"Visualization files: training_curves.png, individual_training_curves.png, model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbe735",
   "metadata": {},
   "source": [
    "## 16.5 Export Models for Adversarial Attacks\n",
    "\n",
    "Export trained models in TensorFlow SavedModel format with preprocessing metadata for ART/Foolbox compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create attack models directory\n",
    "ATTACK_MODELS_DIR = Path('attack_models')\n",
    "ATTACK_MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Preprocessing metadata for each model (required for ART/Foolbox)\n",
    "PREPROCESSING_INFO = {\n",
    "    'VGG19': {\n",
    "        'mode': 'caffe',  # BGR, mean subtraction [103.939, 116.779, 123.68]\n",
    "        'mean': [103.939, 116.779, 123.68],\n",
    "        'std': [1.0, 1.0, 1.0],\n",
    "        'input_range': [0, 255],\n",
    "        'channel_order': 'BGR'\n",
    "    },\n",
    "    'ResNet50': {\n",
    "        'mode': 'caffe',  # BGR, mean subtraction\n",
    "        'mean': [103.939, 116.779, 123.68],\n",
    "        'std': [1.0, 1.0, 1.0],\n",
    "        'input_range': [0, 255],\n",
    "        'channel_order': 'BGR'\n",
    "    },\n",
    "    'DenseNet121': {\n",
    "        'mode': 'torch',  # RGB, normalize to [0,1] then subtract mean/std\n",
    "        'mean': [0.485, 0.456, 0.406],\n",
    "        'std': [0.229, 0.224, 0.225],\n",
    "        'input_range': [0, 1],\n",
    "        'channel_order': 'RGB'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPORTING MODELS FOR ADVERSARIAL ATTACKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    model_dir = ATTACK_MODELS_DIR / model_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Save as TensorFlow SavedModel format (best for ART/Foolbox)\n",
    "    savedmodel_path = model_dir / 'saved_model'\n",
    "    model.save(str(savedmodel_path), save_format='tf')\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  SavedModel: {savedmodel_path}\")\n",
    "    \n",
    "    # 2. Also save as .h5 for easy loading\n",
    "    h5_path = model_dir / f'{model_name}_attack.h5'\n",
    "    model.save(h5_path)\n",
    "    print(f\"  H5 format: {h5_path}\")\n",
    "    \n",
    "    # 3. Save preprocessing metadata\n",
    "    metadata = {\n",
    "        'model_name': model_name,\n",
    "        'input_shape': [224, 224, 3],\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'class_names_file': '../final_models/class_names.txt',\n",
    "        'preprocessing': PREPROCESSING_INFO[model_name],\n",
    "        'training_info': {\n",
    "            'dataset': 'Caltech-101',\n",
    "            'total_images': TOTAL_IMAGES,\n",
    "            'fine_tuned_layers': FINE_TUNE_LAYERS,\n",
    "            'batch_size': CONFIG['batch_size'],\n",
    "            'epochs_trained': len(training_histories[model_name].history['loss']),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = model_dir / 'preprocessing.json'\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"  Metadata: {metadata_path}\")\n",
    "\n",
    "# Save training configuration for reproducibility\n",
    "config_path = ATTACK_MODELS_DIR / 'training_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'config': CONFIG,\n",
    "        'fine_tune_layers': FINE_TUNE_LAYERS,\n",
    "        'min_epochs_before_early_stop': MIN_EPOCHS_BEFORE_EARLY_STOP,\n",
    "        'models': list(trained_models.keys()),\n",
    "        'num_classes': NUM_CLASSES,\n",
    "    }, f, indent=2)\n",
    "print(f\"\\nTraining config: {config_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORT COMPLETE - Models ready for adversarial attacks\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAttack models directory: {ATTACK_MODELS_DIR}\")\n",
    "print(\"\\nTo use with ART (Adversarial Robustness Toolbox):\")\n",
    "print(\"  from art.estimators.classification import TensorFlowV2Classifier\")\n",
    "print(\"  import tensorflow as tf\")\n",
    "print(\"  model = tf.saved_model.load('attack_models/VGG19/saved_model')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e87ff",
   "metadata": {},
   "source": [
    "## 17. Individual Model Predictions\n",
    "\n",
    "Use these cells to make predictions with each model separately. You can provide an image path or use a random test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for single image prediction\n",
    "def predict_single_image(model, model_name: str, image_path: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Make a prediction on a single image using a specific model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Keras model\n",
    "        model_name: Name of the model (for preprocessing selection)\n",
    "        image_path: Path to the image file\n",
    "        top_k: Number of top predictions to show\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, CONFIG['img_size'])\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    \n",
    "    # Apply model-specific preprocessing\n",
    "    preprocess_fn = PREPROCESS_FUNCTIONS[model_name]\n",
    "    img = preprocess_fn(img)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(img, verbose=0)[0]\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
    "    \n",
    "    results = {\n",
    "        'image_path': image_path,\n",
    "        'model': model_name,\n",
    "        'predictions': []\n",
    "    }\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        results['predictions'].append({\n",
    "            'class': CLASS_NAMES[idx],\n",
    "            'confidence': float(predictions[idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_prediction(results: dict, show_image: bool = True):\n",
    "    \"\"\"Display prediction results with optional image.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {results['model']}\")\n",
    "    print(f\"Image: {results['image_path']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if show_image:\n",
    "        img = plt.imread(results['image_path'])\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Predicted: {results['predictions'][0]['class']}\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\nTop {len(results['predictions'])} Predictions:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, pred in enumerate(results['predictions'], 1):\n",
    "        confidence = pred['confidence'] * 100\n",
    "        bar = 'â–ˆ' * int(confidence / 5) + 'â–‘' * (20 - int(confidence / 5))\n",
    "        print(f\"{i}. {pred['class']:<25} {bar} {confidence:.2f}%\")\n",
    "\n",
    "print(\"Prediction helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca023a3",
   "metadata": {},
   "source": [
    "## 16.1 Predict with VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14495d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICT WITH VGG19\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Use a random test image\n",
    "random_idx = np.random.randint(0, len(test_paths))\n",
    "sample_image_path = test_paths[random_idx]\n",
    "true_label = CLASS_NAMES[test_labels[random_idx]]\n",
    "\n",
    "# Option 2: Specify your own image path (uncomment and modify)\n",
    "# sample_image_path = \"path/to/your/image.jpg\"\n",
    "\n",
    "# Load model (from memory or checkpoint)\n",
    "if 'VGG19' in trained_models:\n",
    "    vgg19_model = trained_models['VGG19']\n",
    "else:\n",
    "    vgg19_checkpoint = CHECKPOINT_DIR / \"VGG19_best.h5\"\n",
    "    vgg19_model = keras.models.load_model(vgg19_checkpoint)\n",
    "    print(f\"Loaded VGG19 from: {vgg19_checkpoint}\")\n",
    "\n",
    "# Make prediction\n",
    "vgg19_results = predict_single_image(vgg19_model, 'VGG19', sample_image_path, top_k=5)\n",
    "display_prediction(vgg19_results)\n",
    "\n",
    "print(f\"\\nTrue label: {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae5ee0",
   "metadata": {},
   "source": [
    "## 16.2 Predict with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4890162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICT WITH ResNet50\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Use a random test image\n",
    "# random_idx = np.random.randint(0, len(test_paths))\n",
    "# sample_image_path = test_paths[random_idx]\n",
    "# true_label = CLASS_NAMES[test_labels[random_idx]]\n",
    "\n",
    "# Option 2: Specify your own image path (uncomment and modify)\n",
    "sample_image_path = \"caltech101_data/caltech-101/101_ObjectCategories/lotus/image_0006.jpg\"\n",
    "\n",
    "# Load model (from memory or checkpoint)\n",
    "if 'ResNet50' in trained_models:\n",
    "    resnet_model = trained_models['ResNet50']\n",
    "else:\n",
    "    resnet_checkpoint = CHECKPOINT_DIR / \"ResNet50_best.h5\"\n",
    "    resnet_model = keras.models.load_model(resnet_checkpoint)\n",
    "    print(f\"Loaded ResNet50 from: {resnet_checkpoint}\")\n",
    "\n",
    "# Make prediction\n",
    "resnet_results = predict_single_image(resnet_model, 'ResNet50', sample_image_path, top_k=5)\n",
    "display_prediction(resnet_results)\n",
    "\n",
    "print(f\"\\nTrue label: {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed55ba",
   "metadata": {},
   "source": [
    "## 16.3 Predict with DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11839215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICT WITH DenseNet121\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Use a random test image\n",
    "# random_idx = np.random.randint(0, len(test_paths))\n",
    "# sample_image_path = test_paths[random_idx]\n",
    "# true_label = CLASS_NAMES[test_labels[random_idx]]\n",
    "\n",
    "# Option 2: Specify your own image path (uncomment and modify)\n",
    "sample_image_path = \"caltech101_data/caltech-101/101_ObjectCategories/lotus/image_0006.jpg\"\n",
    "\n",
    "# Load model (from memory or checkpoint)\n",
    "if 'DenseNet121' in trained_models:\n",
    "    densenet_model = trained_models['DenseNet121']\n",
    "else:\n",
    "    densenet_checkpoint = CHECKPOINT_DIR / \"DenseNet121_best.h5\"\n",
    "    densenet_model = keras.models.load_model(densenet_checkpoint)\n",
    "    print(f\"Loaded DenseNet121 from: {densenet_checkpoint}\")\n",
    "\n",
    "# Make prediction\n",
    "densenet_results = predict_single_image(densenet_model, 'DenseNet121', sample_image_path, top_k=5)\n",
    "display_prediction(densenet_results)\n",
    "\n",
    "print(f\"\\nTrue label: {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c35073",
   "metadata": {},
   "source": [
    "## 16.4 Compare All Models on Same Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE ALL MODELS ON THE SAME IMAGE\n",
    "# ============================================\n",
    "\n",
    "# Pick a random test image\n",
    "random_idx = np.random.randint(0, len(test_paths))\n",
    "comparison_image_path = test_paths[random_idx]\n",
    "true_label = CLASS_NAMES[test_labels[random_idx]]\n",
    "\n",
    "# comparison_image_path = \"caltech101_data/caltech-101/101_ObjectCategories/lotus/image_0006.jpg\"\n",
    "\n",
    "print(f\"Comparing all models on: {comparison_image_path}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "\n",
    "# Show the image\n",
    "img = plt.imread(comparison_image_path)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"True: {true_label}\")\n",
    "plt.show()\n",
    "\n",
    "# Predict with each model\n",
    "all_predictions = {}\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "    else:\n",
    "        checkpoint = CHECKPOINT_DIR / f\"{model_name}_best.h5\"\n",
    "        model = keras.models.load_model(checkpoint)\n",
    "    \n",
    "    results = predict_single_image(model, model_name, comparison_image_path, top_k=3)\n",
    "    all_predictions[model_name] = results['predictions']\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Model':<15} {'Top-1 Prediction':<25} {'Confidence':<12} {'Correct?'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name, preds in all_predictions.items():\n",
    "    top_pred = preds[0]['class']\n",
    "    confidence = preds[0]['confidence'] * 100\n",
    "    is_correct = \"âœ“\" if top_pred == true_label else \"âœ—\"\n",
    "    print(f\"{model_name:<15} {top_pred:<25} {confidence:>8.2f}%    {is_correct}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
