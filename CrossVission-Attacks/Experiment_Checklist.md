# âœ… EXPERIMENT CHECKLIST â€” PAPER 1 (ACL-ORIENTED)

## Goal

Evaluate how adversarial visual perturbations propagate from **CNNs â†’ ViTs â†’ VLMs** and cause **semantic and linguistic failures**.

---

## ğŸ”¹ A. DATASET & TASK SETUP (FOUNDATION)

### â˜ Dataset Selection

- â˜ Caltech101 (for CNN & ViT baselines)
- â˜ Imageâ€“text datasets for VLMs (e.g., COCO, VQA-style datasets)
- â˜ Ensure the **same images** flow through CNN â†’ ViT â†’ VLM where possible

### â˜ Prompt Design (VLMs)

- â˜ Fixed prompts (no prompt attack)
- â˜ Multiple prompt types:
  - â˜ Captioning
  - â˜ VQA
  - â˜ Instruction-following  
    *(e.g., â€œDescribeâ€¦â€, â€œCountâ€¦â€, â€œLocateâ€¦â€)*
- â˜ Prompts logged and versioned

### â˜ Train / Test Split

- â˜ Clean evaluation set
- â˜ Attacked evaluation set
- â˜ Same random seed across models

---

## ğŸ”¹ B. MODEL SETUP (NO CONFUSION ALLOWED)

### CNNs

- â˜ VGG19 (pretrained, frozen)
- â˜ ResNet50 (pretrained, frozen)
- â˜ DenseNet121 (pretrained, frozen)

### Vision Transformers (ViTs)

- â˜ Swin Transformer
- â˜ SAM *(segmentation output used for downstream analysis)*

### Visionâ€“Language Models (VLMs)

- â˜ CLIP
- â˜ BLIP
- â˜ PaLI-Gemma
- â˜ LLaVA-1.6

### â˜ Versioning & Configuration

- â˜ Model versions documented
- â˜ Checkpoints recorded
- â˜ Inference settings fixed  
  *(temperature, max tokens, decoding strategy)*

---

## ğŸ”¹ C. THREAT MODEL (REVIEWER MUST UNDERSTAND)

### â˜ Clearly Defined

- â˜ Evasion attacks only (test-time)
- â˜ Image-only perturbation
- â˜ Prompt unchanged
- â˜ White-box access (CNNs, ViTs)
- â˜ Black-box / surrogate access (VLMs)

### â˜ Perturbation Constraints

- â˜ Îµ values specified
- â˜ Norm used (Lâˆ / L2)
- â˜ Imperceptibility verified visually

---

## ğŸ”¹ D. ADVERSARIAL ATTACK IMPLEMENTATION

### Classical Attacks

- â˜ FGSM
- â˜ DeepFool
- â˜ Universal Adversarial Perturbation (UAP)

For each attack:

- â˜ Correct implementation
- â˜ Hyperparameters logged
- â˜ Attack success verified on CNNs

### Generative AI Attacks

- â˜ GAN-based adversarial image generation
- â˜ VAE / AE latent-space perturbation
- â˜ Image realism verified

### â˜ Attack Transferability

- â˜ CNN â†’ ViT
- â˜ ViT â†’ VLM
- â˜ Generative attacks outperform gradient-based attacks

---

## ğŸ”¹ E. CNN & ViT EVALUATION (SUPPORTING EVIDENCE)

### â˜ Metrics

- â˜ Clean accuracy
- â˜ Adversarial accuracy
- â˜ Accuracy drop (%)

### â˜ Representation Analysis

- â˜ Feature drift (layer-wise)
- â˜ Token instability (ViTs)
- â˜ SAM mask distortion

### â˜ Explainability

- â˜ Saliency maps (clean vs adversarial)
- â˜ Attention heatmaps
- â˜ Qualitative comparisons saved

---

## ğŸ”¹ F. VLM EVALUATION (ACL CORE)

### â˜ Language Failure Taxonomy Defined

- â˜ Hallucination
- â˜ Wrong entity grounding
- â˜ Attribute errors (color, count, size)
- â˜ Instruction violation
- â˜ Partial compliance
- â˜ Overconfident incorrect answers

### â˜ Annotation Protocol

- â˜ Manual or semi-automatic labeling
- â˜ Clear definitions per category
- â˜ Representative examples saved

### â˜ Metrics

- â˜ Failure rate per category
- â˜ Overall semantic failure rate
- â˜ Comparison across attack types

### â˜ Qualitative Analysis

- â˜ Clean vs adversarial output pairs
- â˜ Failure explanations per example

---

## ğŸ”¹ G. GENERATIVE ATTACK QUALITY CHECK

### â˜ Image Realism

- â˜ FID score computed
- â˜ Visual inspection performed
- â˜ Human-imperceptibility confirmed

### â˜ Transferability Proof

- â˜ Same image fools multiple VLMs
- â˜ Attack generalizes across different prompts

---

## ğŸ”¹ H. STATISTICAL RIGOR (VERY IMPORTANT)

- â˜ Multiple runs (â‰¥ 3 seeds)
- â˜ Mean Â± standard deviation reported
- â˜ Significance testing (t-test / Wilcoxon)
- â˜ Confidence intervals where applicable

---

## ğŸ”¹ I. ABLATION STUDIES (SMALL BUT POWERFUL)

- â˜ Attack strength vs language failure
- â˜ Gradient-based vs generative attacks
- â˜ CNN-robust â‰  VLM-robust demonstration
- â˜ SAM-based vs non-SAM pipelines

---

## ğŸ”¹ J. FIGURES & TABLES (REVIEWER BAIT)

### â˜ Required Plots

- â˜ Accuracy drop (CNN / ViT)
- â˜ Language failure rates (VLMs)
- â˜ Attack comparison heatmaps

### â˜ Visual Examples

- â˜ Clean vs adversarial images
- â˜ Corresponding VLM outputs
- â˜ Attention / saliency overlays

---

## ğŸ”¹ K. REPRODUCIBILITY CHECK

- â˜ Code runs end-to-end
- â˜ Configuration files saved
- â˜ Random seeds fixed
- â˜ README explains experiments
- â˜ Hardware and environment documented

---

## ğŸ”¹ L. SCOPE SAFETY CHECK (DO NOT FAIL THIS)

- â˜ No defenses included
- â˜ No watermarking methods
- â˜ No control objectives
- â˜ Watermarking mentioned **only as future work**

---

## ğŸ”¹ M. FINAL SANITY QUESTIONS (ASK YOURSELF)

- â˜ Is **language failure** the main result?
- â˜ Are CNNs / ViTs supporting evidence, not the headline?
- â˜ Can a linguistics reviewer understand the failure taxonomy?
- â˜ Is the contribution clear in **three sentences**?

**If YES to all â†’ you are ready to write.**

---

## ğŸ§  Advisor-Level Honesty

If you complete **80â€“90%** of this checklist, your paper will:

- Feel **deliberate**, not exploratory  
- Be hard to dismiss as *â€œjust benchmarkingâ€*  
- Fit **ACL Findings / EMNLP Findings** cleanly
